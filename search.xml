<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[代码实现（五）之Policy Gradient]]></title>
    <url>%2F2019%2F08%2F12%2F%E4%BB%A3%E7%A0%81%2FDRL%2F%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8BPolicy-Gradient%2F</url>
    <content type="text"><![CDATA[前言 Policy Gradient是RL里面基于Policy-Based的方法，与前面的DQN的基于Value-Based的方法不同。其理论部分，查看DRL论文阅读（五） 本篇代码是基于莫烦的代码，然后进行了少量的修改，实践中使用的是离散行为空间的softmax策略函数，而不是连续行为空间的高斯策略函数 一、实验环境 使用CartPole和MountainCar作为实验环境，这里将不再对环境进行说明 二、Policy Gradient算法伪代码：采用的是蒙特卡洛策略梯度reinforce算法(不带基数) 它是一个基于整条回合数据的更新，即在训练之前，要先收集整个episode的(s,a,r,s)，然后在进行参数更新 $\nabla_\theta log \pi_\theta(s,a) v$表示在状态$s$对所选动作$a$的吃惊度，如果$\pi_\theta(s,a)$概率越小，则反向的 $log\pi _\theta(s,a)$)(即$-log\pi _\theta(s,a)$)越大，如果在$\pi_\theta(s,a)$很小的情况下，拿到了一个大的$R$，也就是大的$V$，则$-\nabla_\theta log \pi_\theta(s,a) v$就更大，表示更吃惊（我选了一个不常选的动作，结果发现它得到了一个好的reward，那我就得对我这次的参数进行一个大幅度的修改）。这也就是吃惊度的物理意义。 策略梯度公式 \nabla_\theta J(\theta) = E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a) Q^{\pi\theta}(s,a)]策略函数： softmax策略函数 \pi_\theta(s,a) = \frac{e^{\phi(s,a)^T \theta}}{ \sum_{b} e^{\phi(s,b)^T \theta}} \nabla_\theta log\pi_\theta(s,a) = \phi(s,a) - E_{\pi\theta}[\phi(s,\cdot)]其中，$\phi(s,a)$表示状态-动作对的L维特征向量 高斯策略函数 \mu(s) = \phi(s)^T \theta a \sim N(\mu(s),\sigma^2) \nabla_\theta log \pi_\theta(s,a) = \frac{(a-\mu(s)) \phi(s)}{\sigma^2}三、代码部分3.1 代码组成 代码主要由三部分组成：policy_gradient.py、run_CartPole.py、run_MountainCar.py 3.2 网络-policy_gradient.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#!/usr/bin/env python#-*- coding: utf-8 -*-import numpy as np import tensorflow as tf np.random.seed(1)tf.set_random_seed(1)class PolicyGradient(object): def __init__(self, s_dim, a_dim, learning_rate = 0.01, reward_decay = 0.95, output_graph = False ): self.s_dim = s_dim self.a_dim = a_dim self.lr = learning_rate # 学习率 self.gamma = reward_decay # reward 递减率 self.ep_obs,self.ep_as,self.ep_rs = [],[],[] #用来存储s,a,r self.build_net() # 建立 policy 神经网络 self.sess = tf.Session() if output_graph: tf.summary.FileWriter("logs/",self.sess.graph) self.sess.run(tf.global_variables_initializer()) def build_net(self): # 输入 self.s = tf.placeholder(tf.float32,[None,self.s_dim],name='s_dim') # 接收 observation self.tf_acts = tf.placeholder(tf.int32,[None,],name="actions_num") # 接收我们在这个回合中选过的 actions self.tf_vt = tf.placeholder(tf.float32,[None,],name="actions_value")# 接收每个 state-action 所对应的 value (通过 reward 计算) # 网络参数 w_initializer = tf.random_normal_initializer(0.,0.3) b_initializer = tf.constant_initializer(0.1) c_names = ['softmax_output',tf.GraphKeys.GLOBAL_VARIABLES] # 隐藏层1 使用relu激活函数 with tf.variable_scope('fc1'): w1 = tf.get_variable('w1',[self.s_dim,20],initializer=w_initializer,collections=c_names) b1 = tf.get_variable('b1',[20],initializer=b_initializer,collections=c_names) fc1 = tf.nn.relu(tf.matmul(self.s,w1)+b1) # 隐藏层最后一层，输出是不需要激活函数的 with tf.variable_scope('fc2'): w2 = tf.get_variable('w2',[20,self.a_dim],initializer=w_initializer,collections=c_names) b2 = tf.get_variable('b2',[self.a_dim],initializer=b_initializer,collections=c_names) all_act = tf.matmul(fc1,w2) + b2 # softmax 输出层，输出每个行为所对应的概率 self.all_act_prob = tf.nn.softmax(all_act,name='act_prob') with tf.variable_scope('loss'): # 最大化 总体 reward (log_p * R) 就是在最小化 -(log_p * R), 而 tf 的功能里只有最小化 loss neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act,labels=self.tf_acts) # 所选 action 的概率 -log 值 # 或者使用下面这种方式 # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1) loss = tf.reduce_mean(neg_log_prob * self.tf_vt) # (vt = 本reward + 衰减的未来reward) 引导参数的梯度下降 #使用的是Adam优化器 with tf.variable_scope('train'): self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss) #根据softmax所输出的概率选择行为 def choose_action(self,state): prob_weights = self.sess.run(self.all_act_prob,feed_dict=&#123;self.s:state.reshape(-1,self.s_dim)&#125;) # 所有 action 的概率 action = np.random.choice(range(prob_weights.shape[1]),p=prob_weights.ravel()) # 根据概率来选 action return action #存储整个episode的s,a,r def store_transition(self,s,a,r): self.ep_obs.append(s) self.ep_as.append(a) self.ep_rs.append(r) def learn(self): # 衰减, 并标准化这回合的 reward discounted_ep_rs_norm = self.discount_and_norm_rewards() # train on episode self.sess.run(self.train_op,feed_dict=&#123; self.s:np.vstack(self.ep_obs), # shape=[None, n_obs] self.tf_acts:np.array(self.ep_as), # shape=[None, ] self.tf_vt:discounted_ep_rs_norm # shape=[None, ] &#125;) self.ep_obs,self.ep_as,self.ep_rs = [],[],[] # 训练完后，清空，在重新保存 return discounted_ep_rs_norm # 返回这一回合的 state-action value #和以往的不同的是，这里是从后向前计算 def discount_and_norm_rewards(self): # discount episode rewards discounted_ep_rs = np.zeros_like(self.ep_rs) running_add = 0 for t in reversed(range(0,len(self.ep_rs))): #反转，后向前 running_add = running_add * self.gamma + self.ep_rs[t] discounted_ep_rs[t] = running_add # normalize episode rewards discounted_ep_rs -= np.mean(discounted_ep_rs) discounted_ep_rs /= np.std(discounted_ep_rs) return discounted_ep_rs 部分语句解释： （1）*.shape[1]和ravel函数 这里取prob_weights.shape[1]表示获取prob_weights的列个数，.shape[0]则表示行个数，即行为的维度 ravel函数表示扁平化，即拉成一个维度，ravel函数详情 1action = np.random.choice(range(prob_weights.shape[1]),p=prob_weights.ravel()) （2）zeros_like函数 返回与指定数组具有相同形状和数据类型的数组，并且数组中的值都为0。函数详情 1discounted_ep_rs = np.zeros_like(self.ep_rs) （3）reversed函数 反转，PG算法中是前向，不是后向，因此需要反转。函数详情 1for t in reversed(range(0,len(self.ep_rs))): （4）np.std函数 计算矩阵标准差，函数详情 1discounted_ep_rs /= np.std(discounted_ep_rs) （5）np.vstack函数 按照垂直方向堆叠数组，函数详情 1self.s:np.vstack(self.ep_obs), （6）tf.nn.sparse_softmax_cross_entropy_with_logits函数 这个地方没怎么搞懂，函数详情 1neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act,labels=self.tf_acts) 注：疑惑的地方，没弄懂 （1）这里的这两个输入，为什么是[None,]，后面为什么是空的？ 12self.tf_acts = tf.placeholder(tf.int32,[None,],name="actions_num")self.tf_vt = tf.placeholder(tf.float32,[None,],name="actions_value") （2）计算损失函数 这里的损失函数，干嘛要求均值呢？即用tf.reduce_mean？ softmax策略函数体现在哪里，只是在前面直接用来求最后的输出层就可以了吗？这样就是用的softmax策略函数？那么前面关于softmax策略函数的梯度公式给出是干啥的呢？$\nabla_\theta log\pi_\theta(s,a) = \phi(s,a) - E_{\pi\theta}[\phi(s,\cdot)]$ 12345with tf.variable_scope('loss'): neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act,labels=self.tf_acts) # 或者使用下面这种方式 # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1) loss = tf.reduce_mean(neg_log_prob * self.tf_vt) # reward guided loss 这里用的损失函数的优化器，选用的是Adam，而不是前面DQN相关的RMSP 3.3 主函数-run_CartPole.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/usr/bin/env python#-*- coding: utf-8 -*-import gymfrom policy_gradient import PolicyGradientimport matplotlib.pyplot as pltDISPLAY_REWARD_THRESHOLD = 400 # 当 回合总 reward 大于 400 时显示模拟窗口RENDER = False # 在屏幕上显示模拟窗口会拖慢运行速度, 我们等计算机学得差不多了再显示模拟env = gym.make('CartPole-v0')env.seed(1) #普通的 PG 方法, 使得回合的 variance 比较大, 所以我们选了一个好点的随机种子env = env.unwrapped # 取消限制print(env.action_space)print(env.observation_space)print(env.observation_space.high)print(env.observation_space.low)RL = PolicyGradient( s_dim = env.observation_space.shape[0], a_dim = env.action_space.n, learning_rate = 0.02, reward_decay = 0.99, #output_graph = True # output_graph=True, # 输出 tensorboard 文件 )for i_epsiode in range(3000): s = env.reset() while True: if RENDER: env.render() a = RL.choose_action(s) s_,r,done,info = env.step(a) RL.store_transition(s,a,r) # 存储这一回合的 transition if done: ep_rs_sum = sum(RL.ep_rs) if 'running_reward' not in globals(): running_reward = ep_rs_sum else: # 对应训练过程中的discount reward running_reward = running_reward * 0.99 + ep_rs_sum * 0.01 if running_reward &gt; DISPLAY_REWARD_THRESHOLD: RENDER = True print('episode:',i_epsiode,"reward:",int(running_reward)) vt = RL.learn()# 学习, 输出 vt #这个地方 可以加可以不加 if i_epsiode == 0: plt.plot(vt) plt.xlabel('episode steps') plt.ylabel('normalized state-action value') plt.show() break s = s_, 3.4 主函数-run_MountainCar.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#!/usr/bin/env python#-*- coding: utf-8 -*-import gymfrom policy_gradient import PolicyGradientimport matplotlib.pyplot as pltDISPLAY_REWARD_THRESHOLD = -2000 # renders environment if total episode reward is greater then this thresholdRENDER = False # rendering wastes timeenv = gym.make('MountainCar-v0')env.seed(1) # reproducible, general Policy gradient has high varianceenv = env.unwrappedprint(env.action_space)print(env.observation_space)print(env.observation_space.high)print(env.observation_space.low)RL = PolicyGradient( s_dim = env.observation_space.shape[0], a_dim = env.action_space.n, learning_rate = 0.02, reward_decay = 0.995, #output_graph = True )for i_epsiode in range(1000): s = env.reset() while True: if RENDER: env.render() a = RL.choose_action(s) s_,r,done,info = env.step(a) RL.store_transition(s,a,r) if done: ep_rs_sum = sum(RL.ep_rs) if 'running_reward' not in globals(): running_reward = ep_rs_sum else: running_reward = running_reward * 0.99 + ep_rs_sum * 0.01 if running_reward &gt; DISPLAY_REWARD_THRESHOLD: RENDER = True print('episode:',i_epsiode,"reward:",int(running_reward)) vt = RL.learn() if i_epsiode == 30: plt.plot(vt) plt.xlabel('episode steps') plt.ylabel('normalized state-action value') plt.show() break s = s_ 四、界面显示run_CartPole.py 可以看出，左边的$v_t$有较高的值，右边的$v_t$有较低的值，也就说通过$v_t$再说明： “请重视我这回合刚开始时的一系列动作，因为前面一段时间杆子还没有掉下来，而且请惩罚我之后的一系列动作，因为后面的动作让杆子掉下来了”或者是 “我每次都想让这个动作在下一次增加被选的可能性（$\nabla_\theta log \pi_\theta(s,a)$），但是增加可能性的这种做法是好还是坏呢？这就要由$v_t$告诉我了，所有后一段时间的增加可能性做法并没有被提倡，而前段时间的增加可能性做法是被提倡的。” 这样，$v_t$就能通过 1loss = tf.reduce_mean(neg_log_prob * self.tf_vt) 诱导gradient descent朝着正确的方向发展了。 run_MountainCar.py MountainCar这张图的意思是在说： “请重视我这回合最后的一系列动作, 因为这一系列动作让我爬上了山. 而且请惩罚我开始的一系列动作, 因为这些动作没能让我爬上山”. 同样，通过$v_t$来诱导梯度下降的方向。 总结 和Value-Based相比，网络结构发生了很多变化，并且没有Target网络，也不需要DQN的经验池等等，代码简洁了很多，并且损失函数的优化器不一样。这个只是最基础的PG算法，下面将开始AC算法 在网络的代码中，PG在build_net、choose_action、store_transition处和之前的Value-Based方法有所改变，并且新添加了discount_and_norm_rewards函数 参考链接 莫烦算法介绍 莫烦代码 刘建平算法介绍 刘建平代码 Fisher’s代码介绍 Fisher’s代码（AC含高斯策略函数）]]></content>
      <tags>
        <tag>DRL</tag>
        <tag>OpenAI gym</tag>
        <tag>Policy Gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ardrone强化学习训练环境搭建]]></title>
    <url>%2F2019%2F08%2F11%2FROS%2Fardrone%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言 本篇主要是针对DRL论文阅读（一）中的论文题目来实现，其主要实现了第一部分标记检测，目前是只实现了单纹理下的训练。 其论文题目：Autonomous Quadrotor Landing using Deep Reinforcement Learning 一、基础环境搭建1.1 版本说明 Ubuntu16.04 ROS Kinetic Gazebo8.6 1.2 Gazebo8.6安装12345sudo sh -c 'echo "deb http://packages.osrfoundation.org/gazebo/ubuntu-stable `lsb_release -cs` main" &gt; /etc/apt/sources.list.d/gazebo-stable.list'wget http://packages.osrfoundation.org/gazebo.key -O - | sudo apt-key add -sudo apt-get updatesudo apt-get install gazebo8sudo apt-get install libgazebo8-dev 安装好后，在终端输入gazebo8，出现如下界面，则安装成功 1.3 ros kinetic安装（不安装full版）123456789sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/ros-latest.list'sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116sudo apt-get updatesudo apt-get install ros-kinetic-desktopsudo rosdep initrosdep updateecho "source /opt/ros/kinetic/setup.bash" &gt;&gt; ~/.bashrcsource ~/.bashrcsudo apt-get install python-rosinstall python-rosinstall-generator python-wstool build-essential 安装完后，终端输入roscore，输出的最后一行出现started core service [/roscore]，则ros安装成功 1.4 安装必要ros包123456sudo apt-get install ros-kinetic-gazebo8-msgssudo apt-get install ros-kinetic-gazebo8-ros-controlsudo apt-get install ros-kinetic-gazebo8-pluginssudo apt-get install ros-kinetic-gazebo8-ros-pkgssudo apt-get install ros-kinetic-gazebo8-rossudo apt-get install ros-kinetic-image-view 1.5 已经安装默认的ros-kinetic？ 如果机器已经默认安装了ros-kinetic-desktop-full，则会默认安装gazebo7版本。此时，需要先卸载gazebo7然后重新安装gazebo8 卸载ros-kinetic-desktop-full 1sudo apt-get remove ros-kinetic-desktop-full 卸载gazebo7 1sudo apt-get remove gazebo* 然后重新按照1.2节和1.3节和1.4节在做一遍，在1.3节的步骤中，其实是可以只执行到第四步即可的。因为其他的之前已经安装过了 1sudo apt-get install ros-kinetic-desktop 二、搭建qlab实验环境安装ros-kinetic-ardrone-autonomy 1sudo apt-get install ros-kinetic-ardrone-autonomy 2.1 创建ros工作空间创建工作空间 12345mkdir qlab_wscd qlab_wsmkdir srccd srccatkin_init_workspace 编译和source 1234cd ~/qlab_wscatkin_makeecho "source ~/qlab_ws/devel/setup.bash" &gt;&gt; ~/.bashrcsource ~/.bashrc 2.2 下载qlab源码下载并拷贝完后，~/qlab_ws/src文件夹下将会有deep_reinforced_landing和qlab两个文件 123cd git clone https://github.com/pulver22/QLAB.git -b gazebo8cp -r QLAB/deep_reinforced_landing/ QLAB/qlab/ ~/qlab_ws/src 编译和source 123cd qlab_wscatkin_makesource devel/setup.bash 注：每次ros工作空间下拷贝或新建了新的程序包，都需要重新编译和source 2.3 启动测试相关配置：因为在启动launch文件的时候，它里面会加载一些model模型，这些模型全部在qlab的文件夹下，因此需要在~/.bashrc文件下添加其路径，或者将modes文件夹全部拷贝到.gazebo文件夹下也可以. （1）打开~/.bashrc文件 1sudo gedit ~/.bashrc （2）在最后一行添加如下代码 下面的username要修改为对应的机器的用户名 1export GAZEBO_MODEL_PATH="/home/username/qlab_ws/src/qlab/qlab_gazebo/models" （3）关闭~/.bashrc文件并source 1source ~/.bashrc 启动qlab.launch 12cd ~/qlab_wsroslaunch qlab_gazebo qlab.launch 启动drl.launch 12cd ~/qlab_wsroslaunch deep_reinforced_landing drl.launch 查看rostopic 常用的控制命令 1234rostopic pub -1 /quadrotor/ardrone/takeoff std_msgs/Emptyrostopic pub -1 /quadrotor/ardrone/land std_msgs/Emptyrosrun image_view image_view image:=/quadrotor/ardrone/front/ardrone/front/image_rawrosrun image_view image_view image:=/quadrotor/ardrone/bottom/ardrone/bottom/image_raw 三、强化学习训练3.1 修改相关配置（1）修改无人机最大高度 因为在第一阶段训练的过程中，无人机的高度是固定在20m左右的，而其默认的最大高度为3m，因此需要先修改最大高度限定，可以修改为22m 在~/qlab_ws/src/qlab/qlab_description/urdf文件夹下找到quadrotor_sensors.urdf.xacro文件 然后在其中的第20行，将max_range的值由3.0修改为22.0 这样，无人机的最大高度就修改为了22 （2）训练时，需要关闭clinet，即关闭gazebo图形界面，提升训练速度 训练时，启动qlab.launch进行训练，则对qlab.launch文件进行修改 将其中的第七行的gui的值由true修改为false，如图所示 在该launch文件中，还引用了ardrone_sim_world.launch，因此也去这个launch文件中进行修改 同样的，找到第5行，将gui的值修改为false 注：如果要关闭掉client的图像界面，则最好是在启动的相关的文件中，将所有的gui的值都设为false，这样比较保险 3.2 安装相关包 gym 可以采用pip安装，也可以采用源码安装方式 1sudo pip install gym Tensorflow：详细安装 3.3 DQN代码编写 现在重新整理了代码，正在训练过程中~ 总结 这是做的第一个DRL的强化学习代码训练，最开始确实花了很多功夫，并且状态空间很大，需要有一个好的机器，训练速度才会快，我最开始是在服务器上8G、CPU上进行的训练，耗费了一个多月才训练完单纹理的目标检测阶段。机器尤为重要。 参考链接 gazebo8安装 QLAB源码]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Gazebo</tag>
        <tag>ardrone</tag>
        <tag>DRL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码实现（四）之 Dueling DQN]]></title>
    <url>%2F2019%2F08%2F10%2F%E4%BB%A3%E7%A0%81%2FDRL%2F%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8B-Dueling-DQN%2F</url>
    <content type="text"><![CDATA[前言 Dueling DQN 相比较于之前的Double DQN（目标Q值方面的优化）、Prioritized Experience Replay（经验回放池的优化），这次注重的是神经网络的优化，即将最后的Q网络分为V网络和A网络。 其中，V网络仅仅与状态S有关，与具体要采用的动作A无关，通常称为状态价值函数； A网络同时与状态S和动作A有关，通常称为动作优势函数。 其理论部分，点击DRL论文阅读（四） 本文的代码主要参考莫烦的代码来修改的。 一、实验环境 使用Pendulum来作为实验环境 这个实验环境的动作空间是连续的，其范围为[-2,2]。在前面的Double DQN实验中，我们将连续动作空间离散化为11个动作。 在本节的实验中，将分别离散化为5、15、25、35个行为空间来看其效果。 二、Dueling DQN传统的Nature DQN的输出最后是卷积层先经过全连接层，然后在输出对应于每个动作行为a的值。 而现在的Dueling DQN则是将最后一个卷积层的输出分为两部分，分别是V网络和A网络，如下图下方的分支中，上面的为V网络，下面的为A网络。 其中 V(s)：V(s)表示状态本身的好坏，一般为一个值（标量）。 A(s,a)：A(s,a)表示当前状态下采取的行为的好坏，一般为n个值，因此A网络决定了策略 Dueling DQN的Q网络计算公式如下： Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \left(A(s,a;\theta,\alpha) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s,a';\theta,\alpha) \right)其中，$\theta$为卷积层参数，$\beta$为V网络全连接层参数,$\alpha$为A网络全连接层参数 三、代码部分 Dueling DQN相比于Nature网络，只在最后的网络结构部分改变了一点点，其他的都没有任何变化 若想看完整代码，直接查看所有代码 3.1 代码组成 本节的代码主要包含两个部分：dueling_dqn.py和run_Pendulum.py 3.2 网络-dueling_dqn.py 只在网络结构部分发生了变化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class DueilingDQN(object): def __init__(...,dueling=True,sess=None) self.dueling = dueling #会建立两个DQN，其中一个是Dueling DQN # 针对建立两个 DQN 的模式修改了 tf.Session() 的建立方式 if sess is None: self.sess = tf.Session() self.sess.run(tf.global_variables_initializer()) else: self.sess = sess ... def build_net(self): # ************************ build evaluate net ***************************** with tf.variable_scope('eval_net'): c_names = ['eval_net_params',tf.GraphKeys.GLOBAL_VARIABLES] #第一层，两种DQN都一样 with tf.variable_scope('layer1'): w1 = tf.get_variable('w1',[self.s_dim,20],initializer=w_initializer,collections=c_names) b1 = tf.get_variable('b1',[20],initializer=b_initializer,collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s,w1)+b1) if self.dueling: #Dueling DQN with tf.variable_scope('Value'): #专门分析state的Value w2 = tf.get_variable('w2',[20,1],initializer=w_initializer,collections=c_names) b2 = tf.get_variable('b2',[1],initializer=b_initializer,collections=c_names) self.V = tf.matmul(l1,w2) + b2 with tf.variable_scope('Advantage'): #专门分析每种动作的Advantage w2 = tf.get_variable('w2',[20,self.a_dim],initializer=w_initializer,collections=c_names) b2 = tf.get_variable('b2',[self.a_dim],initializer=b_initializer,collections=c_names) self.A = tf.matmul(l1,w2) + b2 with tf.variable_scope('Q'): # 合并 V 和 A, 为了不让 A 直接学成了 Q, 我们减掉了 A 的均值 self.q_eval = self.V + (self.A - tf.reduce_mean(self.A,axis=1,keep_dims=True)) else: with tf.variable_scope('Q'): # 普通的 DQN 第二层 w2 = tf.get_variable('w2',[20,self.a_dim],initializer=w_initializer,collections=c_names) b2 = tf.get_variable('b2',[self.a_dim],initializer=b_initializer,collections=c_names) self.q_eval = tf.matmul(l1,w2) + b2 # ************************ build target net ***************************** # target net 网络和前面的evaluate net网络一样，除了输入由s变为s_，输出self.q_eval变为self.q_next外，没有任何区别 在V网络中，因为其输出是一个标量值，因此维度这里为[20,1] 1w2 = tf.get_variable('w2',[20,1],initializer=w_initializer,collections=c_names) 而A网络的输出，对应的每个a的值，因此维度为[20,self.a_dim] 1w2 = tf.get_variable('w2',[20,self.a_dim],initializer=w_initializer,collections=c_names) 最后，V和A网络合并，成为Q网络 1self.q_eval = self.V + (self.A - tf.reduce_mean(self.A,axis=1,keep_dims=True)) 3.3 主函数-run_Pendulum.py123456env = gym.make('Pendulum-v0')env = env.unwrappedenv.seed(1)MEMORY_SIZE = 3000ACTION_SPACE = 15 #将在这里修改行为空间的离散值，可以为5、15、25、35等 四、结果显示5个actions 15个actions 25个actions 35个actions 累积奖励reward，在杆子立起来的时候奖励为0，其他时候都是负值，所以当累积奖励没有在降低时2，说明杆子已经被成功立很久了。 我们发现当可用动作越高，学习难度就越大，不过Dueling DQN还是回比Nature DQN学习得更快，收敛效果更好。 总结 至此，DQN即DQN的三个优化方面的代码已经全部简单测试完毕，并且也差不多了解了其代码，对原理将会更加理解，其中Prioritized Experience Replay的代码部分最为复杂，Double DQN只在最后计算Q值的部分改动了一点点，Dueling DQN在网络结构部分改变了一点点。 后面，将会开始Policy Gradient的代码学习。 参考链接 莫烦Dueling DQN代码部分 莫烦Dueling DQN讲解部分]]></content>
      <tags>
        <tag>DRL</tag>
        <tag>OpenAI gym</tag>
        <tag>Dueling DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码实现（三）之Prioritized Experience Replay]]></title>
    <url>%2F2019%2F08%2F09%2F%E4%BB%A3%E7%A0%81%2FDRL%2F%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8BPrioritized-Experience-Replay%2F</url>
    <content type="text"><![CDATA[前言 理论部分，点击DRL论文阅读（三）查看 本文的代码主要参考了莫烦和Fisher’s的代码，在他们的基础之上，修改为自己习惯能看懂的代码 Prioritized DQN不同于DQN/DDQN的是，它关注的是经验回放池中那些很少但有用的正面信息 一、实验环境 使用MountainCar环境来对算法进行测试 和之前有所不同的是，这次我们不需要重度改变它的reward。所以只要没有拿到小旗子，reward = -1，拿到小旗子时，定义reward = 10。比起之前的DQN，这个reward定义更加准确。如果使用这种reward定义方式，可以想象Nature DQN会花很久的时间学习，因为记忆库中只有很少很少的+10 的reward可以学习，正负样本不一样，而使用Prioritized DQN，就会重视这种少量的，但值得学习的样本。 二、Prioritized Experience Replay伪代码 在该算法中，我们batch抽样时，并不是随机抽样，而是按照Memory中的样本的优先级来抽，这样能更有效的找到我们需要学习的样本。 SumTree 由于使用贪婪法来选取优先经验的时间复杂度太高，同时还有其他问题，所以我们用$P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$来定义某个片段的选取概率，其中我们采用比例优先的方式来计算$p_i$，即$p_i = |\delta_i| + \epsilon$，$\delta_i$为TD-Error，$\epsilon$为一个很小的正数，为了避免TD-Error为0的特殊边缘例子也能被采样到，并用SumTree这种数据结构来表示每个存储的片段。 SumTree是一种二叉树类型的数据结构，所有叶子节点存储优先级$p_i$，所有父节点为子节点之和，所以这棵树的根节点为所有叶子节点的和，如下图所示： 抽样时，我们会将$p_i$的总和除以batch_size，分成batch_size个区间，如上图例子所示，一共有3、10、12、4、1、2、8、2等八个优先级节点$p_i$，如果设置batch_size为6，则会分成6个区间，每个区间为：[0-7],[7-14],[14-21],[21-28],[28-35],[35-41]，在分别在这6个区间中均匀的随机选取一个数，从根节点依次往下搜索。 如果在第4个区间[21-28]抽取到了24，则将24与根节点的左节点进行比较，因为24 &lt; 29，所以继续向左搜索，将24与29的左节点比较，发现24 &gt; 13，则继续向右搜索，同时 24 - 13 = 11。将11与16的左节点比较，11 &lt; 12，因为12已经是叶子节点，则搜索完毕，选择12这个优先级。 图中叶子节点下面的括号中的区间表示该优先级可以被搜索到的范围，由此可见优先级大的被搜索到的概率就高，同时优先级小的，也有一定的概率被选中。 三、代码部分 没有按照文中，与Double DQN结合，而是与Nature DQN相结合 若想要看全部代码，直接查看所有代码 3.1 代码组成 代码由两部分组成，分别为prioritized.py 和run_MountainCar.py （1）prioritized.py ​ 这个代码中主要包含三个类：SumTree、Memory(prioritized)、DQNPrioritizedReplay 后面又重新添加了Double DQN的prioritized.py，这里将不对这个进行说明，毕竟代码改动很少 3.2 网络-prioritized.py3.2.1 SumTree有效抽样1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class SumTree(object): def __init__(self,capacity): self.capacity = capacity # SumTree能存储的最多优先级个数，如之前的SumTree图，则值为8 self.tree = np.zeros(2 * capacity - 1) # 顺序表存储二叉树，其个数为 2 * capacity -1 self.data = np.zeros(capacity,dtype=object) # 每个优先级对应的经验片段，dtype必须要添加 self.size = 0 self.curr_point = 0 # 当有新的sample时，添加进tree 和data def add(self,data): self.data[self.curr_point] = data #存储的为(s,a,r,s_,done) self.update(self.curr_point,max(self.tree[self.capacity-1:self.capacity+self.size]) + 1)#添加数据时，默认优先级为当前的最大优先级+1 self.curr_point += 1 if self.curr_point &gt;= self.capacity: self.curr_point = 0 if self.size &lt; self.capacity: self.size += 1 # 当sample被train后，有了新的TD-Error，就在tree中更新 def update(self,point,weight): tree_idx = point + self.capacity - 1 #这样才能得到最下面的叶子节索引 change = weight - self.tree[tree_idx] self.tree[tree_idx] = weight parent = (tree_idx - 1) // 2 while parent &gt;= 0: #这种方法比递归更快 self.tree[parent] += change parent = (parent -1) // 2 def total_p(self): return self.tree[0] #获取所有的叶子节点之和 #获取最小的优先级，在计算重要性比率中将会使用 def get_min(self): return min(self.tree[self.capacity - 1: self.capacity + self.size -1]) #根据一个权重进行抽样 def sample(self,v): idx = 0 while idx &lt; self.capacity-1: l_idx = idx * 2 +1 r_idx = l_idx +1 if self.tree[l_idx] &gt;= v: idx = l_idx else: idx = r_idx v = v - self.tree[l_idx] point = idx - (self.capacity - 1) #计算叶子节点的索引 return point,self.data[point],self.tree[idx] / self.total_p() 3.2.2 Memory（Prioritized的存储，DQN不采用）123456789101112131415161718192021222324252627282930313233343536373839class Memory(object): def __init__(self,batch_size,max_size,beta): self.batch_size = batch_size #mini_batch的大小 #self.max_size = 2**math.floor(math.log2(max_size)) self.beta = beta self.sum_tree = SumTree(max_size) def store(self,s,a,r,s_,done): transitions = (s,a,r,s_,done) self.sum_tree.add(transitions) #采样samples def get_mini_batches(self): n_sample = self.batch_size if self.sum_tree.size &gt;= self.batch_size else self.sum_tree.size #采样的个数 total = self.sum_tree.total_p() #获取所有TD-Error的和 step = total // n_sample #生成n_sample个区间 points_transitions_probs = [] #在每个区间均匀随机的抽取一个数，并去SumTree中采样 for i in range(n_sample): v = np.random.uniform(i * step,(i +1) * step -1) t = self.sum_tree.sample(v) points_transitions_probs.append(t) points,transitions,probs = zip(*points_transitions_probs) #max_importance_ratio = (n_sample * self.sum_tree.get_min())**-self.beta mini_prob = self.sum_tree.get_min() / total #计算最小的p_i importance_ratio = [pow(probs[i] /mini_prob,-self.beta) for i in range(len(probs))] #重要性采样的权重 #tuple(np.array(e) for e in zip(*transitions)) return points,transitions,importance_ratio #训练完抽取的samples后，要更新tree中的sample的TD-Error def update(self,points,td_error): for i in range(len(points)): td_error += 0.01 #为了防止TD-Error为0，加一个小的数，如果不加，则会发生除0错误 self.sum_tree.update(points[i],td_error[i]) #更新TD-Error 在伪代码中，重要性采样的权重 $w_j = \frac{(N P(j))^{-\beta}}{max_i(w_i)} = \frac{(N P(j))^{-\beta}}{max_i((N * P(i))^{-\beta})} = \frac{(P(j))^{-\beta}}{max_i((P(i))^{-\beta})} = (\frac {p_j}{min_iP(i)})^{-\beta}$ 且伪代码中的第十二行赋值实际是$p_j = （|\delta_i| + \epsilon）^\alpha $，如果采用rank-based，则为$rank(i)^{-\alpha}$，我们的代码中$\alpha$设置为了1，因此才会是 1td_error += 0.01 去更新TD-Error 在莫烦的代码中，$\alpha$设置的为0.6，因此更新为这样 123456def batch_update(self, tree_idx, abs_errors): abs_errors += self.epsilon # convert to abs and avoid 0 clipped_errors = np.minimum(abs_errors, self.abs_err_upper) ps = np.power(clipped_errors, self.alpha) for ti, p in zip(tree_idx, ps): self.tree.update(ti, p) 3.3.3 DQNPrioritizedReplay训练 只说明添加了Prioritized之后的代码部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class DQNPrioritizedReplay: def __init__(self): #计算DQN中的experience数量和大小 self.memory_count = 0 self.memory_size = memory_size self.batch_size = batch_size if self.prioritized: self.memory = Memory(batch_size,memory_size,0.9)#Prioritized采用Memory方式 else: self.memory = deque() #DQN自己定义队列 def build_net(self): if self.prioritized: self.importance_ratio = tf.placeholder(tf.float32,[None,1],name = 'importance_ratio')#重要性采样的权重占位符 def store_transition(self,s,a,r,s_,done): transition = (s,a,r,s_,done) if self.prioritized: self.memory.store(s,a,r,s_,done) else: if self.memory_count &lt; self.memory_size: self.memory.append(transition) self.memory_count += 1 else: self.memory.popleft() self.memory.append(transition) def learn(self): if self.prioritized: points,mini_batch,importance_ratio = self.memory.get_mini_batches() else: # sample batch memory from all memory if self.memory_count &gt; self.batch_size: mini_batch = random.sample(self.memory,self.batch_size) else: mini_batch = random.sample(self.memory,self.memory_count) if self.prioritized: _ = self.sess.run(self.train_op,feed_dict=&#123;self.s:states,self.q_target:q_target,self.importance_ratio:np.array([importance_ratio]).T&#125;) #这里要计算TD-Error td_error = self.sess.run(self.td_error,feed_dict=&#123;self.s:states,self.q_target:q_target,self.importance_ratio:np.array([importance_ratio]).T&#125;) loss = self.sess.run(self.loss,feed_dict=&#123;self.s:states,self.q_target:q_target,self.importance_ratio:np.array([importance_ratio]).T&#125;) #计算完后，要更新samples的TD-Error self.memory.update(points,td_error) else: loss = self.sess.run(self.loss,feed_dict=&#123;self.s:states,self.q_target:q_target&#125;) _ = self.sess.run(self.train_op,feed_dict=&#123;self.s:states,self.q_target:q_target&#125;) 这里的Prioritized DQN和Nature DQN的experience存储方式不同。 但突然想到了一个问题： Nature DQN是在Memory里面有很多个数据，从里面去均匀随机抽取 而Prioritized DQN好像只存储了mini-batch个数据，并且对这些数据进行替换更新，但是在更新过程中，会不会存在将重要的数据给替换掉了呢？如这重要数据就是正面奖励的数据，而用负面奖励数据去替换掉了？ 四、结果显示 在训练的时候，将env.render()注释掉，首先训练过程会很快 其次是没有注释时，小车好像很难到达旗子那里去，不知道env.render()的影响为什么这么大 下面的图片结果都是在注释掉的情况下训练产生的，没有注释时，时间太长，且一直找不到旗子 注释下，几分钟就可以完成训练（CPU） 而没注释，20分钟了可能5个episode都没有完成 （1）DQN下的训练 某一次训练 另一次训练结果 （2）Double DQN下的训练 没有加seed 某一次训练 某一次训练 但是在输出中，步数是不止这么一点的，还不太清楚是为什么没有达到6w步（显示），但是能看出Prioritized 在找到一次正面奖励之后，还是比DQN要训练的快的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128#======================== Double DQN ======================================('current steps:', 26000)('episode:', 0, 'finished')('current steps:', 36000)('episode:', 1, 'finished')('current steps:', 39000)('episode:', 2, 'finished')('current steps:', 41000)('episode:', 3, 'finished')('current steps:', 42000)('episode:', 4, 'finished')('current steps:', 44000)('episode:', 5, 'finished') ('current steps:', 45000)('episode:', 6, 'finished')('current steps:', 48000)('episode:', 7, 'finished')('current steps:', 53000)('episode:', 8, 'finished')('current steps:', 54000)('episode:', 9, 'finished')('current steps:', 55000)('current steps:', 56000)('episode:', 10, 'finished')('episode:', 11, 'finished')('current steps:', 57000)('current steps:', 58000)('episode:', 12, 'finished')('episode:', 13, 'finished')('current steps:', 59000)('current steps:', 60000)('episode:', 14, 'finished')('current steps:', 61000)('episode:', 15, 'finished')('episode:', 16, 'finished')('current steps:', 62000)('episode:', 17, 'finished')('current steps:', 63000)('episode:', 18, 'finished')('episode:', 19, 'finished')# ==============================Double DQN withPrioritized ======================('episode:', 0, 'finished')('current steps:', 33000)('current steps:', 37000)('episode:', 1, 'finished')('current steps:', 38000)('episode:', 2, 'finished')('current steps:', 39000)('episode:', 3, 'finished')('episode:', 4, 'finished')('current steps:', 44000)('episode:', 5, 'finished')('current steps:', 45000)('episode:', 6, 'finished')('current steps:', 46000)('episode:', 7, 'finished')('current steps:', 47000)('episode:', 8, 'finished')('current steps:', 48000)('episode:', 9, 'finished') target_params_replaces ('current steps:', 49000)('episode:', 10, 'finished')('episode:', 11, 'finished')('current steps:', 50000)('episode:', 12, 'finished')('episode:', 13, 'finished')('current steps:', 51000)('episode:', 14, 'finished')('episode:', 15, 'finished')('current steps:', 52000)('episode:', 16, 'finished')('episode:', 17, 'finished')('current steps:', 53000)('episode:', 18, 'finished')('episode:', 19, 'finished') 从图中可以看出，我们都从两种方法最初拿到第一个R += 10奖励的时候算起，看看经历过一次R += 10后，他们有没有好好利用这次的奖励，可以看出，有Prioritized replay的可以高效利用这些不常拿到的奖励，并好好学习他们。所以 Prioritized replay会更快结束每个episode，很快就到达了小旗子。 总结 对代码中seed函数不是太能理解，反正是对随机数产生变化的函数 参考链接 莫烦解析 莫烦代码 Fisher’s 博客 Fisher’s代码 刘建平博客 刘建平代码 论文解析]]></content>
      <tags>
        <tag>DRL</tag>
        <tag>OpenAI gym</tag>
        <tag>Prioritized Experience Replay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码实现（二）之Double DQN]]></title>
    <url>%2F2019%2F08%2F08%2F%E4%BB%A3%E7%A0%81%2FDRL%2F%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8BDouble-DQN%2F</url>
    <content type="text"><![CDATA[前言 理论部分，这里将不再强调，直接强调重点部分（理论部分请点击Double DQN） 经验证，DQN中使用max操作，会使得其Q值会出现过估计的情况，因此提出了DQN的改进算法之一：Double DQN；Double DQN就是将DQN中的目标Q值的动作与计算分离，来减缓过估计情况。 本文的代码，主要参考莫烦大神的代码，只做了少量的修改。 一、实验环境 总共测试了三种环境，其中CartPole和MountainCar的环境，在上一节已经介绍过，这一次，添加了一个Pendulum环境。 1.1 Pendulum环境简单介绍目标说明：详细网址 倒立摆摆问题是对照文献中的经典问题。在这个版本的问题中，钟摆以随机位置开始，目标是将其向上摆动，使其保持直立。 环境介绍：详细网址，pendulum源代码 二、Double DQN在DQN中，其$Y_t^{DQN}$的计算公式为： Y_t^{DQN} \equiv R_{t+1} + \gamma \max_a Q(S_{t+1},a; \theta_t^-)其中，$\theta_t^-$为$Target Q$的网络参数。 正是因为其max操作，使得其Q值可能比真实值要高，导致动作-值函数过估计。 而Double DQN中，则对$Y_t^{DQN}$进行了一个小小的改动，改动后如下： Y_t^{DoubleQ} \equiv R_{t+1} + \gamma Q(S_{t+1},\arg\max_a Q(S_{t+1},a;\theta_t);\theta^-_t)使用Q网络的参数$\theta$来估计贪婪策略的价值，使用$TargetQ$网络的参数$\theta^-$来公平的评估该策略的价值 三、代码部分 Double DQN的代码相比于DQN，只改动了一处。直接查看所有代码 3.1 代码组成 所有代码里面，主要包含6个代码： （1）ddqn.py和ddqn_truth.py ​ 这两个代码几乎完全一致，不同的是，在ddqn.py中，只包含ddqn的算法；而在ddqn_truth.py中，同时包含了ddqn算法和dqn算法，并且添加了求真实Q-values的值 （2）run_CartPole.py ​ 这个和上一节DQN里面的代码基本完全一致，较之前，多了一个测试环节，测试训练的效果，测试差不多能达到300分。并且主要用于展示损失函数图和累计奖励图 （3）run_MountainCar.py ​ 这个就和上一节的完全一致了，除了调用的是DDQN外，就没有区别了 （4）run_CartPole_truth.py和run_Pendulum.py ​ 这两个主函数里面代用的则是ddqn_truth.py函数，最终用于展示其DQN和Double DQN的过估计情况 3.2 网络-ddqn.py 主要用来显示损失函数图和累计奖励图 ddqn.py 1234567891011121314151617q_next = self.sess.run(self.q_next,feed_dict=&#123;self.s_:next_states&#125;)q_eval = self.sess.run(self.q_eval,feed_dict=&#123;self.s:states&#125;)# add q_eval_nextq_eval_next = self.sess.run(self.q_eval,feed_dict=&#123;self.s:next_states&#125;)q_target = q_eval.copy()for k in range(len(mini_batch)): if dones[k]: q_target[k][actions[k]] = rewards[k] else: # dqn # q_target[k][actions[k]] = rewards[k] + self.gamma * np.max(q_next[k]) # double dqn max_action_next = np.argmax(q_eval_next,axis=1)# argmax Q #print('q_next',q_next) q_target[k][actions[k]] = rewards[k] + self.gamma * q_next[k,max_action_next[k]] dqn.py 123456789q_next = self.sess.run(self.q_next,feed_dict=&#123;self.s_:next_states&#125;)q_eval = self.sess.run(self.q_eval,feed_dict=&#123;self.s:states&#125;)q_target = q_eval.copy()for k in range(len(mini_batch)): if dones[k]: q_target[k][actions[k]] = rewards[k] else: q_target[k][actions[k]] = rewards[k] + self.gamma * np.max(q_next[k]) Double DQN 和DQN的代码的主要差别如上，很明显： 首先，在Double DQN中，多了一个Q值：q_eval_next 1q_eval_next = self.sess.run(self.q_eval,feed_dict=&#123;self.s:next_states&#125;) 这一行对应了公式里面的：$Q(S_{t+1},a;\theta_t)$ 值得注意的是，这里仍然用的是Q网络，但是状态为下一状态$S_{t+1}$ 然后，在计算时首先求出$argmax$， 1max_action_next = np.argmax(q_eval_next,axis=1) 这一行则对应了$\arg\max_a Q(S_{t+1},a;\theta_t)$，这样就得到了行为$a$ 最后则是计算$Y_t^{DQN}$ 1q_target[k][actions[k]] = rewards[k] + self.gamma * q_next[k,max_action_next[k]] 通过 1q_next[k,max_action_next[k]] 则直接获取$Q’(s,a)$的值，而不再是用 1np.max(q_next[k]) 获取$\max Q’(s,a)$，要注意这其中的区别 3.3 网络-ddqn_truth.py 把dqn和ddqn算法全部添加进来，并且计算真实Q-values值，用于展示dqn和ddqn与真实Q-values之间的差别的图形 （1）在init函数里面，主要多了double_q和sess的变量，其中double_q的变量是为了判断是否用ddqn，还是dqn。因为要展示两种算法与真实Q-values之间的差异，因此还需要接收主函数输入的sess 12345678910def __init__(self,double_q = True,sess = None): self.double_q = double_q #decide to use double q or not if sess is None: self.sess = tf.Session() self.sess.run(tf.global_variables_initializer()) else: self.sess = sess # record the truth self.q = [] #用来记录真实的Q-values值 self.running_q = 0 #用于计算真实Q-values值 （2）在e-greedy行为选择这里，添加了计算真实Q-values的代码，其真值通过 1self.running_q = self.running_q * 0.99 + 0.01 * np.max(actions_value) 来实现（为什么？还没有去思考） 123456789def choose_action(self,state): actions_value = self.sess.run(self.q_eval,feed_dict=&#123;self.s:state.reshape(-1,self.s_dim)&#125;) self.running_q = self.running_q * 0.99 + 0.01 * np.max(actions_value) self.q.append(self.running_q) if np.random.uniform() &lt; self.epsilon: return np.argmax(actions_value) return np.random.randint(0,self.a_dim) （3）在learn函数里面，通过self.double_q来判断是否用double dqn算法 1234567891011121314def learn(self): # double dqn 需要添加的Q值 q_eval_next = self.sess.run(self.q_eval,feed_dict=&#123;self.s:next_states&#125;) for k in range(len(mini_batch)): if dones[k]: q_target[k][actions[k]] = rewards[k] else: if self.double_q: # double dqn max_action_next = np.argmax(q_eval_next,axis=1)# argmax Q q_target[k][actions[k]] = rewards[k] + self.gamma * q_next[k,max_action_next[k]] else: q_target[k][actions[k]] = rewards[k] + self.gamma * np.max(q_next[k]) 3.4 主函数-run_CartPole.py 和第一节有所不同的是，在训练过程中添加了测试的部分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950total_steps = 0total_reward = []EPISODE = 100STEP = 300TEST = 5for i_episode in range(EPISODE): s = env.reset() ep_r = 0 for step in range(STEP): env.render() a = RL.choose_action(s) s_,r,done,info = env.step(a) # the smaller theta and closer to center the better x, x_dot, theta, theta_dot = s_ r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8 r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5 r = r1 + r2 RL.store_transition(s,a,r,s_,done) ep_r += r if total_steps &gt; 1000: RL.learn() if done: print('episode:',i_episode,'ep_r:',round(ep_r,2),'epsilon',round(RL.epsilon,2),'buffer_size:',RL.memory_count,'steps:',total_steps) total_reward.append(ep_r) break s = s_ total_steps += 1 # Test every 10 episodes if i_episode % 10 == 0: total_rewards = 0 for i in range(TEST): state = env.reset() for j in range(STEP): env.render() action = RL.get_action(state) # direct action for test state,reward,done,_ = env.step(action) total_rewards += reward if done: break ave_reward = total_rewards/TEST print ('episode: ',i_episode,'Evaluation Average Reward:',ave_reward) 3.5 主函数-run_Pendulum.py 显示dqn和ddqn算法相比于真实Q-values差异的图像 需要说一下的是，因为需要计算dqn和ddqn的值，并显示，因此在这里需要创建两个RL，通过double_q来判断使用何种算法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/usr/bin/env python#-*- coding: utf-8 -*-import gymimport numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltfrom ddqn_truth import DDQN env = gym.make('Pendulum-v0')env = env.unwrappedenv.seed(1) # 可重复实验MEMORY_SIZE = 3000ACTION_SPACE = 11 # 将原本的连续动作分离成 11 个动作sess = tf.Session()with tf.variable_scope('Natural_DQN'): natural_DQN = DDQN(s_dim = env.observation_space.shape[0], a_dim = ACTION_SPACE, learning_rate = 0.005, e_greedy = 0.9, replace_target_iter = 200, memory_size = MEMORY_SIZE, e_greedy_increment = 0.001, double_q = False, sess = sess)with tf.variable_scope('Double_DQN'): double_DQN = DDQN(s_dim = env.observation_space.shape[0], a_dim = ACTION_SPACE, learning_rate = 0.005, e_greedy = 0.9, replace_target_iter = 200, memory_size = MEMORY_SIZE, e_greedy_increment = 0.001, double_q = True, sess = sess)sess.run(tf.global_variables_initializer())def train(RL): total_steps = 0 s = env.reset() while True: env.render() a = RL.choose_action(s) f_action = (a-(ACTION_SPACE-1)/2)/((ACTION_SPACE-1)/4) # convert to [-2 ~ 2] float actions s_, r, done, info = env.step(np.array([f_action])) r /= 10 # normalize to a range of (-1, 0). r = 0 when get upright # the Q target at upright state will be 0, because Q_target = r + gamma * Qmax(s', a') = 0 + gamma * 0 # so when Q at this state is greater than 0, the agent overestimates the Q. Please refer to the final result. RL.store_transition(s,a,r,s_,done) if total_steps &gt; MEMORY_SIZE: #learning RL.learn() if total_steps - MEMORY_SIZE &gt; 20000: # stop game break s = s_ total_steps += 1 return RL.q q_natural = train(natural_DQN)q_double = train(double_DQN)plt.plot(np.array(q_natural), c='r', label='natural')plt.plot(np.array(q_double), c='b', label='double')plt.legend(loc='best')plt.ylabel('Q eval')plt.xlabel('training steps')plt.grid()plt.show() 另外，这里的行为维度a_dim为11，是将原本的连续动作离散化为11个动作，至于为什么离散化为11个动作？ 可能是因为在源码里面，将行为限定在了[-8,2]里面，因此输入的有11维 123self.max_speed=8 self.max_torque=2. u = np.clip(u, -self.max_torque, self.max_torque)[0] # 这里的u是输入的行为 而后面又将获取到的行为做下面操作 1f_action = (a-(ACTION_SPACE-1)/2)/((ACTION_SPACE-1)/4) # convert to [-2 ~ 2] 是因为在环境介绍部分说明了行为的界限范围[-2,2]，所以才做的该操作 至于$r = r /10$，将奖励限定在(-1,0)的操作还没太看懂，我觉得应该是除以20才对，在介绍部分说明了奖励的取值范围为[-16,0] 最后，则显示了两个算法与真实Q-values的差异图像 四、结果显示DDQN算法 （1）run_CartPole.py ​ 100个episode的损失函数如和累计奖励图 ​ （2）run_CartPole_truth.py 100个episode训练过车中的动作-值函数的近似 可以看出Double DQN比 nature DQN的值还是要低一些的，即能缓解其过度估计情况，不知道真实值是多少，只是输出来看看效果 （3）run_MountainCar.py 15个episode的损失函数图和累计奖励图 （4）run_Pendulum.py ​ 这个是用莫烦的代码跑的 而下图则是我根据莫烦的代码修改后，测试的，图像当然是差不多的，至少证明了我改写过后的代码是没有问题的（虽然只改动了一点点，但是改为自己习惯的就好） 从这个图中则可以看出，我们的真实Q-values值是0，而Nature DQN在预估的过程中，存在不少高估的情况，改用Double DQN，则会好很多 总结 通过这个实验对Double DQN有了更加深刻的了解，并且对DQN的高估也有了一个较直观的理解，但是对里面的一些参数是如何确定的，这点还是有点疑惑，目前还是主要以理解算法为主 参考链接 莫烦-Double DQN pendulum源码 pendulum环境介绍 pendulum目标介绍]]></content>
      <categories>
        <category>代码</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>OpenAI gym</tag>
        <tag>Double DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码实现（一）之Nature DQN]]></title>
    <url>%2F2019%2F08%2F06%2F%E4%BB%A3%E7%A0%81%2FDRL%2F%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8BNature-DQN%2F</url>
    <content type="text"><![CDATA[前言 主要参考莫烦大神的代码，对OpenAI gym的CartPole环境进行算法验证，所用算法为2015版本的Nature DQN。全部代码 一、CartPole说明 下图中的动态图没有显示出来，详细信息，点击CartPole-V0 杆通过未致动的接头连接到推车，推车沿着无摩擦的轨道移动。通过向推车施加+1或-1的力来控制系统。钟摆开始直立，目标是防止它倒下。每个时间步长都会提供+1的奖励，以保持杆保持直立。当极点与垂直方向相差超过15度时，该episode结束，或者推车从中心移动超过2.4个单位。 CartPole的详细说明：wiki介绍 二、算法伪代码 图片主要来源于草帽B-O-Y的博客 NIPS 2013版本 Nature 2015：本篇笔记中使用的算法 三、代码介绍3.1 代码组成 主要由两部分组成：dqn.py和run_CartPole.py 3.2 DQN实现(dqn.py) 主要包含（1）建cnn基本网络和Target目标网络（2）经验回放池数据存储（3）e-greedy行为选择（4）cnn神经网络训练等四部分 3.2.1 程序框架 主要包含以下几个函数 12345678910111213141516171819202122232425262728293031323334353637import numpy as np import tensorflow as tf import randomfrom collections import dequeclass DQN(object): # 初始化 def __init__(self, s_dim, a_dim, learning_rate = 0.01, reward_decay = 0.9, e_greedy = 0.9, replace_target_iter = 300, memory_size = 500, batch_size = 32, e_greedy_increment = None, output_graph = False): # 创建Q网络和目标网络 def build_net(self): # 存储一个transition def store_transition(self,s,a,r,s_,done): # 根据e-greedy进行行为选择 def choose_action(self,state): # 训练cnn网络，并且隔一定步数进行target参数更新 def learn(self): # 将target参数更新单独的写成了一个函数 def train_target(self): #输出损失值 def plot_cost(self): 3.2.2 初始化12345678910111213141516171819202122232425262728293031323334def __init__(self, s_dim, a_dim, learning_rate = 0.01, reward_decay = 0.9, e_greedy = 0.9, replace_target_iter = 300, memory_size = 500, batch_size = 32, e_greedy_increment = None, output_graph = False ): self.s_dim = s_dim #状态维度（4维） self.a_dim = a_dim #行为维度（2维） self.lr = learning_rate #学习率 self.gamma = reward_decay #折扣因子 self.epsilon_max = e_greedy # e_greedy的上限 self.replace_target_iter = replace_target_iter # 经历C步之后更新target参数 self.memory_size = memory_size #经验回放池大小 self.memory_count = 0 # 经验回放池中的个数 self.memory = deque() # 创建经验回放池 self.batch_size = batch_size # mini-batch self.epsilon_increment = e_greedy_increment #e_greedy的增长速度 self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max # total learning step self.learn_step_counter = 0 #总训练步长 # consist of [target_net,evaluate_net] self.build_net() #创建Q网络和Target Q网络 self.sess = tf.Session() #开始会话 self.sess.run(tf.global_variables_initializer()) self.cost_his = [] #记录损失函数值 3.2.3 创建Q网络和目标网络 Q网络和Target Q网络最大的不同在于Q网络的输入是$s$，而Target Q网络的输入是$s’$，其他的网络结构和初始化参数全部相同，Q网络的输出用self.q_eval表示，Target Q网络的输出用self.q_next表示。中间层设置为20 1234567891011121314151617181920212223242526272829303132333435363738394041def build_net(self): # ************************ build evaluate net ***************************** self.s = tf.placeholder(tf.float32,[None,self.s_dim],name='s_dim') #input self.q_target = tf.placeholder(tf.float32,[None,self.a_dim],name='q_target') #for calculating loss w_initializer = tf.random_normal_initializer(0.,0.3) b_initializer = tf.constant_initializer(0.1) with tf.variable_scope('eval_net'): c_names = ['eval_net_params',tf.GraphKeys.GLOBAL_VARIABLES] with tf.variable_scope('layer1'): w1 = tf.get_variable('w1',[self.s_dim,20],initializer=w_initializer,collections=c_names) b1 = tf.get_variable('b1',[20],initializer=b_initializer,collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s,w1)+b1) with tf.variable_scope('layer2'): w2 = tf.get_variable('w2',[20,self.a_dim],initializer=w_initializer,collections=c_names) b2 = tf.get_variable('b2',[self.a_dim],initializer=b_initializer,collections=c_names) self.q_eval = tf.matmul(l1,w2) + b2 with tf.variable_scope('loss'): self.loss = tf.reduce_mean(tf.squared_difference(self.q_target,self.q_eval)) with tf.variable_scope('train'): self.train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss) # ************************ build target net ***************************** self.s_ = tf.placeholder(tf.float32,[None,self.s_dim],name='s_') #input with tf.variable_scope('target_net'): c_names = ['target_net_params',tf.GraphKeys.GLOBAL_VARIABLES] with tf.variable_scope('layer1'): w1 = tf.get_variable('w1',[self.s_dim,20],initializer=w_initializer,collections=c_names) b1 = tf.get_variable('b1',[20],initializer=b_initializer,collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s_,w1)+b1) with tf.variable_scope('layer2'): w2 = tf.get_variable('w2',[20,self.a_dim],initializer=w_initializer,collections=c_names) b2 = tf.get_variable('b2',[self.a_dim],initializer=b_initializer,collections=c_names) self.q_next = tf.matmul(l1,w2) + b2 3.2.4 经验回放池的数据存储 这里的存储方式和莫烦的存储方式不太一样，当数量小于回放池的大小时，直接添加；若回放池已经存满，则删除掉最开始的第一个数据，在进行添加。 我看的网上的很多代码中都用了one_hot_action来表示行为，也不太清楚其功能，我这里没有用 1234567891011121314def store_transition(self,s,a,r,s_,done): # transition = np.hstack((s,[a,r],s_,done)) # # replace the old memory with new memory # index = self.memory_count % self.memory_size # self.memory[index,:] = transition # one_hot_action = np.zeros(self.a_dim) # one_hot_action[a] = 1 transition = (s,a,r,s_,done) if self.memory_count &lt; self.memory_size: self.memory.append(transition) self.memory_count += 1 else: self.memory.popleft() self.memory.append(transition) 3.2.5 e-greedy行为选择 以一定几率选择行为，随训练的进行，逐渐减小随机的可能性，选择最大的值 由于是采用的epsilon增加的方式，因此，当小于epsilon时，从Q网络中进行选取，否则，随机选取 这里添加了一个.reshape(-1,self.s_dim)，是因为在创建网络时，s_dim的维度已经用占位符更改，而这里接收到的state是直接从env环境中获取的，没有进行任何更改，如果不添加reshape，则会报维度不匹配的错误。 123456def choose_action(self,state): if np.random.uniform() &lt; self.epsilon: return np.argmax(self.sess.run(self.q_eval,feed_dict=&#123;self.s:state.reshape(-1,self.s_dim)&#125;)) return np.random.randint(0,self.a_dim) 3.2.6 训练cnn网络 首先，检查Target网络是否需要更新，若达到相应的步数了，则更新； 其次，在从回放池中获取数据时，要进行一下判断，判断当前的回放池数据n是否已经超多mini-batch，若没有，则直接随机采取n个数据，反之，则用mini-batch进行随机采样； 最后，通过mini-batch求期望获取Q值和Target Q值，并根据这两个值求Loss，并进行优化 1234567891011121314151617181920212223242526272829303132333435def learn(self): # cheak ro replace target parameters if self.learn_step_counter % self.replace_target_iter == 0: self.train_target() print('\n target_params_replaces \n') # sample batch memory from all memory if self.memory_count &gt; self.batch_size: mini_batch = random.sample(self.memory,self.batch_size) else: mini_batch = random.sample(self.memory,self.memory_count) states = np.asarray([e[0] for e in mini_batch]) actions = np.asarray([e[1] for e in mini_batch]) rewards = np.asarray([e[2] for e in mini_batch]) next_states = np.asarray([e[3] for e in mini_batch]) dones = np.asarray([e[4] for e in mini_batch]) q_next = self.sess.run(self.q_next,feed_dict=&#123;self.s_:next_states&#125;) q_eval = self.sess.run(self.q_eval,feed_dict=&#123;self.s:states&#125;) q_target = q_eval.copy() for k in range(len(mini_batch)): if dones[k]: q_target[k][actions[k]] = rewards[k] else: q_target[k][actions[k]] = rewards[k] + self.gamma * np.max(q_next[k]) loss = self.sess.run(self.loss,feed_dict=&#123;self.s:states,self.q_target:q_target&#125;) self.cost_his.append(loss) self.sess.run(self.train_op,feed_dict=&#123;self.s:states,self.q_target:q_target&#125;) # 随着训练时间增加，到后面就更大概率选择最大的action self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 3.2.7 目标网络参数更新1234def train_target(self): t_params = tf.get_collection('target_net_params') e_params = tf.get_collection('eval_net_params') self.sess.run([tf.assign(t,e) for t,e in zip(t_params,e_params)]) 3.3 主函数（run_CartPole.py）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#!/usr/bin/env python#-*- coding: utf-8 -*-import gymfrom dqn import DQN env = gym.make('CartPole-v0') #使用CartPole-v0环境env = env.unwrapped #最好是添加， 不做这个会有很多限制print(env.action_space) # 查看这个环境中可用的 action 有多少个print(env.observation_space) # 查看这个环境中可用的 state 的 observation 有多少个print(env.observation_space.high) # 查看 observation 最高取值print(env.observation_space.low) # 查看 observation 最低取值#定义使用DQN算法，设定缓冲区即epsilon增长速度值RL = DQN(s_dim = env.observation_space.shape[0], a_dim = env.action_space.n, learning_rate = 0.01, e_greedy = 0.9, replace_target_iter = 100, memory_size = 2000, e_greedy_increment = 0.001)total_steps = 0 #记录步数total_reward = [] #记录累计奖励for i_episode in range(200): #迭代100次，200次，300次都可以 s = env.reset() # 获取回合 i_episode 第一个 observation ep_r = 0 while True: env.render() # 刷新显示环境 a = RL.choose_action(s) # 根据状态选行为 s_,r,done,info = env.step(a) # 获取下一个 state # the smaller theta and closer to center the better x, x_dot, theta, theta_dot = s_ # 细分开, 为了修改原配的 reward # x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少 r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8 # theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高 r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5 r = r1 + r2 # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率 RL.store_transition(s,a,r,s_,done) # 保存这一组记忆 ep_r += r # 统计当前episode的累计奖励 if total_steps &gt; 1000: RL.learn() #学习训练 if done: print('episode:',i_episode,'ep_r:',round(ep_r,2),'epsilon',round(RL.epsilon,2),'buffer_size:',RL.memory_count,'steps:',total_steps) total_reward.append(ep_r) break s = s_ total_steps += 1RL.plot_cost() #输出cost曲线#输出reward曲线import matplotlib.pyplot as pltimport numpy as npplt.plot(np.arange(len(total_reward)),total_reward)plt.ylabel('Total Reward')plt.xlabel('Episode ')plt.show() 3.4 全部代码 直接查看所有代码 四、结果显示100个episode（某一次）的cost 100个episode（另一次）的cost和reward 200个episode（另一次）的cost和reward 五、MountainCar例子5.1 MountainCar问题说明 MountainCar 汽车位于一条轨道上，位于两个“山脉”之间。目标是在右边开山;然而，汽车的发动机强度不足以在一次通过中攀登山峰。因此，成功的唯一途径是来回驾驶以增强动力。 MountainCar环境介绍 5.2 代码部分 dqn的代码部分不变，变化的知识主函数的代码，和run_CartPole.py的代码很类似，但有点小区别 run_MountainCar.py 下面说下run_MountainCar.py和run_CartPole.py的不同之处 （1）第一处不同（注释的为CartPole，没注释的为MountainCar） 首先，环境不同，因此状态维度和行为维度不相同，这个很正常。但是这里的学习率、目标网络更新步数、回放池大小、探索的增长速度值全部不相同。 123456789101112131415# RL = DQN(s_dim = env.observation_space.shape[0],# a_dim = env.action_space.n,# learning_rate = 0.01,# e_greedy = 0.9,# replace_target_iter = 100,# memory_size = 2000,# e_greedy_increment = 0.001)RL = DQN(s_dim = env.observation_space.shape[0], a_dim = env.action_space.n, learning_rate = 0.001, e_greedy = 0.9, replace_target_iter = 300, memory_size = 3000, e_greedy_increment = 0.0002) （2）第二处不同（奖励不同） MountainCar的奖励设置为position，这里加减是为了限定在[0,1]范围内。和前面一样，不同的环境奖励也不相同，这个奖励一般也还好设置。 1r = abs(position - (-0.5)) # r in [0,1] 5.3 结果显示 我这里的cost的显示图和莫烦的cost的显示图不太一样，但其实在DRL算法中，看中的是累计奖励，即第二幅图和第四幅图。可以看出，累计奖励也是在慢慢的平缓，因为训练到后面，已经知道如何到达目标点，会比较快速的到达，而不用左右来回摇摆，因此是一个下降趋势。 10个episode 20个episode 总结 到后面的时候，cost下降的还是较为明显，但仍然有波动，这可能是随机选择行为导致的结果，从模型中也可以看出有时候不太稳定，因此该算法还需要改进。在实际中，要看重累计奖励而不是损失函数。 从上面的两个不同例子中可以看出，不同的环境，在主函数中，学习率、回放池大小、探索的增长速度值、目标网络更新步数都会有所不同，甚至，在dqn.py中也会改变其网络结构，但是这些值又是如何试验找出一个比较合适的值的呢？这是一个问题 参考链接 莫烦 CartPole源代码 CartPole详细介绍 MountainCar详细介绍 MountainCar源代码]]></content>
      <categories>
        <category>代码</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>OpenAI gym</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建OpenAI gym环境并简单测试]]></title>
    <url>%2F2019%2F08%2F05%2F%E4%BB%A3%E7%A0%81%2FDRL%2F%E6%90%AD%E5%BB%BAOpenAI-gym%E7%8E%AF%E5%A2%83%E5%B9%B6%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[前言 搭建OpenAI gym环境的目的是为了后面的一系列DRL的强化学习代码的实现，因为OpenAI gym提供了很多的环境，并且训练较快，不用耗很长时间才能看到效果，因此是一个不错的环境。 我已经在gazebo中基于ardrone实现了一个关于DQN算法的验证，由于机器性能及其他的原因，耗时很长。并且也在airsim中做了一个简单的DQN实验训练，训练结果很好。但是也正是由于目前只会DQN的算法，而其他的DRL算法不会，因此才搭建该环境用于编写DRL算法，以进一步加深自己的理解。 一、安装gym环境 我这里采用的是pip方式安装，当然也可以git源码进行安装 1sudo pip install gym 二、简单测试coding: 1234567891011121314#!/usr/bin/env python#-*- coding: utf-8 -*-import gymenv = gym.make('CartPole-v0') #定义使用gym库中的哪一个环境env.reset() #重置环境的状态，返回观察for i in range(1000): env.render() #重绘环境的一帧。默认模式一般比较友好，如弹出一个窗口 env.step(env.action_space.sample()) #从行为集中，随机选取一个行为并执行print('action_space',env.action_space) #查看这个环境中可用的action有多少个print('observation',env.observation_space) #查看这个环境中可用的observation有多少个print('max',env.observation_space.high) #查看observation最高取值print('min',env.observation_space.low) #查看observation最低取值 输出： 123456('action_space', Discrete(2))('observation', Box(4,))('max', array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38], dtype=float32))('min', array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38], dtype=float32)) 三、带返回值的测试coding： 1234567891011121314#!/usr/bin/env python#-*- coding: utf-8 -*-import gymenv = gym.make('CartPole-v0')observation = env.reset()for i in range(1000): env.render() action = env.action_space.sample() observation,reward,done,info = env.step(action) print('observation',observation) print('reward',reward) print('action',action) print('info',info) 最后部分输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344('observation', array([ -11.94872903, -0.1539066 , -130.92118709, -11.77476314]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -11.95180716, 0.13505849, -131.15668235, -11.74811091]))('reward', 0.0)('action', 1)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([-1.19491060e+01, 3.22343601e-02, -1.31391645e+02, -1.14306883e+01]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([-1.19484613e+01, -9.94364542e-02, -1.31620258e+02, -1.11078703e+01]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -11.95045003, -0.25909084, -131.84241574, -10.78671897]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -11.95563185, -0.44371529, -132.05815012, -10.48063232]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -11.96450615, -0.64896802, -132.26776277, -10.2072994 ]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -11.97748551, -0.48282207, -132.47190876, -10.53652414]))('reward', 0.0)('action', 1)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -11.98714196, -0.72162235, -132.68263924, -10.37392887]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -12.0015744 , -0.5940356 , -132.89011782, -10.71312018]))('reward', 0.0)('action', 1)('info', &#123;'TimeLimit.truncated': False&#125;)('observation', array([ -12.01345511, -0.86021586, -133.10438022, -10.7166571 ]))('reward', 0.0)('action', 0)('info', &#123;'TimeLimit.truncated': False&#125;) 四、保存训练视频前提：需要安装一些依赖库，否则会报错 1sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig coding: 123456789101112131415161718#!/usr/bin/env python#-*- coding: utf-8 -*-import gymenv = gym.make('CartPole-v0')env = gym.wrappers.Monitor(env,"CartPole-experiment-1",video_callable=lambda episode_id: episode_id % 5 == 0,force=True)for episode in range(20): observation = env.reset() for t in range(100): env.render() print('observation',observation) action = env.action_space.sample() observation,reward,done,info = env.step(action) if done: breakenv.close() 运行完后，会在同级目录生成一个CatPole-experiment-1的文件夹，文件夹里的文件内容如下： 打开其中的MP4时，提示无法打开，然后按照提示默认安装即可 注意： video_callable=lambda episode_id: episode_id % 5 == 0：是每隔5秒保存一次，因此文件夹中有4个MP4视频，但是每个视频都很短暂，每个episode保存视频的话，参考这里 force=True：是每次运行代码时，将之前生成的json文件和MP4文件直接覆盖掉 目前对这些json文件什么的还不是太清楚，总之是能保存一些训练的视频的。 总结 这部分主要是做一个OpenAI gym的环境安装及简单测试，并且测试的代码能够看到图形和实际运动的效果即可。后面将开始DRL的代码编写。 参考链接 莫烦OpenAI gym环境库 OpenAI Gym基础教程 gym官网 cartpole官方源码]]></content>
      <categories>
        <category>代码</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>OpenAI gym</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（九）之A3C算法]]></title>
    <url>%2F2019%2F08%2F05%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E4%B9%9D%EF%BC%89%E4%B9%8BA3C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言 针对普通AC算法中比较难收敛的问题，DeepMind在Asynchronous Methods for Deep Reinforcement Learning一文中，介绍了另一种简单的轻量级深度强化学习框架——异步地进行深度神经网络梯度下降。论文提出了四种标准强化学习算法的异步变体，分别是Asynchronous one-step Q-Learning，Asynchronous one-step Sarsa，Asynchronous n-stepQ-Learning，Asynchronous Advantage Actor-Critic，都取得了不错的稳定效果，尤其是最后一个，简称A3C，更是有着最好的性能表现，在多核CPU上学习比在GPU上学习的时间减少了一半。 一、论文题目 Asynchronous Methods for Deep Reinforcement Learning 二、研究目标 研究一种新的算法架构解决普通AC算法中难收敛的问题及深度神经网络训练中的不稳定性（不使用经验回放机制） 三、问题定义 以往简单的online RL算法与深度神经网络结合是不稳定的，因此许多人提出了多种解决方案来稳定其算法，这些方法有一个共同的想法：一个online agent遇到的观测数据序列是non-stationary 的，online RL的更新是强相关的。因此通过将agent的数据存储在经验回放池（experience replay buffer）中，数据可以从不同的时间步骤上，批处理或随机采样。这种方法可以降低non-stationary和其更新的相关性。但同时也将这些算法限制在了off-policy的RL算法 。 使用经验回放池的方法来解决其不稳定性的算法有DQN、DDPG等。 但是经验回放池存在两个问题： agent与环境的每次实时交互都需要耗费很多的内存和计算力 经验回放机制要求agent采用异策略（off-policy）方法来进行学习，而off-policy方法只能基于就策略生成的数据进行更新 经验回放有什么问题呢？回放池数据相关性太强，用于训练的时候效果很可能不佳。举个例子，我们学习下期，总是和同一个人下，期望能提高棋艺。这当然没有问题，但是到一定程度就很难在提高了，此时最后的方法就是另寻高手切磋。 此外，以往DRL的训练都依赖于计算能力很强的图形处理器（如GPU） 四、A3C算法介绍 A3C算法全称为Asynchronous Advantage Actor-Critic，将AC放到多个线程中同步训练，可以有效的利用计算机资源，提升训练效用。简单概括，就是：它会创建多个并行的环境，让多个拥有副结构的agent同时在这些并行环境上更新主结构中的参数。并行中的agent们互不干扰，而主结构的参数更新受到副结构提交更新的不连续性干扰，所以更新的相关性被降低，收敛性提高。 4.1 背景知识 DRL算法大致可以分为两个类别：Value-Based和Policy-Based，Value-Based的经典算法为Q-Learning，Policy-Based的经典算法为Policy Gradient Method Value-Based：是预测某个state下所有Action的期望价值（Q值），之后通过选择最大Q值对应的Action执行策略，适合仅有少量离散取值的Action的环境 Policy-Based：是直接预测某个state下应该采取的Action，适合高维连续Action的环境，更通用 根据是否对state的变化进行预测，RL又可以分为model-based和model-free： model-based：根据state和采取的action预测接下来的state，并利用这个信息训练强化学习模型（知道状态的转移概率） model-free：不需要对环境状态进行任何预测，也不考虑行动将如何影响环境，直接对策略或action的期望价值进行预测，计算效率非常高 因为复杂环境中难以使用model预测接下来的环境状态，所以传统的DRL都是基于model-free。 4.1.1 基本概念 $s_t $：在t时刻，agent观察到的环境状态，如观察到的环境图像，或agent在环境中的位置、速度、机器人关节角度等 $a_t$：在t时刻，agent选择的行为（action），通过环境执行后，环境状态由$s_t$转换为$s_{t+1}$ $r(s_t,a_t)$函数（转移函数）：环境在$s_t$执行行为$a_t$后，返回的单步奖励值 $R_t$：$：是从当前状态直到将来某个状态，期间所有行为所获得奖励值的加权总和，即discounted future reward R_t = \sum_{i=t}^T \gamma^{i-t} r(s_i,a_i)其中，$\gamma$是discounted rate，表示折扣因子，$\gamma \epsilon[0,1]$，通常取0.99 动作-值函数： Q^\pi(s,a) = E[R_t|s_t = s,a] 状态价值函数： V^\pi(s) = E[R_t|s_t = s ] 4.1.2 Value-Based &amp; model-free神经网络近似动作-值函数：使用参数$\theta$来对动作-值函数进行近似$Q(s,a,\theta)$ one-step Q-Learning： ​ 损失函数Loss Function： L_i(\theta_i) = E(r + \gamma \cdot max_{a'}Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i))^2​ 其中，$r + \gamma \cdot max_{a’}Q(s’,a’;\theta_{i-1}^-)$一般称为Target Q网络 n-step Q-Learning： ​ $Target Q = r_t + \gamma r_{t+1} + … +\gamma ^{n-1}r_{t+n-1} + \gamma^n max_{a’}Q(s’,a’;\theta_{i-1}^-)$ one-step缺点：得到一个奖励r仅仅直接影响了得到该奖励的状态总做对（s,a）的值。其他state action pair的值仅仅间接的通过更新value Q(s,a)来影响，这使得学习过程缓慢，因为许多更新都需要传播一个reward给相关进行的states和actions。 n-step优点：一个奖励r直接影响先前n个state action pair，学习更有效。 4.1.3 Policy-Based &amp; model-free直接将策略参数化：$\pi(a|s;\theta)$，通过迭代更新$\theta$，使总奖励期望$E[R_t]$梯度上升 具体地： ①中，$\pi(a_t|s_t;\theta)$表示在$s_t,\theta$的情况下选择动作$a_t$的概率。概率的对数乘以该动作的总奖励$R_t$，对$\theta$求梯度，以梯度上升的方式更新$\theta$。该公式的意义在于，奖励越高的动作越努力提高它出现的概率。 但是在某些情况下，每个动作的总回报$R_t$都不为负，那么多有的梯度值都大于等于0，此时每个动作出现的概率都会提高，这在很大程度上减缓了学习的速度，而且使得梯度的方差也很大。因此需要对$R_t$使用某种标准化操作来降低梯度的方差。 ②中，可以让$R_t$减去一个基线b(baseline)，b通常设为$R_t$的一个期望估计，即$b_t(s_t) \approx V^\pi(s_t)$，通过求梯度更新$\theta$，总奖励超过基线的动作的概率会提高，反之则降低，同时块还可以降低梯度方差（证明略）。这种方式被叫做Actor-Critic体系结构，其中策略$\pi$是actor，基线$b_t$是critic。 ③中，$R_t - b_t(s_t)$可以使用动作优势函数$A^\pi(a_t,s_t) = Q^\pi(a_t,s_t) - V^\pi(s_t)$代替，因为$R_t$可以视为$Q^\pi(a_t,s_t)$的估计，基线$b_t(s_t)$视为$V^\pi(s_t)$的估计。 4.2 异步RL框架 本文提出了多线程的各种算法的异步变种，即：one-step Q-Learning，one-step Sarsa，n-step Q-Learning，以及advantage actor-critic。设计这些算法的目的是找到RL算法可以训练深度神经网络策略而不用花费太多的计算资源，但是RL算法又不尽相同，actor-critic算法是on-policy策略搜索算法，但是Q-Learning是off-policy value-based方法，我们利用两种主要的想法来实现四种算法以达到我们的目标。 利用异步actor-learners，利用单机的多CPU线程，将这些learner保持在一个机器上，省去了多个learner之间通信的开销，使得可以利用Hogwild!的方式更新而完成训练 利用multiple actor-learners并行性尽可能的去探索环境的不同部分 不同线程的agent，其探索策略不同以保证多样性，不需要经验回放机制，通过各并行agent收集的样本训练降低样本相关性，且学习的速度和线程数大约成线性关系，能适用off-policy、on-policy、离散型、连续性动作。 4.2.1 Asynchronous one-step Q-Learning 算法大致流程：有两个全局的$Q(s,a;\theta)$和$Q(s,a;\theta^-)$，每个线程独立的以$\epsilon - greedy$方式进行行为探索，然后根据Q-Learning算法更新$\theta$参数，但这里的更新不是直接将更新提交到全局$Q(s,a;\theta)$网络中，而是先自己保留更新的梯度，在每隔一段时间，将自己的梯度更新到全局神经网络中，同时每隔一段时间将全局神经网络参数同步到自己的参数中。 4.2.2 Asynchronous one-step Sarsa 相比较Q-Learning，Sarsa只有一处不同，即$TargetQ = r+\gamma Q(s’,a’;\theta^-)$; 且Sarsa是on-policy，Q-Learning是off-policy 4.2.3 Asynchronous n-step Q-Learning 常见的情况下，一般会用后向视角（backward view），即用资格迹（eligibility traces）来更新，但这个算法用不了不大常见的正向视角（forward view），作者解释因为再以动量梯度更新的方式训练神经网络和反向传播过程中，正向视角更加简单。 4.2.4 A3C（Asynchronous advantage actor-critic） A3C的更新公式有两条，一条梯度上升更新策略$\pi$的参数，如前面介绍的actor-critic结构： \nabla_{\theta'} log \pi (a_t|st;\theta')A(s_t,a_t;\theta',\theta'_v)其中，$A(s_t,a_t;\theta’,\theta’_v)$是优势函数的估计（算法中表示为$R-V(s_i;\theta’_v)$） A(s_t,a_t;\theta',\theta'_v) = \sum_{i=0}^{k-1} \gamma^i r_{t+i} +\gamma^kV(s_{t+k};\theta'_v) -V(s_t;\theta'_v)$\theta’$是策略$\pi$的参数，$\theta’_v$是状态值函数的参数。k是可以变化的，上届由n-step决定，即n。 在实际操作中，论文在该公式中加入了策略$\pi$的熵项$\beta\nabla_{\theta’}H(\pi(s_t;\theta’))$，防止过早的进入次优策略。 另一条公式是使用TD方式梯度下降更新状态值函数的参数，即算法中的$\partial (R-V(s_i;\theta’_v))^2 / \partial\theta’_v$ 注意： 上面四个算法都是等所有的异步agent执行完后再用累计的梯度信息更新网络参数。其中n-step的算法（后两个）需要每个agent复制一份子网络，每个agent执行n步后倒退算出每步的总奖励和相关梯度，用累计梯度更新更新主网络参数（如果不复制子网络，则等单个agent执行完n-step耗时太多，而one-step可忽略这个影响）。 总结 A3C解决了Actor-Critic难以收敛的问题，同时更重要的是，它提供了一种通用的异步的并发的强化学习框架，这个框架不止可以用于A3C，还能应用于其他的强化学习算法。 参考链接 本文主要参考了下面链接中的前两篇 深度强化学习—A3C Fisher’s Blog博客 强化学习（十五）A3C]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>A3C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（八）之DDPG算法]]></title>
    <url>%2F2019%2F08%2F04%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E5%85%AB%EF%BC%89%E4%B9%8BDDPG%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言 DDPG （Deep Deterministic Policy Gradient）算法是一种model-free（无环境模型），off-policy（产生行为的策略和进行评估的策略不一样）的强化学习算法，且使用了深度神经网络用于函数近似。相比较于DQN（model-free、off-policy），DQN只能解决离散、低维的动作空间，而DDPG可以解决连续动作空间的问题。 背景： RL要解决的问题：让agent学习在一个环境中如何选择行为动作（action），从而获得最大的奖励值总和（total reward）。这个奖励值一般与agent定义的任务目标关联 agent的主要学习内容：第一是行为策略（action policy），第二是规划（planning）。其中行为策略的学习目标是最优策略，也就是使用这样的策略，可以让agent在特定的环境中的行为获得最大的奖励值，从而实现其目标任务。 行为（action）划分： 连续的：如赛车游戏中的方向盘角度、油门、刹车控制信号，机器人的关节伺服电机控制信号 离散的：如围棋，贪吃蛇游戏。Alpha Go就是一个典型的离散行为agent DDPG是针对连续行为的策略学习方法 一、论文题目 CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING 二、研究目标 研究一种新算法，解决Actor-Critic算法中难收敛和DQN算法中难以解决连续高维空间的问题 三、问题定义 DQN：只能处理离散的、低维的动作空间 DQN不能直接处理连续动作空间的原因是因为它依赖于在每一次最优迭代中寻找动作-值函数的最大值（变现为在Q神经网络中输出每个动作的值函数），针对连续动作空间的DQN没有办法输出每个动作的动作-值函数 如：一个具有6个关节的机械臂，每个关节的角度输出是连续值，假设范围是0~360度，归一化为（-1,1）。若把每个关节角取值范围离散化，比如精确到0.01，则一个关节有200个取值，那么6个关节共有$200^6$个取值，若进一步提升这个精度，取值的数量将成倍增加，而且动作的数量将随着自由度的增加成指数型增长。因此在连续的高维的控制问题中，无法用传统的DQN方法解决。 Actor-Critic：DPG中的Actor-Critic算法相对较难以收敛 四、DDPG算法介绍 DDPG组成：D（Deep） + D（Deterministic） + PG（Policy Gradient） Deep：更深层次的网络结构 PG：策略梯度算法，能够在连续的动作空间根据所学习到的策略（动作分布）随机筛选动作 Deterministic：它的作用就是用来帮助PG不让它随机选择，只输出一个动作值 4.1 随机性策略和确定性策略 随机性策略：策略输出的是动作的概率，使用一个正态分布对动作进行采样选择，即每个动作都有概率被选到 输出：动作的概率，$\pi_\theta(a|s) = P[a|s;\theta]$ 优点：将策略探索和策略改进集中在一个策略中 缺点：需要大量的训练数据 确定性策略：策略的输出是动作 输出：确定的动作，$\pi_\theta(s) = a$ 优点：需要采样的数据少 缺点：无法探索环境 在真实场景下机器人的操控任务中，在线收集并利用大量训练数据会产生十分昂贵的代价，并且动作连续的特性使得在线抽取批量轨迹的方式无法达到令人满意的覆盖面，这些问题会导致局部最优解的出现。 然后使用确定性策略无法探索环境，如何解决？ 利用off-policy学习方法：off-policy是指采样的策略和改进的策略不是同一个策略。类似于DQN，使用随机策略产生样本存放到经验回放机制中，训练时抽取样本，改进的是当前的确定性策略。这个确定性策略的学习框架采用的是AC方法。 4.2 基本概念 $s_t$：在t时刻，agent观察到的环境状态，如观察到的环境图像，或agent在环境中的位置、速度、机器人关节角度等 $a_t$：在t时刻，agent选择的行为（action），通过环境执行后，环境状态由$s_t$转换为$s_{t+1}$ $r(s_t,a_t)$函数（转移函数）：环境在$s_t$执行行为$a_t$后，返回的单步奖励值 上面的关系可以用一个状态转换图来表示： $R_t$：是从当前状态直到将来某个状态，期间所有行为所获得奖励值的加权总和，即discounted future reward R_t = \sum_{i=t}^T \gamma^{i-t} r(s_i,a_i)其中，$\gamma$是discounted rate，表示折扣因子，$\gamma \epsilon[0,1]$，通常取0.99 动作-值函数（Q值）： Q^\pi(s_t,a_t) = E_{s\sim E,a\sim \pi}[R_t|s_t,a_t]用贝尔曼公式表达为： Q^\pi(s_t,a_t) = E_{r_t,s_{t+1}\sim E}[r(s_t,a_t) + \gamma E_{a_{t+1} \sim \pi}[Q^\pi(s_{t+1},a_{t+1})]] 4.3 策略梯度4.3.1 Policy Gradient（随机策略梯度） 通过一个概率分布函数$\pi_\theta(s_t|\theta^\pi)$，来表示每一步的最优策略，在每一步根据该概率分布进行action采样，获取当前的最佳action取值，即：$a_t \sim \pi_\theta(s_t|\theta^\pi)$ 生成action的过程，本质上是一个随机过程，最后学习到的策略，也是一个随机策略（Stochastic policy） 随机策略梯度的梯度计算公式为： \begin{split} \nabla_\theta J(\pi_\theta) &= \int_S \rho^\pi(s)\int_A\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a)dads \\ &=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(a|s)Q^\pi(s,a)] \end{split}其中，$\rho^\pi$表示状态的采样空间，$\nabla_\theta log\pi_\theta(a|s)$是socre function，可以看出随机策略梯度需要在整个动作空间$\pi_\theta$进行采样 4.3.2 DPG（确定性策略梯度） DPG每一步的行为通过函数$\mu$直接获得确定的值：$a_t = \mu(s_t|\theta^\mu)$ 这个函数$\mu$即最优行为策略，不在是一个需要采样的随机策略。 为何需要确定性策略？简单来说，PG方法有以下缺陷： 即使通过PG学习到了随机策略之后，在每一步行为时，我们还需要对得到的最优策略概率分布进行采样，这样才能获得action的具体值；而action通常是高维的向量，比如25维、50维，在高维的action空间中频繁采样，无疑是很耗费计算能力 在PG的学习过程中，每一步计算policy gradient都需要在整个action 空间中进行积分： \nabla_\theta J(\pi_\theta) = \int_S \rho^\pi(s)\int_A\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a)dads​ 这个积分部分一般通过蒙特卡洛采样来进行估算，需要在高维的action空间进行采样，耗费计算能力 但如果采取简单的Greedy策略，即每一步求解$argmax_aQ(s,a)$也不可行，因为在连续的、高维度的action空间中，如果每一步都求全局最优解，太耗费计算性能。 在这之前，业界普遍认为，环境模型无关（model free）的确定性策略是不存在的，在DPG的论文中，被证明存在。然后将DPG算法融合进AC框架，结合Q-learning或Gradient Q-learning这些传统的Q函数学习方法，经过训练得到一个确定性的最优行为策略函数。 确定性策略梯度的梯度计算公式（on-policy）为： \begin{split} \nabla_\theta J(\mu_\theta) &= \int_S \rho^\mu(s)\nabla_\theta \mu_\theta(s) \nabla_aQ^\mu(s,a)|_{a= \mu_\theta(s)}ds \\ &= E_{s\sim\rho^\mu}[\nabla_\theta \mu_\theta(s) \nabla_aQ^\mu(s,a)|_{a= \mu_\theta(s)}] \\ &= E_{s\sim\rho^\mu}[\nabla_\theta Q^\mu(s, \mu_\theta(s))] \end{split}跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数 4.4 DDPG DDPG是将深度学习神经网络融合进DPG的策略学习方法。相对于DPG的核心改进是：采用卷积神经网络作为策略函数$\mu$和$Q$函数的模拟，即策略网络和$Q$网络；然后使用深度学习的方法来训练上述神经网络。 Q函数的实现和训练方法，采用了DQN方法 4.4.1 DDPG相关概念 策略改善：用来更新策略，对应AC框架中的actor 确定性行为策略$\mu$：定义一个函数，每一步的行为可以通过$a_t = \mu(s_t)$计算获得 策略网络：用一个卷积神经网络对$\mu$函数进行模拟，这个网络我们称为策略网络，其参数为$\theta^\mu$ behavior policy（行为策略）$\beta $：在RL训练过程中，我们要兼顾2个e：exploration和exploit；exploration的目的是探索潜在的更优策略，所以训练过程中，我们为action的决策机制引入随机噪声： 将action的决策从确定性过程变为一个随机过程，再从这个随机过程中采样得到action。下达给环境执行 过程如下如所示： 以上这个策略叫做behavior策略，用$\beta$来表示，这时RL的训练方式叫做off-policy。 DDPG中，使用Uhlenbeck-Ornstein随机过程（简称UP过程），作为引入的随机噪声：UO过程在时序上具备很好的相关性，可以使agent很好的探索具备动量属性的环境。 注意： 这个$\beta$不是我们想要得到的最优策略，仅仅是在训练过程中，生成下达给环境的action，从而获得我们想要的数据集，比如状态转换（transitions）、或者agent的行走路径等，然后利用这个数据集去训练策略$\mu$，以获得最优策略 在test和evaluation时，使用$\mu$而不会再使用$\beta$ 策略评估：用来逼近动作-值函数，并提供梯度信息，对应AC框架中的critic 动作-值函数（Q函数）：即action-value函数，定义在状态$s_t$下，采取动作$a_t$后，且如果继续执行策略$\mu$的情况下，所获得的$R_t$期望值，用贝尔曼等式来定义： Q^\mu(s_t,a_t) = E[r(s_t,a_t) + \gamma Q^\mu(s_{t+1},\mu(s_{t+1}))]该公式的期望只与环境有关，所以用off-policy来学习$Q^\mu$，即用一个不同的随机策略$\beta$来生成状态行为轨迹。 Q网络：DDPG中，我们用一个卷积神经网络对Q函数进行模拟，这个网络我们称为Q网络，其参数为$\theta^Q$，采用了DQN相同的方法 如何评估一个策略$\mu$的表现：用一个目标函数$J$来评估，我们叫做performance objective，针对off-policy学习的情况，目标函数定义如下： \begin{split} J_\beta(\mu) &= \int_S \rho^\beta(s) Q^\mu(s,\mu(s))ds \\ &= E_{s \sim \rho^\beta}[Q^\mu(s,\mu(s))] \end{split}其中： s是环境的状态，这些状态（或者说agent在环境中走过的状态路径）是基于agent的behavior策略产生的，它们的分布函数为$\rho^\beta$ $Q^\mu(s,\mu(s))$是在每个状态下，如果都按照$\mu$策略选择action时，能够产生的Q值，即$J_\beta(\mu)$是在$s$根据$\rho ^\beta$分布时，$Q^\mu(s,\mu(s))$的期望值 训练的目标：最大化$J_\beta(\mu)$，同时最小化Q网络的Loss（后面给出） 最优行为策略$\mu$的定义：即最大化$J_\beta(\mu)$的策略： \mu = argmax_\mu J(\mu)训练$\mu$网络的过程，就是寻找$\mu$网络参数$\theta^\mu$的最优解的过程，我们使用SGA（stochastic gradient asent）的方法，即随机梯度上升方法。 最优Q网络定义：具备最小化的Q网络Loss； 训练Q网络的过程，就是寻找Q网络参数$\theta^Q$的最优解的过程，我们使用SGD的方法 Q网络的Loss定义：参考Q-learning、DQN 的方法，使用类似于监督学习的方法，定义loss为MSE：mean squared error： L = \frac{1}{N} \sum_i(y_i - Q(s_i,a_i|\theta^Q)^2)其中，$y_i = r_i + \gamma Q’(s_{i+1},\mu’(s_{i+1}|\theta^{\mu’})|\theta^{Q’})$，$y_i$可以看做为标签 注意： $y_i$的计算，使用的是target的策略网络$\mu’$和target Q网络$Q’$，这样做是为了Q网络参数的学习过程更加稳定，易于收敛 这个标签本身依赖于我们正在学习的target策略，这是区别于监督学习的地方 4.4.2 DDPG实现框架和算法online和target网络 以往的实践证明，如果只使用单个“Q神经网络”的算法，学习过程很不稳定，因为Q网络的参数在频繁gradient update的同时，又用于计算Q网络和策略网络的gradient。基于此，DDPG分别为策略网络、Q网络各创建两个神经网络拷贝，一个叫做online，一个叫做target： 在训练完一个mini-batch的数据之后，通过SGA/SGD算法更新online网络的参数，然后再通过soft update算法更新target网络的参数，soft update是一种running average的算法： 优点：target网络参数变化小，用于在训练过程中计算online网络的gradient，比较稳定，训练易于收敛 缺点：参数变化小，学习过程慢 DDPG框架图： DDPG算法流程 初始化AC的online神经网络参数:$\theta^Q$，$\theta^\mu$ 将online网络的参数拷贝给对应的target网络参数：$\theta^{Q’} \leftarrow \theta^Q $，$\theta^{\mu’} \leftarrow \theta^\mu$ 初始化Replay menory buffer $R$：（后面简称buffer） for each episode： ​ 初始化UO随机过程； ​ for t = 1 , T： ​ 下面额度步骤与上面图片（DDPG框架）中的步骤编号对应： ​ 1、actor根据behavior策略选择一个$a_t$，下达给gym执行该$a_t$ a_t = \mu(s_t | \theta^\mu) + N_t​ behavior策略是一个根据当前online策略$\mu$和随机UO噪声生成的随机过程，从这个随机过程中采样获 得$a_t$的值 ​ 2、gym执行$a_t$，返回reward $r_t$和新的状态$s_{t+1}$ ​ 3、actor将这个状态转换过程（transition）:$(s_t,a_t,r_t,s_{t+1})$存入buffer $R $中，作为训练online网络的 数据集 ​ 4、从buffer $R$beta中，随机采样$N$个transition数据，作为online策略网络、online Q网络的一个mini- batch训练数据，我们用$(s_i,a_i,r_i,s_{i+1})$表示mini-batch中的单个transition数据 ​ 5、计算online Q网络的梯度 ​ Q网络的loss： L = \frac{1}{N} \sum_i(y_i - Q(s_i,a_i|\theta^Q)^2)​ 其中，$y_i = r_i + \gamma Q’(s_{i+1},\mu’(s_{i+1}|\theta^{\mu’})|\theta^{Q’})$ ​ 基于反向传播，就可以求得$L $针对$\theta^Q$的梯度：$\nabla_{\theta^Q} L$ ​ 6、更新online Q网络：采用Adam optiminzer更新$\theta^Q$ ​ 7、计算策略网络的梯度 ​ 策略网络的梯度： \nabla_{\theta^\mu} J_\beta(\mu) \approx E_{s \sim \rho^\beta} [\nabla _a Q(s,a|\theta^Q)|_{a=\mu(s)} \cdot \nabla_{\theta^\mu}\mu(s|\theta^\mu)]​ 即policy gradient是在$s$根据$\rho^\beta$分布时，$\nabla_a Q \cdot \nabla_{\theta^\mu} \mu$的期望值。我们用蒙特卡洛方法来估算这个期望 值： ​ 在buffer中存储的（transition）：$(s_i,a_i,r_i,s_{i+1})$是基于agent的behavior策略$\beta$产生的，它们的 分布函数为$\rho^\beta$，所以当我们从buffer随机采样获得mini-batch数据时，根据蒙特卡洛方法，使用mini- batch数据代入上述policy gadient公式，可以作为上述期望值的一个无偏估计，所以policy gradient 可以改写为： \nabla_{\theta^\mu} J_\beta(\mu) \approx \frac{1}{N} \sum_i(\nabla_aQ(s,a|\theta^Q)|_{s=s_i,a=\mu(s_i)} \cdot \nabla_{\theta^\mu}\mu(s|\theta^\mu)|_{s=s_i})​ 8、更新online 策略网络：采用Adam optimizer更新$\theta^\mu$ ​ 9、soft update target网络$\mu’$和$Q’$： ​ 使用running average的方法，将online的参数，soft update给target网络的参数： \theta^{Q'} \leftarrow \tau \theta^Q + (1-\tau)\theta^{Q'}​ \theta^{\mu'} \leftarrow \tau\theta^\mu + (1-\tau)\theta^{\mu'}​ 其中，$\tau$一般取值0.001 ​ end for time step end for episode 原文DDPG算法流程 DDPG的输入输出图解： 五、DDPG对于DPG的关键改进 使用卷积神经网络来模拟策略函数和Q函数，并用深度学习的方法来训练，证明了在RL方法中，非线性模拟函数的准确性和高性能、可收敛； 而DPG中，可以看成使用线性回归的机器学习方法：使用带参数的线性函数来模拟策略函数函数和Q函数，然后使用线性回归的方法进行训练 experience replay memory的使用：actor同环境交互时，产生的transition数据序列是在时间上高度关联的，如果这些数据直接用于训练，会导致神经网络的过拟合，不易收敛 target网络和online网络的使用，使学习过程更加稳定，收敛更有保障 总结 DDPG结合DQN和DPG的算法，解决了DQN只能运用在离散行为空间上的局限，同时借鉴DQN的神经网络、经验回放和设置target网络使DPG中的Actor-Critic算法更容易收敛 并且DDPG在对target网络的更新时和DQN有所区别：DQN中，每隔一定的迭代次数后，将online网络的参数复制给target网络；而DDPG中target网络的参数每次迭代都以微小量逼近online的参数 参考链接 主要参考了下面链接中的第一二条链接 Deep Reinforcement Learning-1.DDPG原理和算法 深度强化学习—连续动作控制DDPG、NAF DDPG论文笔记 强化学习-DDPG算法详解]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>DDPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（七）之DPG方法]]></title>
    <url>%2F2019%2F07%2F31%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E4%B8%83%EF%BC%89%E4%B9%8BDPG%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言 为什么需要引入确定性策略梯度？ 传统的策略梯度算法以概率形式$\pi_\theta(a|s) = P[a|s;\theta]$来表示一个策略，以此来随机的选择行为。但DPG用一种确定性的策略形式$a=\mu_\theta(s)$。 DPG有着比PG更简单的形式：DPG的策略函数的更新就是action-value的期望，这种简单的形式使得DPG比PG更有效，同时在高维度行为空间也比PG表现更好 一、论文题目 Deterministic Policy Gradient Algorithms 二、研究目标 研究一种更有效的估计策略梯度以及能在高维度行为空间表现更好的算法 三、问题定义 对以往的随机策略而言，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度很大。如果使用使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算出各个可能的动作的价值的话，那需要的样本量是非常大才行的。于是有人就想出确定性策略来简化这个问题。 随机策略：在相同的策略下，在同一个状态处，采用的动作是基于一个概率分布$\pi_\theta(a|s) = P[a|s;\theta]$的，即是不确定的。 确定性策略：在同一个状态处，虽然采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么则会简化很多，即作为确定性策略。相同的策略，在同一个状态处，动作是唯一确定的，即$\pi_\theta(s) = a$。 四、DPG方法介绍4.1 知识回顾4.1.1 随机策略梯度（Stochastic Policy Gradient） 策略梯度的根本目的就是使累计奖励最大，即最大化期望reward： \begin{split} J(\pi_\theta) &= \int_S \rho^\pi(s)\int_A\pi_\theta(s,a)r(s,a)dads \\&= E_{s\sim\rho^\pi,a\sim\pi_\theta}[r(s,a)]\end{split}\tag{1}策略梯度的基本思想就是沿着$\nabla_\theta J(\pi_\theta)$方向调整参数： \begin{split} \nabla_\theta J(\pi_\theta) &= \int_S \rho^\pi(s)\int_A\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a)dads \\ &=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(a|s)Q^\pi(s,a)] \end{split}\tag{2}推导过程： 综上，尽管状态分布$\rho^\pi(s)$依赖于策略的参数，但策略梯度并不依赖于状态分布的梯度 4.1.2 随机Actor-Critic算法（Stochastic Actor-Critic Algorithms） 基于策略梯度理论的actor-critic算法是一个被广泛使用的框架，该框架主要由两部分组成： actor通过随机梯度上升更新随机策略$\pi_\theta(s)$的参数$\theta$ critic来估计行为价值函数$Q^w(s,a) \approx Q^\pi(s,a)$，例如时间差分（TD）算法 注：我们不知道公式中的真实action-value对应关系的函数$Q^\pi(s,a)$，但是我们使用参数$w$创建近似的价值函数$Q^w(s,a) $，通过合适的策略计算算法，使得$Q^w(s,a) \approx Q^\pi(s,a)$ 但是，通常来说，用$Q^w(s,a) $来逼近$ Q^\pi(s,a)$，会引入偏差，为了消除偏差，近似函数应满足： $Q_w(s,a) = \nabla_\theta log \pi_\theta(a|s)^Tw$（对于随机策略的函数逼近为线性） $\varepsilon^2(w) = E_{s\sim\rho^\pi,a\sim\pi_\theta} [(Q^{w}(s,a) - Q^\pi(s,a))^2]$（参数$w$，应该最小化均方差） 则我的策略梯度为： \nabla_\theta J(\pi_\theta)=E_{s\sim\rho^\pi,a\sim\pi_\theta}[\nabla_\theta log\pi_\theta(a|s)Q^w(s,a)在实践中，一般放宽条件2，更有利于算法通过时间差分学习到更有效的评估函数。如果条件1、2全部满足的话，整个算法相当于不适用critic。 4.2 离线策略（异策略）Actor-Critic（Off-Policy Actor-Critic） Off-policy的意思就是我们训练用的数据使用的策略和当前要更新的策略不是一个策略，设数据使用的策略为$\beta$，而当前要训练的策略是$\pi$，这里用到了importance sampling。 即不同于行为策略$\pi$的off-policy（异） 策略$\beta$来选择状态、行为轨迹（trajectories） 目标函数通常修改为目标策略的价值函数： \begin{split} J_\beta(\pi_\theta) &= \int_S \rho^\beta(s) V^\pi(s)ds \\ &=\int_S\int_A\rho^\beta(s)\pi_\theta(a|s)Q^\pi(s,a)dads \end{split}Off-policy的 策略梯度为: \begin{split} \nabla_\theta J_\beta(\pi_\theta) &\approx \int_S\int_A\rho^\beta(s)\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a)dads \\ &= E_{s\sim\rho^\pi,a\sim\pi_\theta}[ \frac{\pi_\theta(a|s)}{\beta_\theta(a|s)}\nabla_\theta log\pi_\theta(a|s)Q^\pi(s,a)] \end{split}\tag{4\5}离线（Off-policy）actor-critic算法（OffPAC）使用行为策略（behaviour policy）$\beta(a|s)$来生成轨迹，critic会用状态价值函数$V^v(s)\approx V^\pi(s)$来进行估计，actor用来更新策略的参数$\theta$，actor和critic都是通过离线的轨迹数据来进行训练，和上面公式不同的是，我们在这里用TD-error $\delta_t = r_{t+1} + \gamma V^v(s_{t+1}) - V^v(s_t)$代替上式中的$Q^\pi(s,a)$，这样可以提供真实梯度的近似值。在更新actor和critic时，都需要用重要性采样比率$\frac{\pi_\theta(a|s)}{\beta_\theta(a|s)}$来进行重要性采样，这一比率来判断action到底是根据策略$\pi$还是$\beta $。 4.3 确定性策略梯度（Gradients of Deterministic Policies） 现在考虑如何将策略梯度扩展到确定性策略，即$a=\mu_\theta (s)$。类似于前面提到的随机策略梯度，确定性策略梯度实际上是随机策略梯度的一个特例 4.3.1 动作-值函数梯度（Action-Value Gradients） 绝大多数的model free强化学习算法都是基于一般的策略迭代，即策略评估（policy evaluation）和策略改善（policy improvement）交替进行。策略评估就是估计动作-值函数$Q^\pi(s,a)$或$Q^\mu(s,a)$，比如用MC（蒙特卡洛）或TD来进行估计，然后在进行策略改善，策略改善最常用的方法是用贪心法：$\mu^{k+1}(s) = argmax_a Q^{\mu^k}(s,a)$ 但是在连续行为空间中，策略改善环节的贪心法就不可行，因为贪心法需要在每一步都最大化，因此就出现了问题。一个简单的替代方案就是将策略往$Q$的梯度方向移动，而不是全局最大化$Q$。具体来说，对于每一个探索果的状态$s$，策略网络的参数$\theta^{k+1}$以$\nabla_\theta Q^{\mu^k}(s,\mu_\theta(s))$的一定比例来更新。每个不同的状态，都提供了一个更新的方向，所有方向的均值，可以看做$\rho^\mu(s)$。 \theta^{k+1} = \theta^k + \alpha E_{s\sim\rho^{\mu^k}}[\nabla_\theta Q^{\mu^k}(s,\mu_\theta(s))]策略改善可以分解为动作-值函数的梯度和评估策略的梯度更新，根据导数的链式法则： \theta^{k+1} = \theta^k + \alpha E_{s\sim\rho^{\mu^k}}[\nabla_\theta \mu_\theta(s) \nabla_a Q^{\mu^k}(s,a)|_{a=\mu_\theta(s)}]按照惯例，$\nabla_\theta \mu_\theta(s)$是一个雅克比矩阵，也就是说，每一列都是梯度$\nabla_\theta[\mu_\theta(s)]|_d$（d是动作空间的维度）。通过改变策略，不同的状态都会被探索，并且状态分布$\rho^\mu(s)$也会改变。 4.4 确定性策略梯度定理（Deterministic Policy Gradient Theorem） 现在考虑带有参数向量$\theta \epsilon R^n$的确定性策略$\mu_\theta:S \rightarrow A$ 定义： 目标函数:$J(\mu_\theta) = E[r_1^\gamma|\mu]$ 概率分布：$p(s \rightarrow s’,t,\mu)$ 折扣状态分布：$\rho^\mu(s)$ 类比于随机策略，因为此时是确定性策略，所以不需要在对行为$a$做积分求期望，则累计奖励期望为： \begin{split} J(\mu_\theta) &= \int_S \rho^\mu(s)r(s,\mu_\theta(s)) ds \\ &=E_{s\sim\rho^\mu}[r(s,\mu_\theta(s))] \end{split}如果MDP过程满足$p(s’|s,a)$，$\nabla_ap(s’|s,a)$，$\mu_\theta(s)$，$\nabla_\theta \mu_\theta(s)$，$r(s,a)$，$\nabla_a r(s,a)$，$p_1(s)$在参数$s,a,s’,x$下都是连续的，那么意味着$\nabla_\theta \mu_\theta(s)$和$\nabla_a Q^\mu(s,a)$存在且确定性策略梯度存在。那么与随机策略梯度相同，我们使用$Q$值来代替即时奖励，则对于$J$的梯度，即DPG为： \begin{split} \nabla_\theta J(\mu_\theta) &= \int_S \rho^\mu(s)\nabla_\theta \mu_\theta(s) \nabla_aQ^\mu(s,a)|_{a= \mu_\theta(s)}ds \\ &= E_{s\sim\rho^\mu}[\nabla_\theta \mu_\theta(s) \nabla_aQ^\mu(s,a)|_{a= \mu_\theta(s)}] \\ &= E_{s\sim\rho^\mu}[\nabla_\theta Q^\mu(s, \mu_\theta(s))] \end{split} \tag{9}可以发现，与随机策略梯度相比，DPG少了对行为的积分，多了对动作-值函数的梯度，这也使得DPG需要更少的采样却能达到比随机策略梯度更好的效果。 证明过程：（我没怎么看懂） 4.4.1 确定性策略是随机策略梯度的极限形式 DPG公式乍看并不像随机策略梯度公式，但实际上DPG是岁及策略梯度的一种特例情况。假设定义随机策略的参数为$\pi_{\mu_\theta,\delta}$，其中$\delta$是方差参数，也就是说，如果$\delta = 0$，则随机策略等于确定性策略$\pi_{\mu_\theta } \equiv \mu_\theta$，所以可以得出策略梯度的极限形式： \lim_{\delta \rightarrow 0} \nabla_\theta J(\pi_{\mu_\theta,\delta}) = \nabla_\theta J(\mu_\theta) \tag{10}4.5 确定性Actor-Critic算法 与随机Actor-Critic算法类似，用一个可导的动作-值函数$Q^w(s,a)$来估计$Q^\mu (s,a) $ 4.5.1 On-Policy 确定性AC 对于同策略（On-Policy）AC，critic使用Sarsa来估计动作-值函数，算法为： 其中，同策略的确定性策略梯度为： 4.5.2 Off-Policy 确定性AC 对于异策略来说，在生成样本轨迹时所用的策略可以使任意的随机行为策略$\beta(s,a)$，目标函数$J$为： 梯度为： critic采用Q-learing的学习策略来估计动作-值函数： 可以看出同策略和异策略的不同之处在于对$a_t$的生成，同策略用的是确定性策略，异策略则用的是一个任意的随机策略。不同的是同策略是选择状态$s_{t+1}$最大的$Q$，而异策略是选择状态$s_{t+1}$和动作$\mu(s_{t+1})$的$Q$。 4.6 无偏证明 这部分是证明用Q代替期望奖励时如何无偏 五、结果 红色是本文结果：DAC off-policy &gt; SAC off-policy &gt; SAC on-policy 总结 好吧，自从看PG以来，关于PG、AC、DPG这里总是感觉有点懂又有一点不懂，作为DDPG以及A3C等的基础，先做一个大概的了解吧，后面看了DDPG和A3C后，在回过来看看，是否会理解更深刻一点 参考链接 主要参考了下面博客中的前两篇 Deterministic Policy Gradient-Fisher’s Blog Deterministic Policy Gradient Algorithms阅读笔记（or翻译） RL论文阅读七 Deterministic Policy Gradient Algorithms 文献笔记Deterministic Policy Gradient Algorithms 强化学习-确定性策略强化学习-DPG&amp;DDPG算法推导及分析]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>DPG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL（六）之Actor-Critic算法介绍]]></title>
    <url>%2F2019%2F07%2F30%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%EF%BC%88%E5%85%AD%EF%BC%89%E4%B9%8BActor-Critic%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[前言 在上一节中，讲到了基于Policy Based的强化学习方法，最后也涉及到了模特卡洛策略梯度reinforce算法，但是在计算之前，需要提前知道完整的状态序列，同时还需要单独对策略函数进行迭代更新，不太容易收敛，并且方差较大。 因此本篇笔记将讨论策略（Policy Based）和价值（Value Based）相结合的方法：Actor-Critic算法 一、Policy Gradient回顾 基于策略梯度的强化学习方法的基本思路是直接更新策略，即直接将参数策略化，并对策略进行近似表示。用公式表示为$\pi_\theta (s,a)= P(a|s,\theta)\approx \pi(a|s)$。利用参数化的线性、非线性函数表示策略，寻找最优策略，而这个最优策略的搜索即是要将某个目标函数最大化 1.1 策略目标函数 策略梯度的目标函数主要有以下三种方式 （1）Start Value：在能够产生完整episode的环境下，即agent可以达到某个终止状态时，可以用start value来衡量策略的优劣，就是初始状态s1的累计奖励： J_1(\theta) = V_{\pi \theta}(s1) = E_{\pi \theta}[v1]（2）Average Value：有的问题没有明确的初始状态，我们可以使用个状态的价值函数平均值： J_{av} V(\theta) = \sum_s d^{\pi \theta}(s) V^{\pi\theta}(s)（3）Average Reward per Time-Step：使用每一时间步长下的平均奖励： J_{av}R(\theta) = \sum_s d^{\pi \theta}(s) \sum_a\pi_\theta(s,a) R^a_s其中，$d^{\pi \theta}(s)$是关于策略$\pi\theta$生成的马尔科夫链关于状态的静态分布 似然比（Likelihood Ratios）：假设策略$\pi_\theta$是可导的且不等于0 ，则似然比为： \nabla_\theta \pi_\theta(s,a) = \pi_\theta(s,a) * \frac{\nabla_\theta \pi_\theta(s,a)} {\pi_\theta(s,a)} = \pi_\theta(s,a) * \nabla_\theta log \pi_\theta(s,a)为了改进策略，我们希望能够按照$J(\theta)$ 的正梯度方向对$\pi_\theta$函数进行更新，因此无论是上面的哪一种目标函数，其策略梯度都可以被表示为 \nabla_\theta J(\theta) = E_{\pi\theta}[\nabla_\theta log\pi_\theta (s,a)Q_\pi(s,a)]其中，$\nabla_\theta log\pi_\theta(s,a)$叫做Score Function 1.2 策略函数 对于不同的情况，策略函数的选取也有所不同，对于离散动作，采用Softmax Policy，对于连续动作，采用Gaussian Policy（高斯策略）。 （1）Softmax Policy：用于离散动作： ​ \pi_\theta(s,a) = \frac{e^{\phi(s,a)^T \theta}}{ \sum_{b} e^{\phi(s,b)^T \theta}} \nabla_\theta log\pi_\theta(s,a) = \phi(s,a) - E_{\pi\theta}[\phi(s,\cdot)]（2）Gaussian Policy（高斯策略）：用于连续动作： ​ 均值为$\mu(s) = \phi(s) ^T \theta$，方差可以固定为$\sigma^2$，也可以参数化 a \sim N(\mu(s),\sigma^2) \nabla_\theta log \pi_\theta(s,a) = \frac{(a-\mu(s)) \phi(s)}{\sigma^2}1.3 策略梯度定理（1）单步（one-step）MDP：初始状态$s\sim d(s)$，只经历一步得到一个即时奖励 $r = R_{s,a}$就终止，由于是单步MDP，所以三种目标函数都是一样的，使用似然比 来计算策略梯度： J(\theta) = E_{\pi \theta}[r] = \sum_{s \epsilon S}d(s) \sum_{a \epsilon A}\pi_\theta(s,a) R_{s,a} \nabla_\theta J(\theta) = \sum_{s\epsilon S}d(s) \sum_{a \epsilon A } \pi_\theta(s,a) \nabla_\theta log \pi_\theta(s,a) R_{s,a} = E_{\pi \theta} [\nabla_\theta log \pi_\theta(s,a) r]（2）多步（multistep）MDP：如果是多步MDP，我们要用长时价值$Q^\pi(s,a)$来代替即时奖励r，并且有如下定理：对于$ J = J_1 ,J_{av}R,\frac{1}{1-\gamma} J_{av}V$ ，策略梯度都有 \nabla_\theta J(\theta) = E_{\pi\theta}[\nabla_\theta log\pi_\theta (s,a)Q_\pi(s,a)]这样有了期望公式，我们就可以用蒙特卡洛采样的方法求出近似期望，用$v_t$ 作为$Q^\pi(s,a)$的无偏采样样本，则传统REINFORCE算法为： 算法描述：先随机初始化策略函数的参数$\theta$，对当前策略下的一个Episode： {(s_1,a_1,r_2,...,s_{T-1},a_{T-1},r_T) \sim \pi_\theta}从 t = 1 到 t =T-1的每一时刻，计算个体获得的奖励v(t)，然后更新参数$\theta$。如此重复每一个Episode，直到结束。 其中的策略梯度可以表示为： \Delta \theta_t = \alpha \nabla_\theta log \pi_\theta (s_t,a_t) v_t参数使用梯度上升法更新： \theta_{t+1} = \theta +\alpha \nabla_\theta log \pi_\theta (s_t,a_t) v_t二、Actor-Critic 蒙特卡洛策略梯度方法有着很高的方差，所以用一个Critic来估计行为价值函数 $Q_w(s,a) \approx Q^{\pi_\theta}(s,a)$。所以Actor_Critic算法包含两种参数： Critic：更新行为价值函数的参数w Actor：更新策略的参数$\theta$，更新过程会受到Critic的价值函数的引导 加入了Actor-critic后的策略梯度公式则变为： \nabla_\theta J(\theta) \approx E_{\pi\theta}[\nabla_\theta log\pi_\theta (s,a)Q_w(s,a)] \Delta \theta = \alpha \nabla_\theta log \pi_\theta (s_t,a_t) Q_w(s,a) 2.1 简单描述Actor-Critic actor：行动者，对应policy critic：评论者，对应value function函数 2.2 为何引入Actor-Critic actor-only：将policy参数化，可以在算法过程中直接优化，因此action可以是连续的。优化方法通常为policy gradient方法，该方法的缺点是在估计梯度的是将会产生较大的方差，因为其更新的方式是回合制更新，因此导致学习速度较慢。 critic-only：使用temporal difference（时间差分法）学习方法，估计过程中方差小。通常用greedy（贪心算法）或者$\varepsilon-greedy$（$\varepsilon$贪心算法）。$\varepsilon-greedy$可以有效的平衡exploration-exploitation的关系，即能探索新的action又能利用原有的经验生成最优的action，但通过greedy算法搜索最优action的计算量非常大，尤其是在action是连续的情况下，因此，critic-only通过将连续的action进行离散化，将优化问题变成一个枚举问题。 actor-critic：整合了上诉两个方法的有点。低方差，连续action 。critic对当前的state以及cation的表现进行估计，得到value function，用来给actor更新梯度。低方差的代价实在学习开始时，由于critic的估计不够准确而使算法具有较大的偏差 。policy-gradient占了该算法的绝大部分，其中可以分为两种standard gradient以及natural gradient，另一部分更新actor。 2.3 Actor Critic策略梯度 因为蒙特卡洛策略梯度方法有着很高的方差，因此用一个Critic来估计行为价值函数$Q_w(s,a) \approx Q^{\pi_\theta}(s,a)$ Critic要做的事情其实我们已经见过：策略评估，他要告诉个体，在由参数$\theta$确定的策略$\pi _\theta$到底表现的怎么样。所以Actor_Critic算法包含两种参数： Critic：更新行为价值函数的参数w Actor：更新策略的参数$\theta$，更新过程会受到Critic的价值函数的引导 （1）一个简单的actor-critic算法可以使用基于行为价值的critic，它使用一个线性价值函数来近似状态行为价值函数： ​ $Q_w(s,a) = \phi(s,a)^Tw$ ​ 这里的$\phi(s,a)$是描述状态和行为的特征（L维向量），个人理解：一般可以看作为输入（即图像），然后进行矩阵转置，乘以参数$w$就表示为线性价值函数了。其中Critic通过线性近似的TD（0）更新$w$，Actor通过策略梯度更新$\theta$。 算法流程如下： 注：该算法仅是基于线性价值函数的近似的Actor-Critic算法。这是一个在线实时算法，针对每一步进行更新，不需要等到Episode结束。 用特征的线性组合来近似$Q_w(s,a)$进而求解策略梯度的方法引入了偏倚，一个偏倚的价值下得到的策略梯度不一定能最后找到较好的解决方案，例如当近似价值函数的$Q_w(s,a)$ 使用可能会引起状态重名的特征时，就不一定能解决问题了。不过幸运的是，如果我们小心设计近似的 $Q_w(s,a)$函数，是可以避免引入偏倚的，这样我们相当于遵循了准确的策略梯度。 （2） 兼容近似函数Compatible Function Approximation 那怎么样才算是小心涉及的$Q_w(s,a)$呢？需要满足下面两个条件： 近似价值函数的梯度完全等同于策略函数对数的梯度，即不存在重名（兼容）情况: \nabla_wQ_w(s,a) = \nabla_\theta log \pi_\theta(s,a) 价值函数参数$w$使得均方差最小 \varepsilon = E_{\pi \theta} [(Q^{\pi_\theta}(s,a) - Q_w(s,a))^2]符合这两个条件，则认为策略梯度是准确的，此时： \nabla_\theta J(\theta) \approx E_{\pi\theta}[\nabla_\theta log\pi_\theta (s,a)Q_w(s,a)]证明过程： ​ 由于要估计Critic，即需要最小化均方差，如果要用参数$w$来使得均方差最小，则$w$一定为0 ，因此： \nabla_w \varepsilon = 0​ \therefore \nabla_w \varepsilon = \nabla_w E_{\pi \theta}(s,a) [(Q^{\pi_\theta} - Q_w(s,a))^2] = E_{\pi \theta} [(Q^{\pi_\theta}(s,a) - Q_w(s,a))\nabla_wQ_w(s,a)] = 0​ 此时，如果 \nabla_wQ_w(s,a) = \nabla_\theta log \pi_\theta(s,a)​ \therefore E_{\pi \theta} [(Q^{\pi_\theta}(s,a) - Q_w(s,a))\nabla_\theta log \pi_\theta(s,a)] = 0 \therefore E_{\pi \theta}[Q^{\pi_\theta}(s,a) \nabla_\theta log \pi_\theta(s,a)] = E_{\pi \theta}[Q_w(s,a) \nabla_\theta log \pi_\theta(s,a)]因此，就会认为策略梯度是准确的。但是如果两个条件都满足的话，整个算法相当于不需要使用critic，也就变成了传统的REINFORCE算法。所以在实践中，一般放宽条件2，更有利于通过时间差分学习到更有效的评估函数。所以策略评估通常使用蒙特卡洛策略评估、TD学习以及TD$(\lambda) $等，当然也可以使用最小方差法。具体见2.5节。 （3）总结 加了Actor Critic的策略梯度公式变为 \nabla_\theta J(\theta) \approx E_{\pi\theta}[\nabla_\theta log\pi_\theta (s,a)Q_w(s,a)]梯度更新公式为： \theta = \theta + \alpha \nabla_\theta log \pi_\theta (s_t,a_t) Q_w(s,a)Actor Critic相比于之前的蒙特卡洛策略梯度REINFORCE方法，这里的$\nabla_\theta log \pi_\theta (s_t,a_t)$是我们的socre function，不变。变化的是原来的$v_t$不在是由蒙特卡洛方法采样得到，而是由Critic得到。 而对于Critic来说，这块知识是新的，不过我们可以参考之前DQN的做法，即用一个Q网络来作为critic，这个Q网络的输入可以是状态，输出是每个动作的价值或者最优动作的价值。 总的来说，就是Critic通过Q网络计算状态的最优价值$v_t$，而Actor利用$v_t$这个最优价值迭代更新策略函数的参数$\theta$，进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数w。 2.4 Actor-Critic要点概括一句话概括Actor Critic方法 结合了Policy Gradient（Actor）和Function Approximation（Critic）的方法。Actor基于概率选行为，Critic基于Actor的行为评判行为的得分，Actor根据Critic的评分修改选行为的概率。 Actor Critic方法优势：可以进行单步更新，比传统的Policy Gradient要快 Actor Critic方法劣势：取决于Critic的价值判断，但是Critic难收敛，在加上Actor的更新，就更难收敛。为了解决收敛问题，Google Deepmind提出了 Actor Critic升级版 Deep Deterministic Policy Gradient。后者融合了DQN的优势，解决了收敛难的问题。 2.5 Actor-Critic 的评估 Actor-Critic的评估一般有以下几种方法，主要变化的是$Q_w (s,a)$ （1）基于状态价值：这和之前的蒙特卡洛策略梯度REINFORCE方法一样，这时Actor的策略函数参数更新的法公式为： \theta = \theta + \alpha \nabla_\theta log \pi_\theta (s_t,a_t) V(s,w)（2）基于动作价值：在DQN中，我们一般使用的都是动作价值函数Q来做价值评估，这时Actor的策略函数参数更新的法公式为： \theta = \theta + \alpha \nabla_\theta log \pi_\theta (s_t,a_t) Q(s,a,w)（3）基于TD（时序差分法）误差：这时Actor的策略函数参数更新的法公式为： \theta = \theta + \alpha \nabla_\theta log \pi_\theta (s_t,a_t) \delta(t)​ 其中，$\delta(t) = R_{t+1} + \gamma V(S_{t+1}) - V(s_t)$ 或者$\delta(t) = R_{t+1} + \gamma Q(S_{t+1} ,A_{t+1})-Q(S_t,A_t)$ （4）基于优势函数：和之前Dueling DQN中的优势函数一样，其优势函数A的定义为：$A(S,A,w,\beta) = Q(S,A,w,\alpha,\beta) - V(S,w,\alpha)$，即动作价值函数和状态价值函数的差值，这时Actor的策略函数参数更新的法公式为： \theta = \theta + \alpha \nabla_\theta log \pi_\theta (s_t,a_t)A(S,A,w,\beta)（5）基于TD($\lambda$)误差：一般都是基于后向TD($\lambda $)误差，是TD误差和效用迹E的乘积，这时Actor的策略函数参数更新的法公式是： \theta = \theta + \alpha \delta(t)E(t)​ 其中，$\delta(t) = R_{t+1} + \gamma V(S_{t+1}) - V(s_t)$，$E(t) = \gamma \lambda E_{t-1} + \alpha \nabla_\theta log \pi_\theta (s_t,a_t)$ ​ 当$\lambda = 0$时，就是第（2）步的普通TD方法 对于Critic本身的模型参数w，一般都是使用均方误差损失函数来做迭代更新，类似于之前DQN中所讲到的迭代方法。如果我们使用的是最简单的线性Q函数，比如$Q(s,a,w) = \phi(s,a)^T w$，则Critic本身的模型参数w的更新公式可以表示为： \delta(t) = R_{t+1} + \gamma Q(S_{t+1} ,A_{t+1})-Q(S_t,A_t) w = w + \beta \delta\phi(s,a)通过对均方误差损失函数求导可以很容易的得到上式，当然实际应用中，一般不使用线性Q函数，而使用神经网络表示状态和Q值的关系。 2.6 Actor-Critic算法流程 在网上找的关于Actor-Critic的算法流程 而这个是刘建平博客里的算法流程介绍： 算法流程这一块，暂时也不好理解，也确实没有搜到到底哪一个才是真正的算法流程，后面再看代码去加深理解吧 总结 首先，在网上找的关于Actor-Critic的论文阅读，阅读过程一脸懵逼，没办法，后来又在网上找了很多的博客，通过浏览别人的博客进行理解。 阅读的原论文题目是：Actor-Critic Algorithms ，然后网上又有好多的论文笔记是关于这一篇的：Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor 具体的，我也不知道是看哪一篇，只知道前一篇Actor-Critic Algorithms的引用次数多。因此阅读的前一篇，后面的一片soft的暂时还没有阅读。 参考链接 主要参考了刘建平的博客笔记和Fisher’s Blog，还有一些其他人的博客笔记，感谢他们的笔记 策略梯度 Policy Gradient 强化学习（十四）Actor-Critic Actor Critic（Tensorflow）-莫烦 强化学习actor-critic算法 强化学习 Actor-Critic算法详解 UCL的强化学习第7讲 强化学习第七讲策略梯度 相关笔记： 演员-评论家方法（Actor-Critic） Actor-Critic算法小结 强化学习—策略梯度与Actor-Critic算法]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>Actor-Critic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python UDP编程]]></title>
    <url>%2F2019%2F07%2F29%2F%E8%AF%AD%E8%A8%80%2FPython%2FPython-UDP%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言 在机器人的运动控制过程中，关于多机通信控制，除了用ros外，还可以进行网络编程，这里就用的是UDP编程实现两台电脑间的通信 两台电脑信息 A电脑用于服务端，B电脑用于客户端 A电脑：10.10.100.56 B电脑：10.10.100.54 注：两台电脑必须在同一个局域网内 一、UDP简单样例1.1 本地测试（服务端单向接收客户端数据） server.py 服务端，用于接收客户端传来的数据 12345678910111213141516#-*- coding: utf-8 -*-from socket import *udp_server = socket(AF_INET, SOCK_DGRAM) # 创建UDP套接字(SOCK_DGRAM为UDP)local_addr = ('127.0.0.1', 9999) #由于是本地，ip可以设置为127.0.0.1，并且自定义设置端口udp_server.bind(local_addr) # 绑定ip和端口while True: client_data,addr = udp_server.recvfrom(1024) #1024表示本次接收的最大字节数 client_data = client_data.decode('utf-8') #对client传过来的数据进行解码 print(client_data) #client_data 为client传过来的数据信息 print(addr) #addr为client的ip和端口 if client_data == "stop": breakudp_server.close() #关闭套接字 client.py 客户端，用于发送数据给服务端 12345678910111213#-*- coding: utf-8 -*-from socket import *udp_client = socket(AF_INET, SOCK_DGRAM) # 创建udp套接字(SOCK_DGRAM为UDP)#设置服务端的IP地址和端口dest_addr = ('127.0.0.1', 9999) # 注意 是元组，ip是字符串，端口是数字while True: send_data = input("&gt;&gt;") #待发送的数据，从键盘获取 udp_client.sendto(send_data.encode('utf-8'), dest_addr) # 发送数据到指定的电脑上的指定程序中 if send_data == "stop": breakudp_client.close() #关闭套接字 1.2 两台电脑间通信（同时发送与接收） server.py 使用A电脑作为服务端，服务端ip为10.10.100.56 1234567891011121314151617#-*- coding: utf-8 -*-from socket import *udp_server = socket(AF_INET, SOCK_DGRAM)local_addr = ('10.10.100.56', 7996)udp_server.bind(local_addr)while True: client_data,addr = udp_server.recvfrom(1024) #接收来自client的数据 client_data = client_data.decode('utf-8') print(client_data) print(addr) udp_server.sendto(client_data,addr) #数据原样返回给客户端 if recv_result == "stop": breakudp_server.close() client.py B电脑作为客户端，客户端ip地址为10.10.100.54 123456789101112131415#-*- coding: utf-8 -*-from socket import *udp_client = socket(AF_INET, SOCK_DGRAM) dest_addr = ('10.10.100.56', 7996) #这里的ip和端口要保持和server一致while True: send_data = input("&gt;&gt;") udp_client.sendto(send_data.encode('utf-8'), dest_addr) #发送数据到服务端 server_data,addr = udp_client.recvfrom(1024)# 接收服务端数据 server_data = server_data.decode("utf-8") print('server data',server_data) if send_data == "stop": breakudp_client.close() 运行结果显示 1234567891011&gt;&gt;stateserver data state&gt;&gt;takeoffserver data takeoff&gt;&gt;moveserver data move&gt;&gt;stateserver data state&gt;&gt;landserver data land&gt;&gt; 二、UDP发送列表数据（json） 在第一步的样例中，UDP发送和接收的都是一串字符串数据，而有时候服务端返回的数据是一个列表信息，如坐标（x,y,z），此时用字符串信息返回就比较麻烦，需要在客户端和服务端都做一些相应的转化。因此，为了解决该问题，可以使用python的json库 2.1 python对象转换为json字符串（json.dumps函数） 下面的是自己的电脑与B电脑进行通信测试的，不是A与B之间的通信 将B电脑作为服务端：10.10.100.54，进行测试 server.py 123456789101112131415161718192021222324#!/usr/bin/env python#-*- coding: utf-8 -*-from socket import *import jsonudp_server = socket(AF_INET, SOCK_DGRAM)local_addr = ('10.10.100.54', 7996) udp_server.bind(local_addr)while True: client_data,addr = udp_server.recvfrom(1024) client_data = client_data.decode('utf-8') print(client_data) # json样例测试 mylist = [1,2,3,4] json_string = json.dumps(mylist) udp_server.sendto(json_string,addr) if recv_result == "stop": breakudp_socket.close() client.py不变 结果显示 2.2 json字符串转换为python对象（json.loads函数） 这里将只给出核心部分 为什么要做json字符串转python对象？因为有时候从客户端发送过去的数据也是一个列表信息，如发送xyz的速度控制，要根据xyz值的不同进行相应的操作，此时就最好是传输python对象比较好 server.py 123456789101112131415161718import jsonwhile True: client_data,addr = udp_server.recvfrom(1024) client_data = client_data.decode('utf-8') client_data = json.loads(client_data) print('client_message',client_data) if client_data[0] == 'state': quad_pos = ... print('quad_pos',quad_pos) json_string = json.dumps(quad_pos) #python对象转json字符串 udp_server.sendto(json_string,addr) elif client_data[0] == 'takeoff': ... elif client_data[0] == 'move' and len(client_data) == 5: vx,vy,vz,t = int(client_data[1]),int(client_data[2]),int(client_data[3]),int(client_data[4]) ... client.py 123456789101112131415import jsonwhile True: send_data = input("&gt;&gt;") msg = send_data.split(' ') print('msg',msg) json_string = json.dumps(msg) #转换成json字符串 udp_client.sendto(json_string, dest_addr) if send_data == 'state': server_data,addr = udp_client.recvfrom(1024) server_data = server_data.decode("utf-8") print('server data',server_data) if send_data == "stop": break 结果显示： 客户端的输入输出显示 服务端的输出显示 三、UDP传输图像 用opencv来读取和显示，因此需要先安装opencv 1sudo proxychains4 pip install opencv-python 3.1 Python获取本地摄像头并显示1234567891011121314import cv2import numpy as npcap = cv2.VideoCapture(0)while(1): if cap.isOpened(): # get a frame ret, frame = cap.read() # show a frame cv2.imshow("capture", frame) if cv2.waitKey(1) &amp; 0xFF == ord('q'): breakcap.release()cv2.destroyAllWindows() 3.2 UDP传输图片并显示（小尺寸图片） 如果图片太大，则下面的代码将会报错 server.py 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-import socketimport cv2import numpy as nps = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# 绑定端口:s.bind(('10.10.100.54', 9999))print('Bind UDP on 9999...')while True: # 接收数据: data, addr = s.recvfrom(400000) print('Received from %s:%s.' % addr) #解码 nparr = np.fromstring(data, np.uint8) #解码成图片numpy img_decode = cv2.imdecode(nparr, cv2.IMREAD_COLOR) cv2.imshow('result',img_decode) cv2.waitKey() reply = "get message!!!" s.sendto(reply.encode('utf-8'), addr) cv2.destroyAllWindows() client.py 123456789101112131415161718# -*- coding: utf-8 -*-import socketimport cv2import numpy as nps = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)img = cv2.imread('view.jpg')img_encode = cv2.imencode('.jpg', img)[1]data_encode = np.array(img_encode)data = data_encode.tostring()# 发送数据:s.sendto(data, ('10.10.100.54', 9999))# 接收数据:print(s.recv(1024).decode('utf-8'))s.close() 3.3 UDP传输视频流信息 server.py 12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-import cv2import numpyimport socketimport structHOST='10.10.100.54'PORT=9999buffSize=65535server=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) #创建socket对象server.bind((HOST,PORT))print('now waiting for frames...')while True: data,address=server.recvfrom(buffSize) #先接收的是字节长度 if len(data)==1 and data[0]==1: #如果收到关闭消息则停止程序 server.close() cv2.destroyAllWindows() exit() if len(data)!=4: #进行简单的校验，长度值是int类型，占四个字节 length=0 else: length=struct.unpack('i',data)[0] #长度值 data,address=server.recvfrom(buffSize) #接收编码图像数据 if length!=len(data): #进行简单的校验 continue data=numpy.array(bytearray(data)) #格式转换 imgdecode=cv2.imdecode(data,1) #解码 print('have received one frame') cv2.imshow('frames',imgdecode) #窗口显示 if cv2.waitKey(1)==27: #按下“ESC”退出 breakserver.close()cv2.destroyAllWindows() client.py 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import cv2import numpyimport socketimport structHOST='10.10.100.54'PORT=9999server=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) #socket对象server.connect((HOST,PORT))print('now starting to send frames...')capture=cv2.VideoCapture(0) #VideoCapture对象，可获取摄像头设备的数据try: while True: if capture.isOpened(): success,frame=capture.read() #frame = cv2.imread('normal.jpg) 以图片形式发送 result,imgencode=cv2.imencode('.jpg',frame,[cv2.IMWRITE_JPEG_QUALITY,50]) #编码 server.sendall(struct.pack('i',imgencode.shape[0])) #发送编码后的字节长度，这个值不是固定的 server.sendall(imgencode) #发送视频帧数据 print('have sent one frame')except Exception as e: print(e) server.sendall(struct.pack('c',1)) #发送关闭消息 capture.release() server.close() 四、Arisim的UDP通信代码实例4.1 服务端 connect_simulator.py 用于连接Airsim 123456789101112#!/usr/bin/env python#-*- coding: utf-8 -*-import airsimclass Connect(object): def __init__(self): self.client = airsim.MultirotorClient() self.client.confirmConnection() self.client.enableApiControl(True) self.client.armDisarm(True) msg_server.py 接收client传来的特定的消息，并根据适当的消息返回姿态数据和图像数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/usr/bin/env python#-*- coding: utf-8 -*-# python libraryfrom socket import *import jsonfrom PIL import Image import numpy as np import cv2import threading# airsimfrom connect_simulator import Connect import airsim# udpdest_ip = '10.10.100.56'msg_port = 9999# connect the airsim simulatorclient = Connect().client# image transformdef transform_input(responses, img_height, img_width): img1d = np.array(responses[0].image_data_float, dtype=np.float) img1d = np.array(np.clip(255 * 3 * img1d, 0, 255), dtype=np.uint8) img2d = np.reshape(img1d, (responses[0].height, responses[0].width)) image = Image.fromarray(img2d) image = np.array(image.resize((img_width, img_height)).convert('L')) #cv2.imwrite('b.png', image) # image = np.float32(image.reshape(1, img_height, img_width, 1)) # image /= 255.0 return imagedef receive_msg(recv_msg): while True: client_data,addr = recv_msg.recvfrom(1024) client_data = client_data.decode('utf-8') client_data = json.loads(client_data) print('client_message',client_data) if client_data[0] == 'state': quad_pos = client.getMultirotorState().kinematics_estimated.position quad_pos = [quad_pos.x_val,quad_pos.y_val,quad_pos.z_val] print('quad_pos',quad_pos) json_string = json.dumps(quad_pos) recv_msg.sendto(json_string,addr) elif client_data[0] == 'takeoff': client.takeoffAsync().join() elif client_data[0] == 'land': client.landAsync().join() elif client_data[0] == 'move' and len(client_data) == 5: vx,vy,vz,t = int(client_data[1]),int(client_data[2]),int(client_data[3]),int(client_data[4]) #client.moveByVelocityAsync(1,0,0,1).join() client.moveByVelocityAsync(vx,vy,vz,t).join() client.moveByVelocityAsync(0,0,0,0.1).join() elif client_data[0] == 'image': responses = client.simGetImages([airsim.ImageRequest(1,airsim.ImageType.DepthVis,True)]) #image observation = transform_input(responses,144,256) img = observation img_encode = cv2.imencode('.jpg', img)[1] data_encode = np.array(img_encode) data = data_encode.tostring() recv_msg.sendto(data,addr) elif client_data[0] == 'stop': break recv_msg.close() def main(): recv_msg = socket(AF_INET,SOCK_DGRAM) recv_msg.bind((dest_ip,msg_port)) receive_msg(recv_msg)if __name__ == '__main__': main() img_server.py 启动这个py，在客户端启动相应的py，则可以观看Airsim的图像流信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/usr/bin/env python#-*- coding: utf-8 -*-# python libraryfrom socket import *import jsonfrom PIL import Image import numpy as np import cv2import threading# airsimfrom connect_simulator import Connect import airsim# udpdest_ip = '10.10.100.56'img_port = 7996# connect the airsim simulatorclient = Connect().client# image transformdef transform_input(responses, img_height, img_width): img1d = np.array(responses[0].image_data_float, dtype=np.float) img1d = np.array(np.clip(255 * 3 * img1d, 0, 255), dtype=np.uint8) img2d = np.reshape(img1d, (responses[0].height, responses[0].width)) image = Image.fromarray(img2d) image = np.array(image.resize((img_width, img_height)).convert('L')) #cv2.imwrite('b.png', image) # image = np.float32(image.reshape(1, img_height, img_width, 1)) # image /= 255.0 return imagedef send_img(recv_img): while True: client_data,addr = recv_img.recvfrom(1024) client_data = client_data.decode('utf-8') responses = client.simGetImages([airsim.ImageRequest(1,airsim.ImageType.DepthVis,True)]) #image observation = transform_input(responses,144,256) img = observation img_encode = cv2.imencode('.jpg', img)[1] data_encode = np.array(img_encode) data = data_encode.tostring() recv_img.sendto(data,addr) recv_img.close()def main(): recv_img = socket(AF_INET,SOCK_DGRAM) recv_img.bind((dest_ip,img_port)) send_img(recv_img)if __name__ == '__main__': main() 4.2 客户端 client.py 在这个py程序中，输入相应的指令，然后服务端相应，目前是手动的，后面会改成自动的 1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/env python#-*- coding: utf-8 -*-from socket import *import numpy as npimport cv2import jsonfrom recv_img import Imgudp_client = socket(AF_INET, SOCK_DGRAM)dest_addr = ('10.10.100.56', 9999) while True: send_data = raw_input("&gt;&gt;") msg = send_data.split(' ') print('msg',msg) json_string = json.dumps(msg) #udp_client.sendto(send_data.encode('utf-8'), dest_addr) udp_client.sendto(json_string, dest_addr) if send_data == 'state': server_data,addr = udp_client.recvfrom(1024) server_data = server_data.decode("utf-8") print('server data',server_data) if send_data == 'image': data, addr = udp_client.recvfrom(400000) #解码 nparr = np.fromstring(data, np.uint8) #解码成图片numpy img_decode = cv2.imdecode(nparr, cv2.IMREAD_COLOR) print('img',img_decode) if send_data == "stop": breakudp_client.close() recv_img.py 启动该py程序，并在服务端启动了对应的py程序后，则能看到窗口 12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python#-*- coding: utf-8 -*-from socket import *import numpy as npimport cv2udp_client = socket(AF_INET, SOCK_DGRAM)dest_addr = ('10.10.100.56', 7996) class Img(): img = Nonedef main(): img = Img().img while True: udp_client.sendto('image'.encode('utf-8'), dest_addr) data, addr = udp_client.recvfrom(400000) #解码 nparr = np.fromstring(data, np.uint8) #解码成图片numpy img_decode = cv2.imdecode(nparr, cv2.IMREAD_COLOR) img = img_decode print('img',img) img_decode = cv2.resize(img_decode,(640,480),interpolation=cv2.INTER_CUBIC) cv2.imshow('result',img_decode) cv2.waitKey(1) #cv2.destroyAllWindows() udp_client.close()if __name__ == '__main__': main() 4.3 结果显示 总结 基于UDP传输的基本样例，并且用到的暂时只有这么多，在机器人控制中，服务端主要返回机器人的图像信息和姿态信息，而客户端要发送的主要是机器人的控制信息，如何控制等等，但是我在测试的过程中，如果要在客户端时刻显示机器人的相机信息，则需要新建一个server.py程序，设置一个端口，然后在客户端也新建一个client.py信息，通过这个client来时刻显示视频。当然，如果不需要显示画面的话，则不需要新建 参考链接 python对象转换为json字符串：python socket编程传输列表数据 json字符串转换为python对象：python对象和json互相转换 UDP传输图片并显示 UDP传输视频流信息]]></content>
      <categories>
        <category>语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初始Carla（一）之Carla预编译版安装使用]]></title>
    <url>%2F2019%2F07%2F25%2FCarla%2F%E5%88%9D%E5%A7%8BCarla%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8BCarla%E9%A2%84%E7%BC%96%E8%AF%91%E7%89%88%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言 Carla和Airsim都是基于UE4引擎的仿真模拟器，但是经过安装笔记，Carla的源码安装是真的比Airsim复杂很多，因为之前源码安装的失败了，因此这里先记录如何使用预编译版的Carla 版本说明 Ubuntu16.04 CARLA_0.9.6 后面的安装步骤都是在刚装完ubuntu系统基础之上安装的，因此有一些基础的python包也会进行安装，并且这里是已经配置了Ubuntu下终端翻墙的，所以在安装时最好也是已经配置好，这样在下载一个软件包什么的都比较快，能节省不少时间 一、安装GPU显卡驱动 因为我这里用的是比较差的显卡，GT730，因此安装的是384驱动 12345sudo add-apt-repository ppa:graphics-drivers/ppasudo proxychains4 apt-get install nvidia-384 nvidia-settings nvidia-primesudo proxychains4 apt-get install mesa-common-devsudo proxychains4 apt-get install freeglut3-devsudo reboot 安装完并重启后，输入nvidia-smi则会有信息输出 二、下载Carla压缩文件 carla官方github 点击此处，进入到Carla的下载界面，此时有两种方式进行下载，下载Carla0.9.6，大概有3个G左右 网页下载：直接点击CARLA_0.9.6.ta.gz，但是这种下载网络较慢 终端下载：利用翻墙下载，速度快很多，能达到3M/s左右 首先，右键CARLA_0.9.6.tar.gz，复制LinkLocation 然后，打开终端，执行如下命令下载 1proxychains4 wget http://carla-assets-internal.s3.amazonaws.com/Releases/Linux/CARLA_0.9.6.tar.gz 三、解压文件 用右键解压下载的CARLA_0.9.6.tar.gz文件，并解压到home目录 四、Carla使用4.1 Python包的安装4.1.1 setuptools安装 采用源码安装方式，下载地址，下载zip文件，下载完后解压并进去到该解压目录 1sudo python setup.py install 4.1.2 pip安装 源码安装pip，下载地址，下载.tar.gz文件，下载完后解压，并进入到该解压目录 12sudo python setup.py buildsudo python setup.py install ​ 4.1.3 pip安装其他包12sudo proxychains4 pip install pygamesudo proxychains4 pip install numpy 4.1.4 安装libpng1sudo apt-get install libpng16-16 4.1.5 安装PythonAPI依赖（Carla相关） 找到到之前解压的Carla文件夹，并进入到~/CARLA_0.9.6/PythonAPI/carla/dist目录下 1sudo easy_install carla-0.9.6-py2.7-linux-x86_64.egg 4.2 Carla验证使用 在Carla0.9.6文件夹下运行./CarlaUE4.sh 全局视角 在PythonAPI/examples下运行 matual_control.py 汽车的第三视角 在pygame windows上就能用键盘去控制了 五、Carla的五种城市地图切换 使用的是0.9.5版本，0.9.6版本的好像有点问题，只需要下载0.9.5，并且easy_install 0.9.5版本的PythonPAI依赖包就可以了 Town01 1./CarlaUE4.sh /Game/Carla/Maps/Town01 Town02 1./CarlaUE4.sh /Game/Carla/Maps/Town02 Town03 1./CarlaUE4.sh /Game/Carla/Maps/Town03 Town04 1./CarlaUE4.sh /Game/Carla/Maps/Town04 Town05 1./CarlaUE4.sh /Game/Carla/Maps/Town05 总结 Carla的预编译版安装就比较简单，也不需要下载UE4，并进行编译等等。初步将Carla加载出来了，后面就可以写代码进行测试，并进行源码的安装测试 参考链接 Carla0.9.5简单高效安装方法]]></content>
      <categories>
        <category>Carla</category>
      </categories>
      <tags>
        <tag>Unreal Engine</tag>
        <tag>Carla</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（五）之Policy Gradient理解]]></title>
    <url>%2F2019%2F07%2F23%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E4%BA%94%EF%BC%89%E4%B9%8BPolicy-Gradient%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言 在前面的四篇DQN强化学习算法中，主要是对价值函数进行了近似表示，这是只一种基于价值函数（Value Based）的方法，是基于状态价值函数V(s)或者动作价值函数Q(s,a)的控制问题解法。在确定性的环境中，当我们知道了每个状态价值函数V(s)的数值之后，策略就可以选择使得下一个状态期望状态价值函数最大的行动a。但是在随机的环境中状态价值函数V(s)就不太好用了，因为它不能直接指导我们进行行动的选择。因此，在控制问题中更常用的是动作价值函数Q(s,a)，它描述了在某个状态s下采取不同的行动a产生的期望收益。但是这是一种间接的做法，那有没有更直接的做法呢？ 有！那就是直接更新策略。本文将介绍一种更为直接的方式解决强化学习中的控制问题—策略梯度方法，它属于另一类，即基于策略的方法（Policy Based）。 一、论文题目 Policy Gradient Methods for Reinforcement Learning with Function Approximation 二、研究目标 研究一种更直接的方法解决强化学习中的控制问题，之前的控制方法是一种间接方法 三、问题定义3.1 Value Based方法的局限性 首先，它面向的是确定性策略，而最优策略通常是随机的 其次，DQN系列方法对连续动作的处理能力不足。如果在一个连续空间内挑选动作，则Q值将会在无穷多的动作中计算价值，从而选择行为，这可吃不消 最后，算法收敛障碍。一个行动的估计值的任意小的变化可以导致它被选择或不被选择，这种不连续变化是算法保证收敛的一个关键障碍 3.2 Policy Based方法的优缺点优点： 连续的动作空间（高维空间）中更加高效 可以实现随机化的策略 某种情况下，价值函数可能比较难以计算，而策略函数比较容易 缺点： 通常收敛到局部最优而非全局最后 评估一个策略通常低效（这个过程可能慢，但是具有更高的可变性，其中也会出现很多并不有效的尝试，且方差高） 四、Policy Gradient算法介绍4.1 Value Based网络 在Value Based强化学习方法中，我们对价值函数进行了近似表示，引入了一个动作价值函数Q，这个函数由参数w描述，并接受状态s与动作a的输入，计算后得到近似的动作价值，即： Q(s,a,w)\equiv\pi(s,a)4.2 Policy Network Policy Gradient 基本思想就是通过更新Policy Network来直接更新策略 Policy Network：实际上就是一个神经网络，输入是状态，输出直接就是动作（不是Q值） 输出一般有以下两种方式： 概率方式：输出某一个动作的概率 确定性方式：输出具体的某一个动作 举例如下图所示：输入当前的状态，输出action的概率分布，选择概率最大的一个action作为要执行的操作。 在Policy Based强化学习方法下，我们采样类似的思路，只不过这时我们对策略进行近似表示。此时策略π可以被描述为一个包含参数θ的函数,即： \pi_\theta(s,a) = P(a|s,\theta) \equiv\pi(a|s)Policy Based的方法就是使用参数θ来逼近拟合状态值函数V_π（s）和状态动作值函数Q_π（s，a）的分布。 V_\theta(s) \equiv V_\pi(s) Q_\theta(s,a) \equiv Q_\pi(s,a)将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法了，那么这个梯度对应的优化目标如何定义呢？ 4.3 更新Policy Network Policy Gradient不通过误差反向传播，它通过观测信息选出一个行为直接进行反向传播，当然出人意料的是他并没有误差，而是利用reward奖励直接对选择行为的可能性进行增强和减弱，好的行为会被增加下一次被选中的概率，不好的行为会被减弱下次被选中的概率。 如果要更新 Policy Network 策略网络，或者说要使用梯度上升的方法来更新网络，需要有一个目标函数，且此时的策略Policy参数化为 \pi_\theta(s,a) = P(a|s,\theta) \equiv\pi(a|s)使用model-free的方法，不借助与agent做决策而是将agent丢入不确定的动态环境下，不提供动态环境的信息，让agent自己瞎溜达根据所获得的策略信息更新参数。 对于所有强化学习的任务来说，其实目标都是使所有带衰减 reward 的累加期望最大。即如下式所示 J(\theta) = E{(r_1 + \gamma*r_2 + \gamma^2 * r_3 +...|\pi_\theta)}4.3.1 策略梯度的目标函数 策略梯度的目标函数主要有以下三种方式 （1）最简单的目标函数就是初始状态收获的期望，即目标函数为： J_1(\theta) = V_{\pi\theta}(s1) = E_{\pi\theta}[v1]（2）有的问题没有明确的初始状态，那么我们的目标函数可以定义为平均价值，即 J_{av}V(\theta) = \sum_s d^{\pi\theta}(s)V^{\pi\theta}(s)其中，$d^{\pi\theta}(s)$是基于策略πθ生成的马尔科夫链关于状态的静态分布 （3）使用每次time-step的平均奖励： J_{av}R(\theta) = \sum{_s}d^{\pi\theta}(s)\sum{_a}\pi_{\theta}(s,a)R^a_s为了改进策略，我们希望能够按照J(θ)的正梯度方向对π函数进行更新。假设θ是策略函数π的参数，本文的第一个基本结论为，无论上面哪种形式的J(θ)，其梯度都可以被表示为（后面会稍微证明一下）: \nabla_\theta J(\theta) = E_{\pi\theta}[\nabla_\theta log\pi_\theta (s,a)Q_\pi(s,a)]其中的 \nabla_\theta log\pi_\theta (s,a)我们一般称为分值函数（score function） 我们暂定使用初始值法做目标函数 J(\theta) = V^{\pi\theta}(s1) = E_{\pi\theta}[v1] = E(r_1 + \gamma * r_2 + \gamma^2 *r_3 + ......|\pi_\theta )对该目标函数进行最大化也就是在搜索一组参数向量θ，使得目标函数最大。这实际做的事改变策略概率而非改变行动轨迹的工作，所以我们接下来就要使用梯度下降求解 \nabla_\theta J(\theta) 在连续策略上选用Gaussian Policy，在离散策略下采用softmax Policy 4.3.2 策略梯度定理（The policy gradient theorem） 由于我们是基于model-free的所以无法事先知道动态环境的状态分布，而奖励函数又依赖于动作和状态分布，所以无法进行求导，所以我们需要把奖励采用无偏估计的方法计算出来，首先随机采样然后取均值来估计 假设一个只有一步的MDP，对它使用策略梯度下降。πθ(s,a)表示关于参数θ的函数，映射是P(a|s,θ)。它在状态s所执行-a动作的奖励为r = R(s,a)。那么选择动作a的奖励为πθ(s,a) * R(s,a)：表示在该策略下所获得的奖励，在状态s的加权奖励为 J(\theta) = \sum_{a \epsilon A}\pi_\theta(s,a)R(s,a)推导过程如下： 由于 J(\theta) = E_{\pi\theta}[R(s,a)]因此 J(\theta) = \sum_{s\epsilon S}d(s) \sum_{a \epsilon A } \pi_\theta(s,a) R(s,a)梯度为： \nabla_\theta J(\theta) = \nabla_\theta \sum_{s\epsilon S}d(s) \sum_{a \epsilon A } \pi_\theta(s,a) R(s,a) \nabla_\theta J(\theta) =\sum_{s\epsilon S}d(s)\sum_{a \epsilon A } \nabla_\theta \pi_\theta(s,a) R(s,a)假设策略πθ为零的时候可微，则 \because (log_a y)' = \frac{y' }{y * ln a} \therefore \nabla_\theta \pi_\theta(s,a) = \pi_\theta(s,a) * \frac{\nabla_\theta \pi_\theta(s,a)} {\pi_\theta(s,a)} = \pi_\theta(s,a) * \nabla_\theta log \pi_\theta(s,a) \therefore \nabla_\theta J(\theta) = \sum_{s\epsilon S}d(s) \sum_{a \epsilon A } \pi_\theta(s,a) \nabla_\theta log \pi_\theta(s,a) R(s,a)其中，d(s)是策略中的状态分布，π(s,a)是当前状态的动作概率分布，所以可以将策略梯度恢复成期望形式 \nabla_\theta J(\theta) = E[\nabla_\theta log \pi_\theta(s,a) R(s,a)]且 \nabla_\theta log \pi_\theta(s,a)为分值函数（score function） 然后在将似然率方式的策略梯度方法应用到多步MDPs上，此时因为奖励值应该为过程中的多步奖励值之和，在这里使用Qπ(s,a)代替单步奖励值R(s,a)，对于任意可微的策略梯度如下（策略价值计算公式）： \nabla_\theta J(\theta) = E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a) Q^{\pi\theta}(s,a)]策略梯度定理详细推导过程如下 策略梯度公式已经求出，并且我们的分值函数也能求出，但是关于策略函数πθ(s,a)又是如何确定的呢？ 4.3.3 策略函数的设计 常用的策略函数有两种，分别是softmax策略函数和高斯策略函数 现在回头看前面策略函数πθ(s,a)的设计，在之前它一直都是一个数学符号 softmax策略函数：主要应用于离散空间，softmax策略函数使用描述状态和行为的特征 \phi (s,a)与参数θ的线性组合来权衡一个行为发生的几率，即： \pi_\theta(s,a) = \frac{e^{\phi(s,a)^T \theta}}{ \sum_{b} e^{\phi(s,b)^T \theta}}其中， \phi_{sa}表示状态-动作对的L维特征向量 所以，其分值函数为： \nabla_\theta log\pi_\theta(s,a) = \phi(s,a) - E_{\pi\theta}[\phi(s,\cdot)] 高斯策略函数：应用于连续行为空间的一种常用策略，通常对于均值有一个参数化的表示，同样也可以是一些特征的线性代数和： \mu(s) = \phi(s)^T \theta方差可以是固定值，也可以用参数化表示 行为对应于一个具体的数值，该数值以从 \mu(s)为均值， \sigma为标准差的高斯分布中随机采样产生： a \sim N(\mu(s),\sigma^2)对应的分值函数是： \nabla_\theta log \pi_\theta(s,a) = \frac{(a-\mu(s)) \phi(s)}{\sigma^2}有了策略梯度和策略函数，我们就可以得到第一版的策略梯度算法了 4.4 蒙特卡洛策略梯度reinforce算法(不带基数) 针对具有完整Episode的情况，我们应用策略梯度理论，使用随机梯度上升来更新参数，对于公式里的期望，我们通过采样的形式来替代，即时候用t时刻的奖励（return）作为当前策略下行为价值的无偏估计，表现为：返回v(t)作为Qπ（st，at）的无偏估计，使用价值函数V(s)近似代替了Qπ（s,a） 策略梯度就可以为： \Delta \theta_t = \alpha \nabla_\theta log \pi_\theta (s_t,a_t) v_t参数更新： \theta_{t+1} = \theta +\alpha \nabla_\theta log \pi_\theta (s_t,a_t) v_t算法描述：先随机初始化策略函数的参数θ，对当前策略下的一个Episode： {(s_1,a_1,r_2,...,s_{T-1},a_{T-1},r_T) \sim \pi_\theta}从t=1 到t = T-1的每一个时刻，计算个体获得的奖励v(t) ,然后更新参数θ。如此重复每一个Episode，直到结束. 算法伪代码如下： 需要注意的是： 只有当一个Episode结束之后，我们才能得到该轨迹上各个状态对应的v(t)。因此该方法需要先采样一个完整的轨迹，然后在倒回来对每个状态更新参数 上面描述的Vt就是奖励，使用v而不是G可能考虑的是用它作为价值的期望，从这里也能看出这是有噪声的采样 但是该方法虽然是无偏的，但是方差非常大，所以运行速度也很慢。这个时候可以提出一个baseline来减少方差，那么如何选择baseline？ 4.5 蒙特卡洛策略梯度reinforce算法（带基数） 在某些情况下，可能会出现每一个动作的奖励值都是正数，但是由于我们是通过采样的方式进行更新的，所以这时候可以引入一个基数b，则原式需要修改为 \nabla_\theta J(\theta) = E_{\pi\theta}[\nabla_\theta log \pi_\theta (s,a)[Q^{\pi\theta} (s,a) - b(s)]]总结 PG算法主要是直接对策略进行更新，看过原文，对原文中有很多都没有较好的理解，因此就在网上浏览了很多的博客加强自己的理解，现在仍然比较懵，可能也是刚开始的原因，后面就要开始真真学习DRL的Policy Based方法了 参考链接 第七讲：强化学习策略梯度类方法 强化学习笔记（3）-从Policy Gradient 到A3C Policy Gradient算法详解 深入浅出介绍策略梯度 文献笔记 强化学习 第七讲 策略梯度]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>Policy Gradient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（四）之DQN改进网络结构（Dueling DQN）]]></title>
    <url>%2F2019%2F07%2F19%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8BDQN%E6%94%B9%E8%BF%9B%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88Dueling-DQN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言 DDQN对DQN的目标Q值进行优化，去除max操作来减少过度估计的问题，Prioritized Replay DQN对DQN的经验回访池进行优化，给每个experience一个权重值，并按权重采样来优化算法提高训练速度，而这篇博客即将介绍的是Dueling DQN，对DQN的神经网络结构进行优化 一、论文题目 Dueling Network Architectures for Deep Reinforcement Learning 二、研究目标 改进DQN的神经网络结构来加快学习速率 三、问题定义 DQN的神经网络结构优化问题 以往的DQN网络最后的输出为每个action的Q-values值（在某个确定的状态下）。但是对于许多状态，根本没有必要去预测每个action对应的Q-value值 四、Dueling DQN方法介绍 Dueling DQN将Q网络分成两部分: 第一部分仅仅与状态S有关，与具体要采用的动作A无关，这部分称为状态价值函数部分，记 V(s;\theta,\beta)第二部分同时与状态S和动作A有关，这部分称为动作优势函数，记 A(s,a;\theta,\alpha)其中，theta是卷积层参数，alpha和beta分别是两只路的全连接层参数 4.1 Dueling DQN网络结构 Dueling DQN的网络结构如下图第二个网络结构 如上图所示，第一个网络结构是一般的DQN网络模型(输入层-&gt;三个卷积层-&gt;两个全连接层-&gt;输出每个动作的Q值) 而第二个网络结构则是本文即将介绍的Dueling DQN网络模型，Dueling DQN将卷积层提取的抽象特征分流到两个支路中。其中上路代表的是状态价值函数V（s），表示静态的状态环境下本身具有的价值，它表明了状态的好坏程度；下路代表的是依赖状态的动作优势函数A（s，a），表示在某个状态下，选择某个action额外带来的价值，它表明了在这个状态下各个动作的相对好坏程度。最后这两路聚合在一起得到每个动作的Q值。 最终Dueling DQN可以如下图： 注： V（s）：V（s）表示状态本身的好坏，一般为一个值（标量） A（s，a）：A（s，a）表示当前状态下采取的行动的好坏，一般为n个值，因此A（s，a）决定了策略 4.2 Dueling DQN的Q值 由前面可知，Q值由V(s) 和A(s，a)合并得到，其Q值的公式如下 其中，theta是卷积层参数，beta是V(s)的全连接层参数，alpha是A(s，a)的全连接参数 但是上面的公式也存在一些问题，效果并不好，因为它不具备可辨识性：通过Q值无法反过来确定V和A。 因此，可以强制令所选择的贪婪动作优势函数为0： 这样我们就能得到唯一的值函数： 其中，a*如下： 说明：在公式（8）中减去了一个最优的a所对应A值，这样A-A就为0了，A本身就是要寻找到最合适的a，然后确定A值，这样Q值就等于V值，并且V网络给出了最优V-值的估计，那么相应的A网络也给出了A的估计 但是在实际操作中，我们采用的方法是减去A的平均，本文的Dueling DQN方法就是采用的这种方式，因此，公式（8）修改为公式（9）： 虽然这样得到的V和A不在是具有真实意义的V和A，但是能增加稳定性（因为采用 -max会在更新时损害最优A的值），并且通过减去A的平均能有以下三个优点： 平均值和最大值的测试结果非常类似，但是平均值公式比较简洁 没有改变A(s,a)的相对顺序，保证了该状态下各个动作的优势函数相对顺序不变，保留了Dueling DQN的Q值的所有性质 能够去除多余的自由度，提高算法的稳定性 4.3 举例说明 Dueling DQN能学习到在没有动作的影响下环境状态的价值V(s)。如下图 下图中的左右两列分别表示V(s)和A(s,a)，图中的红色区域表示V(s)和A(s,a)所关注的地方 在训练过程中，V(s)关注地平线上是否有车辆出现（此时动作的选择影响不大）以及分数，如下图第一行；而A(s,a)则更关心会立即造成碰撞的车辆，此时动作的选择很重要，如下图第二行 五、实验5.2 实验方法 采用方法：Dueling DQN + DDQN + Prioritized Replay DQN MDP： 状态集：70（水平50，两垂直各10） 行为集：5（上、下、左、右、无操作） 转换函数：model free 奖励函数：红点获得正面奖励 起始状态：左下角 结束状态：右下角 5.2 实验结果 当行为越多时，dueling DQN的性能就更好 其中，5,10,20的含义是：5个行为时表示上下左右、无操作共五个，而10和20则分别是在5个行为的基础之上添加了5个和15个无操作行为。SE表示平方误差 六、结论 Dueling DQN的优势部分在于其有效学习状态值函数的能力 V函数可以得到更多的学习机会，因为以往一次只更新一个动作对应的Q值 V函数的泛化性更好，当动作越多时，优势越明显。直观上看，当有新动作加入时，它并不需要从零开始学习 因为Q函数在动作和状态的维度上的绝对值往往差很多，这会引起噪声和贪婪策略的突变，使用Dueling DQN可以改善这个问题 总结 到目前为止，关于DQN的方法和有关DQN的优化方面，算是已经全部阅读完了，后续要在开始看其他的DRL论文，后面要加强的是这几种方法的一些小训练测试 参考链接 Dueling DQN论文理解： 博文1 博文2 博文3 博文4 DQN代码（还没有测试过）：博文1]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>Dueling DQN</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（三）之DQN改进随机采样（Prioritized Experience Replay）]]></title>
    <url>%2F2019%2F07%2F18%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8BDQN%E6%94%B9%E8%BF%9B%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%EF%BC%88Prioritized-Experience-Replay%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言 上一篇介绍了Double DQN对DQN的目标Q值优化，这篇将介绍在DDQN的基础上，对经验回放池的优化 一、论文题目 PRIORITIZED EXPERIENCE REPLAY 二、研究目标 在DDQN基础上，改进经验回放池的均匀随机采样算法，使得学习更高效 三、问题定义 DDQN的均匀随机采样问题 以往的DQN或DDQN的做法是从buffer中均匀随机采样，这样会导致一些有用的样本可能被抽到的次数少，并且存储数据的空间有限，当空间存满之后，每次放入一个experience就要丢弃先前的一个experience。那么就要思考下面两个问题 选择那些experience保存 选择那些experience被回放 四、PRIORITIZED REPLAY 方法介绍 作者针对上面的两个问题，着重解决的是第二个问题，即选择那些experience被回放 4.1 举例说明 如上图所示，本文给出了一个例子来充分的说明优先的潜在好处。引入了称为“Blind Cliffwalk”的环境，来示例说明当奖赏非常 rare的时候，探索所遇到的挑战。假设仅有 n 个状态，这个环境就要求足够的随机步骤直到得到第一个非零奖励；确切的讲，随机的选择动作序列就会有 2^（-n）的概率才能得到第一个非零奖赏。此外，最相关的 experience却藏在大量的失败的尝试当中。 本文利用这个例子来显示了两个agents的学习次数的不同。可以看到这两个agent都从同一个经验回放池中去获取Q-learning的更新，第一个agent随机均匀采样experience，第二个agent唤醒一个oracle来进行优先级experience采样。这个oracle贪婪的选择使得在当前状态下最大化降低全局损失的experience。 从上图右侧，也能看出，按照一定优化序列得到的experience比均匀随机采样要花费很少的尝试步骤，这明显的提升了训练速度。 注：这里的oracle可以理解为上帝，文中的意思指的是实验跑完了，回过来看如何如何做最好，相当于一种离线学习，即oracle采样的数据是最好的一组batch 4.2 Prioritizing with TD-Error Temporal-difference（TD）error表示一个experience的惊奇度或出乎意料的程度 引入TD-Error：引入TD-Error的目的是给每一个experience添加一个TD-Error标准，在每次进行更新时，从buffer中选择绝对值对打的TD-Error的样例进行回放。然后对该样例进行Q-learning的更新，更新Q值和TD-Error的权重。新的experience到来之后，没有已知的 TD-error，所以我们将其放到最大优先级的行列，确保所有的 experience 至少回放一次。 TD-Error公式如下： 引入TD-Error后的好处： oracle的做法可以极大的降低无用的尝试，加速了算法的执行速度 4.3 Stochastic Priorization 引入 Stochastic Priorization的目的是为了缓解多样性的损失 在上一步引入TD-Error后会产生两个问题： 为了避免消耗太多资源遍历整个memory，我们只为那些被replay的experience更新TD-error；如果一开始就被赋予一个很低的TD-error，在很长一段时间内可能都不会被replay 贪婪优先集中于一个小的经验子集，误差收缩的很慢，特别是使用函数估计的时候，意味着初始的高误差转移被经常回放。缺乏多样性使得该系统倾向于 over-fitting （为什么丧失多样性造成过拟合：对于DQN，回放就是把经验池中取出的experience中的s状态输入到Q-network中，next state 输入到target Q-network中，然后进行一系列计算loss，反向传播更新网络参数，如果一直重复这一条信息输入，那么我对于训练网络过程来说精度会很高，而当测试的时候，输入别的状态就会使测试精度降低，即过拟合） 对 noise spikes 非常敏感，bootstrapping 会加剧该现象，估计误差又会成为另一个噪声的来源 为了解决上述问题，我们引入了一个随机采样的方法，该方法结合了 纯粹的贪婪优先 和 均匀随机采样。我们确保被采样的概率在experience优先级上是单调的，与此同时，确保最低优先级的experience的概率也是非零的。具体的，我们定义采样experience i 的概率为： 其中，pi 是experience i 的优先级。指数 α 决定了使用多少优先级，当 α 等于 0 的时候是均匀的情况 pi有以下两种变体： 直接的、成比例优先（Proportional Variant）： 这里的 \epsilon是一个很小的数字，这样就保证了有些TD-Error为0的特殊边缘例子也能够被采样到 间接的、基于排行的优先（Rank-based Variant）： rank（i）是根据 |\delta|排序后，i 的排名 两个分布都是随着误差单调的，但是后者更鲁棒，因为其对离群点不敏感。两个变体相对均匀的baseline来讲都是有很大优势 4.3.1 Proportional Variant 本文提出的Sum Tree，Sum Tree是一种树形结构，每片树叶存储每个样本的优先级 p, 每个树枝节点只有两个分叉, 节点的值是两个分叉的合, 所以 SumTree 的顶端就是所有 p 的合. 正如下面图片(来自Jaromír Janisch), 最下面一层树叶存储样本的 p, 叶子上一层最左边的 13 = 3 + 10, 按这个规律相加, 顶层的 root 就是全部 p 的合了. 抽样时, 我们会将 p 的总合 除以 batch size, 分成 batch size 那么多区间, (n=sum(p)/batch_size). 如果将所有 node 的 priority 加起来是42的话, 我们如果抽6个样本, 这时的区间拥有的 priority 可能是这样. [0-7], [7-14], [14-21], [21-28], [28-35], [35-42] 然后在每个区间里随机选取一个数. 比如在第区间 [21-28] 里选到了24, 就按照这个 24 从最顶上的42开始向下搜索. 首先看到最顶上 42 下面有两个 child nodes, 拿着手中的24对比左边的 child 29, 如果 左边的 child 比自己手中的值大, 那我们就走左边这条路, 接着再对比 29 下面的左边那个点 13, 这时, 手中的 24 比 13 大, 那我们就走右边的路, 并且将手中的值根据 13 修改一下, 变成 24-13 = 11. 接着拿着 11 和 13左下角的 12 比, 结果 12 比 11 大, 那我们就选 12 当做这次选到的 priority, 并且也选择 12 对应的数据. 注：上面的说明是参照的网上教程写的，这里在写一下自己的理解 个人理解：图片中的所有叶子节点（3、10、12、4、1、2、8、2）对应于每一个experience的TD-Error，然后两个两个组合，构成哈夫曼树，即会生成顶层的根root，并且其值为42。然后在进行分区，分区后再每个区间进行抽值，如抽到的为n。抽值后，从根节点开始找（从上至下，从左至右），如果遇见其子节点m比n大，则走该子节点的路，如果遇见子节点m比n小，则走另一个子节点路，并且n=n-m.，直到找到叶子节点，选择叶子节点中那个比当前n值大的experience。 4.3.2 Rank-based Variant 将buffer分为k个等概率的分段，从每一个分段中进行贪婪优先采样 4.4 Annealing the bias 利用随机更新得来的期望值的预测依赖于这些更新，对应其期望的同样的分布。优先回放引入了误差，因为它以一种不受控的形式改变了分布，从而改变了预测会收敛到的 solution（即使 policy 和 状态分布都固定）。我们可以用下面的重要性采样权重（importance-sample weights）来修正该误差： Importance sampling的影响： 在典型的强化学习场景中，更新的无偏性在训练结束接近收敛时是最重要的，因为由于策略、状态分布和引导目标的改变，有bias会高度不稳定，与未修正的优先重放相比，Importance sampling使学习变得不那么具有侵略性，一方面导致了较慢的初始学习，另一方面又降低了过早收敛的风险，有时甚至是更好的最终结果。与uniform重放相比，修正的优先级排序平均表现更好。 4.4 本文采用方法 本文将优先回放和 Double Q-learning 相结合，就是将 均匀随机采样 替换为 本文提出的 随机优先和重要性采样方法，具体算法见下图 总结 Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。 参考链接 论文理解1：传送门 论文理解2：传送门 代码样例：传送门 莫烦大神：传送门 很不错的网址：传送门 4.3.1里面的图片来源：传送门]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>DQN</tag>
        <tag>Prioritized Experience Replay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（二）之DQN改进目标Q值计算（Double DQN）]]></title>
    <url>%2F2019%2F07%2F18%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8BDQN%E6%94%B9%E8%BF%9B%E7%9B%AE%E6%A0%87Q%E5%80%BC%E8%AE%A1%E7%AE%97%EF%BC%88Double-DQN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言 在DDQN之前，基本所有的目标Q值都是通过贪婪法直接得到，无论是Q-learning还是DQN（Nature）。虽然DQN中用了两个Q网络并使用目标Q网络计算Q值，但是在得到a的过程中，还是使用的贪婪法计算得到。 使用max虽然可以让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计，过度估计即最使得估计得值函数比值函数的真实值大，从而可能会影响最终的策略决策，导致最终的策略并非最优，而是次优 一、论文题目 Deep Reinforcement Learning with Double Q-learning 二、研究目标 改进目标Q网络算法解决DQN存在的过度估计问题 三、问题定义 DQN的过度估计问题 如果过度估计确实存在，是否会对实践中的表现产生负面影响 四、DDQN介绍4.1 Q-learning参数更新 Q-learning在参数更新过程中，用于计算动作值函数的Yt Q网络的参数与值函数的Q网络参数相同，这样在数据样本和网络训练之间存在相关性 计算时，公式（2）里面是计算出最大的Q值（找出每个a对应的Q值，并求出最大值） 4.2 Deep Q Network参数更新 DQN在Q-learning的基础之上引入了目标Q网络和经验回放，在进行参数更新的过程中，使用Target Q来计算动作值函数，Target Q网络和值函数的Q网络结构相同，并且初始参数相同，在固定的step下，将值函数的Q网络参数传递给Target Q网络参数，进行更新，打破了数据样本和网络训练之间的相关性 4.3 Double DQN 参数更新4.3.1 Double Q思想 在Q-learning和DQN中动作值函数都采用了max操作，可能会导致动作值函数的过度估计。为了防止这种情况，Double Q-leaning通过解耦目标Q值动作的选择与目标Q值的计算这两步，来消除过度估计得问题 可以将公式（2）修改为下图，在计算时，首先求出使得Q值最大的行为a，然后将该a作为当前状态的输入行为，求出Q值 4.3.2 Double Q-learning 使用Q网络参数，估计贪婪政策的价值，并用Target Q网络的参数来公平的评估该策略的价值 4.3.3 Double DQN 五、实验5.1 Q-learning过度估计 Q-learning的过度估计随着action的增加而增加，而Double Q-learning则是无偏的 5.2 真实action下的过度统计 过度估计可能会阻碍学习到最优策略，而使用Double Q-learning减少过度估计，策略会得到改善 5.3 DQN的过度估计分析 Double DQN在值函数的准确性和策略质量方面都优于DQN 六、讨论6.1 误差来源 环境噪声 函数近似 非平稳性 6.2 贡献 证明了Q学习在大规模问题中为什么会出现过度估计的情况 通过分析Atari游戏里的价值估计，发现这些过度估计在实践中比以前承认的更为普遍与严重 证明Double Q-learning可以大规模使用，能成功减少这种过度估计，是学习更加稳定可靠 提出了Douuble DQN算法 证明了Double DQN能找到更好测策略 总结 Q-learning和DQN中的max操作使得估计得值函数比值函数的真实值大。 如果值函数每一点的值都被过估计了相同的幅度，即过估计量是均匀的，那么由于最优策略是贪婪策略，即找到最大的值函数所对应的动作，这时候最优策略是保持不变的。也就是说，在这种情况下，即使值函数被过估计了，也不影响最优的策略。强化学习的目标是找到最优的策略，而不是要得到值函数，所以这时候就算是值函数被过估计了，最终也不影响我们解决问题。然而，在实际情况中，过估计量并非是均匀的，因此值函数的过估计会影响最终的策略决策，从而导致最终的策略并非最优，而只是次优。 为了解决值函数过估计的问题，Double Q-learning 将动作的选择和动作的评估分别用不同的值函数实现 参考链接 Double DQN理解：传送门 Double DQN和DQN讨论：传送门 代码参考：传送门]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>DQN</tag>
        <tag>Double DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRL论文阅读（一）之DQN方法]]></title>
    <url>%2F2019%2F07%2F17%2F%E8%AE%BA%E6%96%87%2F%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2FDRL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8BDQN%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言 以往的Q-learning算法只适合解决小问题，如果问题太大，则无法分别学习到所有状态下的所有动作值，因此提出了DQN。DQN的本质上仍然是Q-learning，只是利用了神经网络表示动作值函数，并利用了经验回放和单独设立目标网络这两个技巧，但是DQN本身是无法克服Q-learning本身锁固有的缺点-过估计。过估计是指估计的值函数比真实值函数要大。之所以存在过估计的问题，根源在于Q-learning中的最大化操作 一、论文题目 Autonomous Quadrotor Landing using Deep Reinforcement Learning 二、研究目标 在仿真中使用DRL方法实现无人机在标记logo上的自主降落 三、问题定义 地标着陆问题：使无人机精准降落在地标上 标记检测：无人机固定高度，只在xy方向上移动，最终到达标记点正上方 垂直下降：在标记点正上方，慢慢下降，减少无人机与地标的距离 降落：降落在标记点上 四、研究方法介绍 DRL：DQN方法 4.1 马尔科夫决策过程 目标：训练一个无人机agent，它从环境中得到状态和奖励，然后选取一个行为，作用于环境 MDP定义： 状态集：一般为xy坐标或者图片的集合（本文中为图片集合） 行为集：agent可能采取的动作的集合（本文中一般为前后左右） 转换函数：状态s在行为a下转移到s’的概率（本文中为mode free） 奖励函数：不同的任务一般奖励不同（本文正面奖励：+1，负面奖励：-1，生活成本：-0.01） 起始状态：每次训练之前的agent位置（本文中为固定区域内的随机方向和随机位置） 结束状态：训练完成后的agent位置（本文中为降落到标记点上） ​ 结束条件：当前episode达到最大步数或者强制截止（超出边界） 4.2 CNN网络 对于相对较大较复杂的环境，一般Q表无法存储，此时，使用CNN进行函数值逼近 输入：灰度图像（4张） 输出：行为集 4.3 DQN（标记检测） 主要包含：经验池、CNN Network、CNN Target Netword 其中target网络和cnn网络局哟相同的网络结构 4.3.1 DQN之经验回放 训练DQN网络之前，先要进行随机行为选取，然后将agent执行随机行为产生的数据存储到经验池中，经验池中的数据为（s,a,s’,r,done）：当前状态、当前状态采取的动作，执行完动作后的下一个状态，执行完动作后的奖励、执行完动作后当前episode是否结束 在训练的过程中，需要将经验池中的（s,a）作为输入，输入到CNN网络中，每次选取经验池中的一批数据进行输入，然后CNN网络训练参数，最终agent从环境中获取s，输入到训练好的CNN网络中，CNN返回最大概率的行为给agent，agent执行该动作，作用于环境 4.3.2 DQN调参 在DQN的两个网络中，target网络和cnn网络具有相同的网络结构，并且初始网络参数相同 训练过程中，cnn网络的参数一直在更新，当执行到一定的步数之后，将cnn网络的参数赋给target网络参数，保持两个网络参数相同，然后在进行cnn网络参数更新，以此往复 4.3.3 DQN网络测试 训练好DQN网络之后，agent首先感知环境，从环境中获取状态s，并把状态s作为输入，输入到DQN网络，DQN网络返回动作a给agent，agent执行该动作a作用于环境，环境发生变化，agent会获得新的状态s，以此往复，就能利用DQN网络进行测试了 4.4 Double DQN + 优先级经验回放：垂直下降 在垂直下降部分，使用的Double DQN方法和优先级经验回放 4.5 DQN 与Double DQN比较 只是目标Q网络稍有不同，其他一样 五、实验环境 仿真平台：Gazebo7 ROS版本：ros kinetic 无人机：Parrot AR Drone2 纹理：自定义的一些纹理图 训练硬件（训练5.2天完成）： 处理器：i7 8核 RAM：32G 显卡：NVIDIA Quadro K2200 六、实验结果6.1 标记检测 标记检测部分，训练的agent能超过人类水平 6.2 垂直下降 垂直下降部分，训练的agent能接近人类水平 七、未来展望 未来的工作将会关注仿真和现实中的一些差距方面 总结 DQN方法的创新点（相比于Q-learning） 加入神经网络：DQN利用深度卷积神经网络逼近值函数 DQN的行为值函数用神经网络逼近，属于非线性逼近。 加入经验回放：DQN利用经验回放训练强化学习过程 在训练神经网络时，存在的假设是训练数据是独立同分布的，但是通过强化学习采集的数据之间存在关联性，利用这些数据进行顺序训练，神经网络不稳定。经验回放可以打破数据间的关联。 在强化学习训练过程中，智能体将数据存储到一个经验回放池中，在利用均匀随机采样的方法从经验池中抽取数据，然后利用抽取的数据训练神经网络，这样就打破了数据之间的关联性 加入目标网络：DQN利用目标网络降低当前Q值和目标Q值间的关联性 在利用神经网络逼近值函数时，计算TD目标的动作值函数所用的网络参数 \Theta与梯度计算中要逼近的值函数所用的网络参数相同，这样就存在关联性（数据样本和网络训练之间的相关性） 为了解决该问题，加入了目标网络，计算值函数逼近的网络表示为 \Theta用于动作值函数逼近的网络表示为 \Theta'而该动作值函数网络在固定的步数更新一次 参考链接 DQN与Double DQN讨论：DQN方法创新 ​ ​]]></content>
      <categories>
        <category>论文</category>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>DRL</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ros自定义msg消息类型及roslaunch启动多节点]]></title>
    <url>%2F2019%2F07%2F13%2FROS%2Fros%E8%87%AA%E5%AE%9A%E4%B9%89msg%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B%E5%8F%8Aroslaunch%E5%90%AF%E5%8A%A8%E5%A4%9A%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[前言 有时候需要自己定制一些特定的msg消息类型，而原有的就不一定能满足，这时候就需要自己进行一些相关的设置了 对于自定义的ros节点，也能通过roslaunch启动节点（py程序） rosmsg相关设置默认msg消息类型 官网 如geometry_msgs：msg API 一、创建工作空间 我这里采用的编译方式是catkin build，而不是catkin_make，是因为一些需要，如mavros和mavlink的编译方式为catkin build，如果不需要下载这两个包，则采用哪一种编译方式都不影响 1234mkdir -p catkin_ws/srccd catkin_wscatkin buildsource devel/setup.bash 下图是我已经全部创建好后的所有ros包，也正是有mavlink和mavros，所有才采用catkin build编译 二、在工作空间创建新的ros消息包123456cd ~/catkin_ws/srccatkin_create_pkg ldg_msgs rospy std_msgs roscppcd ldg_msgsmkdir msgcd msgtouch Velocity.msg 然后打开Velocity.msg 1gedit Velocity.msg 将如下代码复制粘贴进去 1234float32 vxfloat32 vyfloat32 vzint32 t 文件结构如下图 三、修改package.xml 因为需要message_generation生成C++或Python能使用的代码，所以向package.xml文件中添加如下两行： 12&lt;build_depend&gt;message_generation&lt;/build_depend&gt;&lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt; 在如下位置进行添加 四、修改CMakeLists.txt CMakeLists.txt中有四个地方需要修改： 4.1 添加message_generation 向find_package()中添加message_generation 123456find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation) 在如下位置进行修改为上面的代码 4.2 添加Velocity.msg 向add_message_files()中添加Velocity.msg 1234add_message_files( FILES Velocity.msg) 在如下位置进行修改为上面的代码 4.3 去掉generate_message（）注释 4.4 添加message_runtime 向catkin_package()添加message_runtime 123catkin_package( CATKIN_DEPENDS roscpp rospy std_msgs message_runtime) 在如下位置进行修改为上面的代码 五、编译123cd ~/catkin_wscatkin buildsource devel/setup.bash 六、测试 首先，要启动roscore 然后，在另一个新终端，执行如下命令 123cd ~/catkin_wssource devel/setup.bashrosmsg show ldg_msgs/Velocity 出现如下信息，则说明配置成功 python代码测试 创建test.py程序，拷贝如下代码，不报错即可 1234567891011121314151617181920#!/usr/bin/env python# -*-coding:utf-8 -*-import rospyimport timefrom ldg_msgs.msg import Velocitymsg = Nonedef receiveVel(data): #vel = Velocity() msg = data print('data',data)test = rospy.Subscriber("/test",Velocity,receiveVel)rospy.init_node('test')while not rospy.is_shutdown(): print('msg',msg) time.sleep(0.1) 七、添加多个自定义msg消息类型7.1 创建多个msg文件 ldg_msgs文件夹下中添加VelocityZ.msg、Imu.msg、Position.msg等文件 ldg_msgs/msg文件目录结构 msg消息类型 7.2 CMakeLists.txt添加msg 7.3 编译123cd ~/catkin_wscatkin buildsource devel/setup.bash 已经定义过msg消息类型后，在此基础之上，只需要照着第七步就可以完成后面的多个msg消息类型的定制化了，同样的，也能参照第六步进行相关测试 roslaunch相关设置 已经通过多个py程序定义了多个ros节点 一、文件结构 工作空间名：catkin_ws 编译方式：catkin build 自定义msg包名：ldg_msgs ros控制程序包名：ros_controller roslaunch启动文件：ros_airsim.launch ros节点对应驱动py：*_driver.py ros控制程序py：ros_controller.py 二、roslaunch文件编写 在catkin_ws/src文件夹下创建一个和include、src同级的launch文件夹，并在launch文件夹下创建.luanch文件，.launch文件内容如下 1234567891011121314&lt;launch&gt; &lt;group ns = "Drone1"&gt; &lt;node pkg="ros_controller" name="drone1_takeoff_driver" type="drone1_takeoff_driver.py" /&gt; &lt;node pkg="ros_controller" name="drone1_camera_driver" type="drone1_camera_driver.py" /&gt; &lt;/group&gt; &lt;group ns = "Drone2"&gt; &lt;node pkg="ros_controller" name="drone2_takeoff_driver" type="drone2_takeoff_driver.py" /&gt; &lt;node pkg="ros_controller" name="drone2_camera_driver" type="drone2_camera_driver.py" output = "screen" /&gt; &lt;/group&gt;&lt;/launch&gt; pkg是ros的程序包名，name是对节点重新进行命名，type是对应相应节点的py程序 三、roslaunch启动多节点123cd catkin_wssource devel/setup.bashroslaunch ros_controller ros_airsim.launch 启动launch文件后，终端输出信息如下： 利用rostopic list查看信息，这里自定义的节点订阅和发布的topic就都显示出来了 参考文献 rosmsg相关设置：rosmsg roslaunch相关设置：roslaunch 总结 因为控制代码使用python语言写的，因此在利用roslaunch启动ros的多个节点时，并不需要在CMakeLists.list和package.xml里面进行相关的配置，但是自定义msg消息类型则是需要进行配置的，至此，就能全部通过ros来控制了，最开始我通过shell命令来启动多个python文件从而来启动多个节点，这种方式不如roslaunch好]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Airsim（四）之QGC、SITL、户外UE4场景配置]]></title>
    <url>%2F2019%2F07%2F10%2FAirsim%2F%E5%88%9D%E8%AF%86Airsim%EF%BC%88%E5%9B%9B%EF%BC%89%E4%B9%8BQGC%E3%80%81SITL%E3%80%81%E6%88%B7%E5%A4%96UE4%E5%9C%BA%E6%99%AF%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言 这篇博客，更多的是记录一下ubuntu下QGC的安装及使用，Airsim的SITL配置以及如何添加UE4的定制场景等。但是在Airsim的HITL配置方面还没有成功过 相关关键词 QGC SITL PX4 一、PX4 SITL配置1.1 设置dialout用户1sudo usermod -a -G dialout $USER 执行完后，注销，重新登录，重启更好 1.2 安装Pixhawk/Nuttx（and JMAVSim） 在桌面新建sh文件，并命名为：ubuntu_sim_nuttx.sh 点击此处，拷贝ubuntu_sim_nuttx.sh源代码，粘贴到刚才新建的ubuntu_sim_nuttx.sh文件中 修改权限，并执行sh文件 12chmod u+x ubuntu_sim_nuttx.sh./ubuntu_sim_nuttx.sh 下载过程中，有些需要提示的地方进行确认，下载完成后，重启计算机 1.3 获取PX4源码12345mkdir -p PX4cd PX4git clone https://github.com/PX4/Firmware.gitcd Firmwaregit checkout v1.8.2 1.4 编译 要确保到 PX4/Fireware 目录下，然后执行如下命令进行编译 1make posix_sitl_ekf2 none_iris 编译完成后，窗口会出现等待UDP连接的提示 二、QGC和Mavros的安装2.1 QGC安装（1）下载QGC 下载QGC，请点击此处 （2）修改权限 12cd ~/Downloadschmod +x ./QGroundControl.AppImage （3）运行QGC 1./QGroundControl.AppImage 最终，会弹出QGC窗口 2.2 mavros和mavlink的源码安装2.2.1 二进制安装方式（1）安装mavros 1sudo apt-get install ros-kinetic-mavros ros-kinetic-mavros-extras （2）安装GeographicLib 1234wget https://raw.githubusercontent.com/mavlink/mavros/master/mavros/scripts/install_geographiclib_datasets.shchmod u+x ./install_geographiclib_datasets.shsudo suproxychains4 ./install_geographiclib_datasets.sh 2.2.2 源码安装（1）创建工作空间 1234mkdir -p ~/catkin_ws/srccd ~/catkin_wscatkin buildwstool init src （2）安装mavlink 1rosinstall_generator --rosdistro kinetic mavlink | tee /tmp/mavros.rosinstall （3）安装mavros 1rosinstall_generator --upstream mavros | tee -a /tmp/mavros.rosinstall （3）更新 123wstool merge -t src /tmp/mavros.rosinstallwstool update -t src -j4rosdep install --from-paths src --ignore-src -y （4）安装GeograohicLib 12sudo suproxychains4 ./src/mavros/mavros/scripts/install_geographiclib_datasets.sh （5）编译 1catkin build （6）source 1source devel/setup.bash 2.3 安装相关依赖包1sudo apt-get install python-catkin-tools 三、测试 首先，将前面所打开的所有UE4界面和终端窗口全部关掉 3.1 settions.json配置 需要将settion.json里面的配置修改为PX4配置 settions.json文件在~/Documents/Airsim目录下 配置信息修改如下： 1234567891011&#123; "SeeDocsAt": "https://github.com/Microsoft/AirSim/blob/master/docs/settings.md", "SettingsVersion": 1.2, "SimMode":"Multirotor", "Vehicles":&#123; "PX4":&#123; "VehicleType":"PX4Multirotor", "UseSerial":false &#125; &#125;&#125; 3.2 SITL连接和QGC连接（1）编译posix_sitl 进入到PX4/Fireware目录下，编译 1make posix_sitl_ekf2 none_iris 编译完成后，等待UDP的链接 （2）打开QGC 进入到 QGroundControl.AppImage 所在目录，打开QGC 1./QGroundControl.AppImage 打开后，也会等待无人机的连接 （3）打开UE4Editor 双击运行 UnrealEngine/Engine/Binaries/Linux/UE4Editor 文件，选择之前添加的插件Blocks，原来的Rolling没有AirsimGameMode 点击打开后，点击播放，加载出来后，会发现PX4 的UDP已经连接上，QGC也已经连接上 3.3 测试3.3.1 QGC起飞测试 点击QGC的起飞按钮，并滑动确定，最终Multirotor会起飞 3.3.2 Firmware窗口命令起飞降落测试 在之前的等待UDP连接的窗口上输入命令测试 12commander takeoffcommander land 3.3.3 ros命令测试 打开一个新终端，执行如下命令 1roslaunch mavros px4.launch fcu_url:="udp://:14540@127.0.0.1:14557" 此时，可以通过调用service服务，来实现起飞 打开新终端，执行下面命令，能执行起飞降落 123rosservice call /mavros/cmd/arming truerosservice call /mavros/cmd/takeoff -- 0 0 47.6420 -122.1404 126.1rosservice call /mavros/cmd/land -- 0 0 47.6420 -122.1404 123 rostopic list信息查看 rosservice 命令结果输出 输出要为true才有效 四、添加城市道路场景 Airsim有一些关于UE4的城市道路场景，但是在linux下我成功加载的目前只有两个，总共没测试过几个，能成功加载的是Africa、和Neighborhood，其中有加载过关于City的，但是这是两个zip.001这样的文件组成的，目前还没有解压成功（Linux下），在windows下应该能成功解压 在打开下载的这些场景时，setting.json的配置信息不能修改为SITL的配置，为最初始的默认配置即可 （1）下载场景 下载场景，请点击此处 （2）加载场景 首先下载的场景，要为如下的文件结构才行 windows的场景为exe文件，linux下的为sh文件，如果没有sh文件这样，则不能加载 在新终端执行如下命令加载（进入到AirSimNH.sh文件所在目录） 1./AirSimNH.sh 同理，Africa的场景也是同样的方式 参考网址 PX4 SITL配置：Airsim Github官网 QGC安装：QGC官网 Mavros安装：mavos官网 settings配置：github文档 roslaunch测试：PX4官网 总结 总的来说，Airsim的SITL配置没有问题，但是要使用SITL时，必须打开UE4Editor，然后自己定义场景，而不能使用以及下载好的.sh场景，这个以及测试过。如果要是用城市道路场景，则在setting.json里面一定不能配置PX4的信息。 目前Airsim的HITL配置没有成功，主要是用的Pixhawk进行的测试，并且没有使用遥控器，在linux下不能利用QGC完成起飞，但是在Windows下可以 SITL或HITL都是利用mavros来完成通信的，这个可能会暂时告一段落，后面会自己利用Airsim提供的API，自己去写ROS节点，利用ROS来实现控制，并且可以利用ROS的通信机制，来完成另种方式的HITL控制]]></content>
      <categories>
        <category>Airsim</category>
      </categories>
      <tags>
        <tag>Airsim</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下sublime相关配置]]></title>
    <url>%2F2019%2F07%2F10%2F%E7%B3%BB%E7%BB%9F%2FWindows%E4%B8%8Bsublime%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言 在windows上下载sublime，一个是为了编辑txt文本文件方便，另一个也能通过Sublime来运行python代码，而不用在cmd窗口或者pycharm上 一、Sublime Text3下载 Sublime Text3 下载，请点击官网 有一段时间，不能通过官网下载，每次一点下载的时候，就网页报错，目前还可以下载，当然也是打开了VPN的 二、Sublime配置运行python程序2.1 下载Python Python下载，请点击官网 我这里下载的是3.73版本，这一段时间，下载一些软件，特别慢，可能是近期比较敏感，因此，我通过在linux下终端翻墙下载，然后在拷贝到本地，进行安装的 linux下终端翻墙下载python的命令如下： 1proxychains4 wget https://www.python.org/ftp/python/3.7.3/python-3.7.3-amd64.exe 2.2 安装Python 安装过程中，选择自定义安装，并且在 添加到Path变量中一栏处，打勾，将Python添加到环境变量中。最终Python安装在C:\Program Files\Python37 目录下 2.3 测试Python 打开cmd窗口，输入python，如果进入python环境，则安装成功 2.4 Sublime配置2.4.1 插件配置 选择 首选项-&gt;浏览插件 在打开的文件夹中，找到Python文件夹，默认是没有的，下图显示的是我已经新建过的Python文件夹 在python文件夹下新建一个文件，并且命名为Python3.sublime-build，文件里的内容如下 说明：下图中的路径要修改为自己安装的python路径 12345&#123; &quot;cmd&quot;: [&quot;C:\\Program Files\\Python37\\python.exe&quot;, &quot;-u&quot;, &quot;$file&quot;], &quot;file_regex&quot;: &quot;^[ ]File \&quot;(…?)\&quot;, line ([0-9]*)&quot;, &quot;selector&quot;: &quot;source.python&quot;&#125; 2.4.2 编译配置 点击 工具-&gt;编译系统-&gt;自动编译，这里要选择为自动编译 说明：在网上的一些教程很多是选择的Python3，而不是自动编译，这个Python3是我们第一步插件配置完成后产生的 2.4.3 测试 新建一个py程序，在py程序中，写入如下代码 1print(&apos;hello world&apos;) 按快捷键ctrl +b即可运行 2.5 Sublime提示缺少相关python包 在写python代码的过程中，可能经常要导入一些包，而这些包也没有安装，又需要在sublime下运行时，此时需要先安装相关python包，如pyperlicp （1）首先要确保pip已经安装，利用pip来安装依赖包 （2）在windows上搜索命令提示符，右键以管理员身份运行 （3）安装pyperclip 1pip install pyperclip 一定要在管理员身份权限下进行安装，否则会报权限错误 更新三、Sublime 配置sublimeREPL 主要问题是写的py程序，在sublime下不能进行input，在运行时会报如下的错误： EOFError: EOF when reading a line，因此参考网上的教程对该问题进行了解决 3.1 安装Package Control包管理工具 按快捷键 ctrl + ~或者 查看 -&gt; 显示控制栏 来调出命令界面 然后输入如下代码到命令输入框中 1import urllib.request,os,hashlib; h = &apos;6f4c264a24d933ce70df5dedcf1dcaee&apos; + &apos;ebe013ee18cced0ef93d5f746d80ef60&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) 稍等一会儿，重启sublime，如果在首选项下看到有Package Control，则安装成功 3.2 安装sublimeREPL 下面的所有打开sublime操作，皆在windows的菜单栏搜索sublime，并且以管理员身份打开，否则会无效 打开sublime，按快捷键ctrl + shift +p 或者首选项-&gt;Package Control，输入install后选择 Package Control:Install Package 在弹出的界面输入sublimeREPL，回车等待sublimeREPL的安装（我这里因为已经安装了sublimeREPL，所以并没有出现） 安装完成后，重启sublime，如果在菜单栏工具下出现sublimeREPL，则安装成功 3.2 sublimeREPL配置 在SublimeText3路径下，我的是C:\Program Files (x86)\Sublime Text3\Data\Packages\SublimeREPL\config\Python中找到Main.sublime-menu文件，然后对文件里面的内容进行修改 由于直接用sublime打开该文件，保存时会因为权限不够而无法保存，因此我先将该文件拷贝到其他位置，然后修改过后，在重新拷贝过来进行覆盖 （1）修改Main.sublime-menu文件 ​ 通过搜索repl_python，定位到修改内容所在周围，将cmd里面的内容修改为： 1&quot;cmd&quot;: [&quot;python&quot;, &quot;-i&quot;, &quot;-u&quot;,&quot;$file_basename&quot;], （2）设置快捷键 仍然需要注意的是，以管理员身份运行sublime ​ 打开sublime，然后打开首选项-&gt;按键绑定-用户，然后输入如下代码： 123456789101112131415161718[ &#123; &quot;keys&quot;: [&quot;f5&quot;], &quot;caption&quot;: &quot;SublimeREPL: Python - RUN current file&quot;, &quot;command&quot;: &quot;run_existing_window_command&quot;, &quot;args&quot;: &#123; &quot;id&quot;: &quot;repl_python_run&quot;, &quot;file&quot;: &quot;config/Python/Main.sublime-menu&quot;&#125; &#125;, &#123; &quot;keys&quot;: [&quot;f8&quot;], &quot;caption&quot;: &quot;SublimeREPL: Python - PDB current file&quot;, &quot;command&quot;: &quot;run_existing_window_command&quot;, &quot;args&quot;: &#123; &quot;id&quot;: &quot;repl_python_pdb&quot;, &quot;file&quot;: &quot;config/Python/Main.sublime-menu&quot;&#125; &#125;,] 最后进行保存即可 （3）配置单窗口调试 打开C:\Program Files (x86)\Sublime Text3\Data\Packages\SublimeREPL\config\Python文件夹下的Main.sublime-menu；和前面一样，仍然是先拷贝到外面进行修改，然后重新拷贝回来进行覆盖，否则会报权限问题而无法保存 打开Main.sublime-meun文件后，通过搜索repl_python_run，快速定位到修改内容周围，然后在下图所示位置添加代码： 1&quot;view_id&quot;:&quot;*REPL* [python]&quot;, 最后对C:\Program Files (x86)\Sublime Text3\Data\Packages\SublimeREPL文件夹下的sublimerepl.py进行修改（先将py拷贝到外面修改，然后拷贝回来覆盖） 通过搜索view.id快速定位到修改内容附近，安徽将view.id() 修改为view.name() 3.3 输入测试 以管理员身份打开sublime，并新建py文件，然后按F5键进行测试 如果不是以管理员身份运行的sublime的，则无法输入输出 参考网址 Sublime配置：百度经验 Package Control安装 Sublime Text3中运行Python提示… 解决sublimeEPL在同一个窗口中调试的问题 总结 Sublime作为一个文本编辑器，既能编辑py代码，又能运行的话，是值得作为py编辑器的，相比较而言，能用Sublime运行py的话，就不要用Pycharm，毕竟轻量很多]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16下GPU Tensorflow安装详细步骤]]></title>
    <url>%2F2019%2F07%2F05%2FLinux%2FUbuntu16%E4%B8%8BGPU-Tensorflow%E5%AE%89%E8%A3%85%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[前言 以往的GPU Tensorflow安装都是在服务器上安装，并且步骤不太详细，这一次，是认为比较详细的一步步的安装方法，有了这个笔记，以后参照这个应该就能很快的安装好Tensorflow了 版本说明 系统：Ubuntu16.04 CUDA版本：cuda10.0 cuDNN版本：cudnn7.4.2 Tensorflow版本：tensorflow1.13.1 Linux下 CUDA cuDNN Tensorflow 对应关系 一、安装nvidia显卡驱动1sudo add-apt-repository ppa:graphics-drivers/ppa 二、查看可安装的驱动版本1ubuntu-drivers devices 安装nvidia-410，我这里安装的是410版本 1sudo apt-get install nvidia-410 nvidia-settings nvidia-prime 说明：我最开始安装过推荐的430版本，结果安装完成后，重启，界面进不去，会有报错，因此安装的低版本，目前测试过的410和390版本都没有问题 安装一些必要软件 12sudo apt-get install mesa-common-devsudo apt-get install freeglut3-dev 安装完成后，重启，以确认生效 1sudo reboot 重启后，输入nvidia-smi，则能看到相关信息 三、安装cuda3.1 下载cuda点击cuda官网，下载CUDA10.0版本，下载runfile文件 3.2 安装cuda12cd ~/Downloadssudo sh cuda_10.0.130_410.48_linux.run 运行后，稍等一会，会看见一些相关信息，此时需要按键，跳到100%浏览完成 按空格键跳到100%，直至出现如下界面 输入accept，等待下一个提示 是否安装410.48驱动，由于前面已经安装，这里选择否，输入n，等待下一个提示 提示，安装CUDA10.0 Toolkit，选择y，等待下一个提示 按回车键，选择默认路径，等待下一个提示 输入y，安装symbolic link，等待下一个提示 输入y，安装CUDA10.0 样例 最后，按回车键，选择默认路径，至此，已经CUDA安装步骤已经全部完成 安装完成后，界面显示如下： 四、安装cuDNN4.1 下载cuDNN点击cuDNN官网，下载cuDNN7.4.2版本，选择cuDNN7.4.2 for CUDA 10.0 下载时，选择cuDNN Library for linx，下载.tgz文件 4.2 安装cuDNN首先，解压文件 12cd ~/Downloadstar -zxvf cudnn-10.0-linux-x64-v7.4.2.24.tgz 安装配置 123sudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 五、配置cuda环境首先，需要打开~/.bashrc文件 1sudo gedit ~/.bashrc 在文件末尾添加两行代码 12export PATH=/usr/local/cuda-10.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:$LD_LIBRARY_PATH 随后，source一下，以保存配置 1source ~/.bashrc 六、安装gpu-tensorflow我这里添加了proxychains4，是实现在终端翻墙的效果，这样下载安装会很快 1sudo proxychains4 pip install tensorflow-gpu==1.13.1 七、测试在终端输入python，进入python环境 然后 输入import tensorflow，不报错，即配置成功 八、帮助信息8.1 查看cuda版本信息1cat /usr/local/cuda/version.txt 8.2 查看cuDNN版本信息cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 cuda和cuDNN版本信息如下图 8.3 查看GPU显卡信号和驱动版本信息12lspci | grep -i nvidiasudo dpkg --list | grep nvidia-* 8.4 查看可安装的gpu-tensorflow版本信息输入 1sudo proxychains4 pip install tensorflow-gpu== 这里是会报错，因为没有输入对应版本，但是提示的错误信息中，能查看所有的可安装的gpu版本 如 12sudo proxychains4 pip install tensorflow-gpu==1.13.1sudo proxychains4 pip install tensorflow-gpu==1.13.0rc0 九、安装过程中，遇到过的问题9.1 nvidia-*版本覆盖安装 最开始我安装的是nvidia-390的版本，然后输入nvidia-smi时，出现如下信息，这里没有显示CUDA的版本 后来，我直接又安装了nvidia-410版本，没有删除之前的任何信息，此时输入nvidia-smi，出现如下信息 这样也可能导致，我在这里输入sudo dpkg --list | grep nvidia-*，查看信息的时候会有两个的原因吧 总的来说，最后Tensorflow导入是没有问题的 9.2 markdown3.1.1错误提示 在安装gpu-tensorflow1.13.1版本的过程中，提示markdown错误 原因：pip版本过低 解决方法：升级pip 1python -m pip install --upgrade pip 参考链接 GPU Tensorflow安装：ldg个人博客 cuda、cuDNN、Tensorflow对应版本关系：Tensorflow中文社区 总结 又重新在真机上安装配置了一遍GPU Tensorflow，并且笔记较之前更为详细，相信后面直接参考该教程是没有任何问题的了]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Linux</tag>
        <tag>CUDA</tag>
        <tag>CUDNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Airsim（三）之Ubuntu下Airsim平台搭建]]></title>
    <url>%2F2019%2F07%2F02%2FAirsim%2F%E5%88%9D%E8%AF%86Airsim%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8BUbuntu%E4%B8%8BAirsim%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言 微软推出的airsim对windows的支持更好，但是也能在Ubuntu下进行安装，由于本人习惯在ubuntu下进行开发，因此，参考了airsim的官方文档和网上的一些文章，在ubuntu下搭建成功 版本说明 Ubuntu16.04 Unreal Engine 4.18 Airsim 1.2 一、安装Chrome浏览器 不是必须的，只是习惯用chrome浏览器而已 1234sudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -sudo apt-get updatesudo apt-get install google-chrome-stable 二、安装qt5 配置终端翻墙 需要有自己的vpn账号才行，配置这个，是为了终端翻墙，在下载一些文件的时候能够快速下载 123sudo add-apt-repository ppa:hzwhuang/ss-qt5sudo apt-get updatesudo apt-get install shadowsocks-qt5 ubuntu下，实现终端翻墙，请参考该网址，进行相关配置 三、Unreal Engine 4.18下载配置3.1 安装git这里添加了proxychains4 是已经配置好第二步，即能实现终端翻墙后添加的翻墙操作 1sudo proxychains4 apt-get install git 3.2 下载UE4（1）如果没有UE4的账号，先去官网，注册一个账号，然后进行登录 （2）绑定github账号，在用户-&gt;个人-&gt;链接的账户中，与自己的github账号绑定，绑定之后，会发邮箱，然后确认即可 （3）如果是第一次下载需要先安装一些软件包 1sudo apt-get install build-essential mono-mcs mono-devel mono-xbuild mono-dmcs mono-reference-assemblies-4.0 libmono-system-data-datasetextensions4.0-cil libmono-system-web-extensions4.0-cil libmono-system-management4.0-cil libmono-system-xml-linq4.0-cil cmake dos2unix clang-5.0 libfreetype6-dev libgtk-3-dev libmono-microsoft-build-tasks-v4.0-4.0-cil xdg-user-dirs 然后输入以下命令 123sudo ln -s /usr/bin/clang-5.0 /usr/bin/clangsudo ln -s /usr/bin/clang++-5.0 /usr/bin/clang++clang -v 为了，防止后面编译出现下面的错误 （4）下载UE4.18源码 在git的过程中，需要输入之前绑定的github的用户名和密码 1git clone -b 4.18 https://github.com/EpicGames/UnrealEngine.git 3.3 编译UE41234cd UnrealEngine./Setup.sh./GenerateProjectFiles.shmake 编译成功后，界面如下： 四、Airsim配置4.1 下载Airsim1git clone https://github.com/Microsoft/AirSim.git 4.2 编译 有的命令前，加上proxychains4，可能会更快，如：proxychains4 ./setup.sh 123cd AirSim./setup.sh./build.sh ./setup.sh成功后界面如下 ./build.sh成功后界面如下 五、Airsim 使用方法 在Unreal Engine/Engine/Binaries/Linux/文件夹下，搜索UE4Editor，然后双击打开 5.1 创建工程选择新建项目，C++，名称注意写英文，不要写中文 5.2 打开工程项目创建完成后，在刚才创建的目录下，双击打开Rolling.uproject 5.3 界面显示打开后，会成功加载除UE4的场景界面，但是此时还没有无人机或车辆 5.4 加载无人机或小车在5.3的窗口框中，选择 文件 -&gt; 打开项目，在弹出的框中选择浏览，找到 AirSim/Unreal/Environments/Blocks 文件下的.uproject文件 然后，点击打开，弹出提示需要转换副本时，选择打开一个副本 稍等一会，加载出来后，就能在UE4界面看到无人机了，当然需要设置 选择 设置-&gt;世界设置，在右侧栏GameMode这里选择AirsimGameMode 点击播放，在弹出的框中选择不，则能看到无人机加载出来 加载成功后，也能在Documents文件夹下多出一个Airsim文件夹，这个里面是settions.json文件，里面是它的配置信息 将其中的SimMode设为Car，则会加载出小车，设为Multirotor，则会是无人机，默认是Multirotor 5.5 重新打开UE4，并带有无人机 关掉之前打开的UE4界面，重新在Unreal Engine/Engine/Binaries/Linux下双击UE4Editor 在弹出的项目中，选择我们之前加载的插件Blocks，打开 然后点击播放，就能看见小车出来，并且能够手动控制，这里出现小车，是因为我在settiong.json中将SimMode设置为了Car，而且小车是能在界面上进行控制的，无人机不行 参考网址 csdn博客，Ubuntu下安装Airsim，传送门 Airsim官方文档，传送门 总结 ubuntu下的Airsim配置还是Windows下的配置有些不一样，如Windows下需要安装VS2017，在VS2017中打开sln文件等，以及windows下有Epic Game Launch网上商店，而ubuntu下没有，如果要在ubuntu下加载网上商店上的一些场景，需要先在windows下进行下载才行]]></content>
      <categories>
        <category>Airsim</category>
      </categories>
      <tags>
        <tag>Airsim</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)双系统之Win重装后linux引导损坏]]></title>
    <url>%2F2019%2F07%2F01%2FLinux%2F%E8%BD%AC-%E5%8F%8C%E7%B3%BB%E7%BB%9F%E4%B9%8BWin%E9%87%8D%E8%A3%85%E5%90%8Elinux%E5%BC%95%E5%AF%BC%E6%8D%9F%E5%9D%8F%2F</url>
    <content type="text"><![CDATA[前言 由于一些需求，有时需要重装电脑，而本来是双系统的电脑，重装后，Linux的引导修复可能缺失，即在启动的过程中只有windows的界面，而没有win与linux的选择界面，因此，在网上找了几篇博客，几经对比后，还是这一片写的比较好（对我而言），并且已经测试成功 原文网址原文 安装步骤（1）首先，得制作一个ubuntu系统启动盘（不论14、16、18这样），制作方法这里不做说明（2）开机，从U盘进入（不同的电脑按键不一样，大部分是F12），选择try install ubuntu，不要选择安装ubuntu（3）进到ubuntu试用版后，连接wifi，打开终端（ctrl+alt+t）（4）在终端输入如下命令 添加源，更新 12sudo add-apt-repository ppa:yannubuntu/boot-repairsudo apt-get update 安装boot-repair 1sudo apt-get install -y boot-repair 安装成功后，在终端输入boot-repair，启动工具，在弹出的界面框中选择recommended repair，稍等一会，进行修复 修复完成后，重启电脑，此时重启过程中就能看到ubuntu的选项了 我这里的选择项有点奇怪，但是暂时也不去管它了，倒数第二个是windows的启动项 总结 我按照上面的方法就完成了linux 的grub引导修复，在修复过程中没有出现原文中所遇到的windows引导损坏的问题，但还是在这里记录一下windows引导损坏的解决方法，如下 在linux的终端窗口中，输入 1sudo update-grub 然后重启，就会出现双系统的选择界面了，这个还没有尝试，不过应该问题不大]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu下科学上网]]></title>
    <url>%2F2019%2F06%2F27%2FLinux%2Fubuntu%E4%B8%8B%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[前言 其实这一个笔记和前面的笔记（ubuntu16下翻墙搭建）差不多，但是之前的笔记只记载了chrome浏览器下，网页端科学上网的方法，关于终端下如何科学上网，并没有记载，因此这篇笔记，更多的是说明如何在终端上科学上网，当然也能在firefox上，chrome浏览器上还没有测试 一、安装chrome浏览器 安装chrome浏览器，只是习惯用chrome，还有就是chrome浏览器的默认下载是在~/Downloads下，而firefox的默认下载是在一个tmp文件夹下，也懒得去修改，所以还是先安装了chrome浏览器，为后面的文件下载做铺垫 1234$ sudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/$ wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install google-chrome-stable 二、安装qt5并进行配置2.1 安装shadowsocks-qt5 添加源 12sudo add-apt-repository ppa:hzwhuang/ss-qt5sudo apt-get update 安装qt5 1sudo apt-get install shadowsocks-qt5 安装完成后，在菜单栏搜索shadows，找到shadowsocks-qt5，打开并进行配置 2.2 shadowsocks-qt5配置（1）添加连接 点击 Connection -&gt; Add -&gt; Manually （2）设置IP、Port、加密方式 ip 和port设置为自己的vpn账号和端口 （3）VPN连接 三、Proxychains安装并配置 下载文件时，使用chrome浏览器打开链接进行下载 3.1 下载zip文件：github网址 3.2 解压12cd ~/Downloadsunzip pro*.zip 3.3 安装proxychains12345cd ~/Downloadscd proxychains-ng-master/./configuresudo make &amp;&amp; sudo make installsudo cp ./src/proxychains.conf /etc/proxychains.conf 3.4 编辑proxychains配置 打开./conf文件进行配置 1sudo gedit /etc/proxychains.conf 将最后一行的 socks4 127.0.0.1 9050修改为socks5 127.0.0.1 1080，最终如下图 四、科学上网测试4.1 测试 需要先安装curl 12sudo apt-get install curlproxychains4 curl www.google.com 出现下图，则配置成功 4.2 终端上网 输入下面命令 1proxychains4 sudo apt-get update 或者 1sudo proxychains4 apt-get update 出现下图信息，则成功，下图对应第一条命令 4.3 firefox科学上网 在终端输入如下命令 1proxychains4 firefox 接着会弹出一个firefox的界面，在url地址栏输入www.google.com即能google了 4.4 流量损耗 在能够科学上网后，也能看到shadowsocks5 里面的流量损耗 参考链接： proxychains配置 proxychains使用方法 ping 不通google说明 总结 暂时还没有找到如何通过proxychains4 让chrome浏览器科学上网的方法，但是目前的设置，能够在firefox进行相应的搜索，但需要执行一次proxychains4的命令，另外终端执行命令前加上proxychains也能科学上网]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Airsim（二）之Windows下ROS安装配置]]></title>
    <url>%2F2019%2F06%2F27%2FAirsim%2F%E5%88%9D%E8%AF%86Airsim%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8BWindows%E4%B8%8BROS%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言 在刚开始接触Airsim的过程中，其环境是在Windows下搭建的，而平常用的ros做机器人的开发已经习惯了，因此就尝试了在Windows下配置ROS环境 一、前提 已经装好Visual Studio 2017 社区版，并且安装路径是默认安装位置路径 因为最开始修改了默认安装位置路径，在安装过程中出现了一些小问题，后来按照默认安装路径安装测试的 二、ROS安装2.1 创建快捷方式并配置 桌面空白处，右键，新建快捷方式 在输入框中，复制粘贴如下内容 1C:\Windows\System32\cmd.exe /k &quot;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\VsDevCmd.bat&quot; -arch=amd64 -host_arch=amd64 命名快捷方式为ROS 将该快捷方式设置为Administrator 右键单击快捷方式，然后选择“属性” 选择快捷方式选项卡 点击高级按钮，在高级属性中选中用管理员身份运行 在所有需要点击确定的地方点击确定，以进行保存 2.2 安装Chocolatey 和Git（1）安装Chocolatey 在windows下，找到命令提示符，以管理员身份运行，然后输入如下代码，安装Chocolatey 1@&quot;%SystemRoot%\System32\WindowsPowerShell\v1.0\powershell.exe&quot; -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command &quot;iex ((New-Object System.Net.WebClient).DownloadString(&apos;https://chocolatey.org/install.ps1&apos;))&quot; &amp;&amp; SET &quot;PATH=%PATH%;%ALLUSERSPROFILE%\chocolatey\bin&quot; 最终，窗口出现Chocolatey is now ready，则安装成功 （2）安装git 打开之前创建的ROS命令提示符 输入如下命令，安装git： 1choco install git -y 安装完成后，关掉ros窗口，重新打开，输入 1git --version 检查，git是否安装成功 2.3 ROS安装 输入如下命令，进行ros的安装，安装过程有点久 12choco source add -n=ros-win -s=&quot;https://roswin.azurewebsites.net/api/v2&quot; --priority=1choco upgrade ros-melodic-desktop -y 在安装的过程中，可能会报错，然后多次输入 1choco upgrade ros-melodic-desktop -y 进行重装，或者输入 1choco upgrade ros-melodic-desktop --force 或者，输入 1choco upgrade ros-melodic-desktop_full -y --execution-timeout = 0 最后，有点懵的就装好了，安装成功后，会在C盘根目录下有opt\ros的文件夹 三、ROS测试（1）打开一个ROS窗口 首先要进入到x64的目录下，运行setup.bat，这个就有点类似于linux的source devel/setup.bash一样 123cd C:\opt\ros\melodic\x64 setup.batroscore 最终出现如下界面则安装成功 （2）查看rostopic 打开另一个ros窗口 123cd C:\opt\ros\melodic\x64 setup.batrostopic list 四、ROS命令简化配置 在第三步的测试过程中，每次输入ros相关的命令，都要先进入到x64目录下，然后运行setup.bat，甚是麻烦，因此才有了下面的简化配置，配置好后，直接在ros窗口输入命令即可 选中ROS快捷方式，右键，打开属性 选择快捷方式选项卡，找到目标 然后复制粘贴如下内容，进行替换 1C:\Windows\System32\cmd.exe /k &quot;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools\VsDevCmd.bat&quot; -arch=amd64 -host_arch=amd64 &amp;&amp; C:\opt\ros\melodic\x64\setup.bat 重新打开ros窗口，即可直接输入roscore，启动ros管理器 参考网址： ros wiki：ros will 官网 博客：古月居 总结 至此，windows下的ros环境就已经配好了，后续可以做其他的操作，但是需要安装一些软件包 如: msgpack-rpc-python 、airsim]]></content>
      <categories>
        <category>Airsim</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2017配置（二）之GDAL配置]]></title>
    <url>%2F2019%2F06%2F26%2F%E7%B3%BB%E7%BB%9F%2FVS2017%E9%85%8D%E7%BD%AE%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8BGDAL%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言 暂时还不太清楚GDAL是用来干啥的，总之需要用，于是参照网上教程，在VS2017上配置了GDAL，后面再去了解这个的相关作用 一、GDAL下载 点击官网，下载gdal，我这里下载的是2.3.3版本 二、文件解压并解压到特定路径 我这里是在D盘的Program Files文件夹下，创建了GDAL文件夹，也将解压文件存放在GDAL文件夹下 三、GDAL编译安装3.1 修改源代码 找到解压地址 D:\Program Files\GDAL\gdal-2.3.3文件夹下的nmake.opt文件，打开，对文件进行修改，保存 （1）修改第41行的代码为：MSVC_VER=1910 （2）修改第57行的代码为：GDAL_HOME = “D:\Program Files\GDAL\gdal-2.3.3” 注意：这里的地址要根据自己的实际地址设置 （3）修改第184行的代码为：#WIN64=YES 原来的可能中间有空格，原来可能是这样的# WIN64=YES 3.2 编译 首先，打开适用于VS2017的X64的本级工具命令提示 在该窗口，进入到gdal 2.3.3目录下 1cd D:\Program Files\GDAL\gdal-2.3.3 然后执行下面代码进行编译 1nmake /f makefile.vc MSVC_VER=1910 WIN64=yes 说明：在编译过程中，比较耗时，但是编译过程没有问题 3.3 安装GDAL12nmake /f makefile.vc install MSVC_VER=1910 WIN64=yesnmake /f makefile.vc devinstall MSVC_VER=1910 WIN64=yes 四、VS2017配置GDAL4.1 打开VS2017，新建一个C++空项目4.2 点击 菜单栏-&gt;视图-&gt;其他窗口-&gt;属性管理器4.3 找到 Debug|x64 ，右键 Microsoft.CPP.x64.user，点击属性4.4 在Microsoft.CPP.x64.user属性页中，添加包含目录、库目录 添加包含目录 添加库目录 在所有需要点确定的地方，点击确定以保存 4.5 在Microsoft.CPP.x64.user属性页中，添加附加依赖项 手动输入：gdal_i.lib，最后记得确定保存 五、设置系统变量 右键此电脑，点击属性，在弹出的界面框中，点击高级系统设置，然后选择环境变量，最终出现如下界面 选择系统变量Path，点击编辑 保险起见，将电脑重启，确保环境变量生效 六、测试6.1 新建main.cpp 6.2 拷贝代码123456789101112131415161718#include "gdal_priv.h"#include&lt;iostream&gt; using namespace std;int main()&#123; const char* pszFile; GDALAllRegister(); pszFile = "C:\\Users\\Administrator\\Desktop\\1.jpg"; GDALDataset *poDataset = (GDALDataset*)GDALOpen(pszFile, GA_ReadOnly); GDALRasterBand *poBand = poDataset-&gt;GetRasterBand(1); int xsize = poBand-&gt;GetXSize(); int ysize = poBand-&gt;GetYSize(); cout &lt;&lt; xsize &lt;&lt; endl; cout &lt;&lt; ysize &lt;&lt; endl; system("pause"); return 0;&#125; 6.3 解决方案配置 这一步很重要 6.4 编译 菜单栏 生成-&gt;重新生成解决方案 6.5 运行 按F5键运行，此时报如下错误：gdal203.dll 等相关的错误 解决方法： 将D:\Program Files\GDAL\gdal-2.3.3目录下的gdal203.dll拷贝到C:\Windows\System32文件夹下 重新编译运行，成功输出图片大小，则配置成功，此时运行成功后的界面如下： 参考链接 GDAL编译链接参考（前三步）：csdn博客 GDAL在VS2017的配置参考（四五六步）：csdn博客 gdal203.dll错误解决参考：csdn博客 总结 相比于opencv，这个稍微复杂了一点，参照的文档多一些忙不过好在还是配置成功]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>VS2017</tag>
        <tag>GDAL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2017配置（一）之opencv配置]]></title>
    <url>%2F2019%2F06%2F26%2F%E7%B3%BB%E7%BB%9F%2FVS2017%E9%85%8D%E7%BD%AE%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8Bopencv%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言 帮朋友搭建配置的，因为配置过程稍微有些复杂，因此还是做个笔记记录一下，说不定以后就会涉及相关的事情 一、Opencv下载 点击官网，下载Opencv，我这里下载的是4.1.0版本 二、文件解压并解压到自定义路径位置 双击exe文件，进行解压，我解压放在D盘自定义的文件夹下 解压完成后，文件内容如下： 三、系统变量配置 我下面的界面显示是在Win10系统下，如果是Win7，某些界面会有所区别 右键此电脑，点击属性，在弹出的界面框中，点击高级系统设置，然后选择环境变量，最终出现如下界面 找到系统变量中的Path，点击编辑，添加opencv相应目录路径，如下图： 要添加的路径，就是opencv之前所解压的位置，具体到哪一级目录，看下图中的添加示例 最后，在所有需要点击确定的地方，点击确定以保存 四、相关文件配置 将第三步添加的bin目录下的两个文件（opencv_world410.dll、opencv_world410d.dll）拷贝到C:\Windows\SysWOW64文件夹下 将bin目录下的opencv_ffmpeg410_64.dll拷贝到C:\Windows\System32文件夹下 如果版本不一致，则添加对应的dll文件即可 五、VS2017配置5.1 ，打开VS2017，新建一个c++空项目 5.2 打开属性管理器 5.3 配置Debug|x64找到Debug|x64，右键Microsoft.CPP.x64.user，点击属性 5.4 添加包含目录和库目录 添加包含目录 添加库目录 在所有需要点击确定的地方，点击确定以保存 5.5 添加附加依赖项 在所有需要点击确定的地方，点击确定以保存 5.6 解决方案配置 在解决方案这里，选择Debug、X64配置 六、测试6.1 打开Project1.sln 右键源文件，点击添加，选择新建项，创建main.cpp文件 6.2 拷贝代码 将下面代码拷贝到main.cpp中，需要注意的是，载入图像这里要自己保存一张图片，并能够读取 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt; #include &lt;opencv2/imgproc/types_c.h&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/highgui/highgui_c.h&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;Vec3b RandomColor(int value); //生成随机颜色函数 int main()&#123; Mat image = imread("20.jpg"); //载入图像 imshow("Source Image", image); //灰度化，滤波，Canny边缘检测 Mat imageGray; cvtColor(image, imageGray, CV_RGB2GRAY);//灰度转换 GaussianBlur(imageGray, imageGray, Size(5, 5), 2); //高斯滤波 imshow("Gray Image", imageGray); Canny(imageGray, imageGray, 80, 150); imshow("Canny Image", imageGray); //查找轮廓 vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarchy; findContours(imageGray, contours, hierarchy, RETR_TREE, CHAIN_APPROX_SIMPLE, Point()); Mat imageContours = Mat::zeros(image.size(), CV_8UC1); //轮廓 Mat marks(image.size(), CV_32S); //Opencv分水岭第二个矩阵参数 marks = Scalar::all(0); int index = 0; int compCount = 0; for (; index &gt;= 0; index = hierarchy[index][0], compCount++) &#123; //对marks进行标记，对不同区域的轮廓进行编号，相当于设置注水点，有多少轮廓，就有多少注水点 drawContours(marks, contours, index, Scalar::all(compCount + 1), 1, 8, hierarchy); drawContours(imageContours, contours, index, Scalar(255), 1, 8, hierarchy); &#125; //我们来看一下传入的矩阵marks里是什么东西 Mat marksShows; convertScaleAbs(marks, marksShows); imshow("marksShow", marksShows); imshow("轮廓", imageContours); watershed(image, marks); //我们再来看一下分水岭算法之后的矩阵marks里是什么东西 Mat afterWatershed; convertScaleAbs(marks, afterWatershed); imshow("After Watershed", afterWatershed); //对每一个区域进行颜色填充 Mat PerspectiveImage = Mat::zeros(image.size(), CV_8UC3); for (int i = 0; i &lt; marks.rows; i++) &#123; for (int j = 0; j &lt; marks.cols; j++) &#123; int index = marks.at&lt;int&gt;(i, j); if (marks.at&lt;int&gt;(i, j) == -1) &#123; PerspectiveImage.at&lt;Vec3b&gt;(i, j) = Vec3b(255, 255, 255); &#125; else &#123; PerspectiveImage.at&lt;Vec3b&gt;(i, j) = RandomColor(index); &#125; &#125; &#125; imshow("After ColorFill", PerspectiveImage); //分割并填充颜色的结果跟原始图像融合 Mat wshed; addWeighted(image, 0.4, PerspectiveImage, 0.6, 0, wshed); imshow("AddWeighted Image", wshed); waitKey();&#125;Vec3b RandomColor(int value) //生成随机颜色函数&#123; value = value % 255; //生成0~255的随机数 RNG rng; int aa = rng.uniform(0, value); int bb = rng.uniform(0, value); int cc = rng.uniform(0, value); return Vec3b(aa, bb, cc);&#125; 6.3 编译 确保下面编译成功才进行后续的操作，解决方案配置一定要设置为Debug x64 6.4 运行 按F5键运行，最终出现下图则全部配置完成 为了保证代码完整性，也就是在前面的代码中没有进行删除，否则可以删除部分代码，进行简单一点的测试 参考链接：csdn博客 总结通过上面的参考链接，没有遇到什么错误的完成了opencv在VS2017上面的配置，感谢别人的付出]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>VS2017</tag>
        <tag>Opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Airsim（一）之Airsim平台搭建]]></title>
    <url>%2F2019%2F06%2F16%2FAirsim%2F%E5%88%9D%E8%AF%86Airsim%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8BAirsim%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言 基于Airsim，搭建一个更逼真的仿真环境，比原有的Gazebo效果更好 一、相关概念介绍 Airsim：AirSim 是微软开源的一个跨平台的建立在虚幻引擎（ Unreal Engine）上的无人机以及其它自主移动设备的模拟器。 它支持硬件在循环与流行的飞行控制器的物理和视觉逼真模拟。它被开发为一个虚幻的插件，可以简单地放到任何你想要的虚幻环境中。 该模拟器创造了一个高还原的逼真虚拟环境，模拟了阴影、反射等其它现实世界中容易干扰的环境，让无人机不用经历真实世界的风险就能进行训练。 AirSim 的目标是作为AI研究的平台，以测试深度学习、计算机视觉和自主车辆的增强学习算法。为此， AirSim 还公开了 API，以平台独立的方式检索数据和控制车辆。 Airsim官方Github：https://github.com/Microsoft/AirSim Unreal Engine：Unreal是UNREAL ENGINE（虚幻引擎）的简写，由Epic开发，是目前世界知名授权最广的游戏引擎之一，占有全球商用游戏引擎80%的市场份额。 “Unreal Engine 3”3D引擎采用了目前最新的即时光迹追踪、HDR光照技术、虚拟位移…等新技术，而且能够每秒钟实时运算两亿个多边形运算，效能是目前“Unreal Engine”的100倍，而通过nVIDIA的GeForce 6800显示卡与“Unreal Engine 3”3D引擎的搭配，可以实时运算出电影CG等级的画面，效能非常非常恐怖。 基于它开发的大作无数，除《虚幻竞技场3》外，还包括《战争机器》、《质量效应》、《生化奇兵》等等。在美国和欧洲，虚幻引擎主要用于主机游戏的开发，在亚洲，中韩众多知名游戏开发商购买该引擎主要用于次世代网游的开发，如《剑灵》、《TERA》、《战地之王》、《一舞成名》等。 iPhone上的游戏有《无尽之剑》（1、2、3）、《蝙蝠侠》等 二、版本说明 AirSim最新版本已支持Visual Studio 2017与Unreal Engine 4.18 Windows10 Visual Studio 2017（需要安装VC++ 和Windows SDK8.1） Unreal Engine 4.18（通过Epic Games Launcher安装） Git（下载Airsim1.2源码，不要用VS2017 URL链接下载） Airsim1.2 三、软件安装3.1 Visual Studio2017安装（1）点击下载，下载VS2017社区版 （2）安装VS2017 安装过程中请确保安装VC++ 和Windows SDK8.1（或Windows SDK10）的安装 此外，我还安装了单个组件中的游戏和图形的部分组件 总安装大小6个多G，耗时有点长 3.2 虚幻引擎（Unreal Engine）的安装（1）点击下载，下载Epic Games Launcher 下载过程中，若没有注册过Epic账号的，需要先注册，然后在登录下载 （2）运行Epic Games Launcher，在弹出的界面中，选择Library，点击+号，选择4.18版本进行安装 注意一定要下载4.18版本，下载了多个版本也没有关系，最后需要启动4.18版本 3.3 Git安装（1）点击下载，下载Git （2）默认安装即可 四、搭建Airsim环境并配置4.1 下载Airsim源码（1）在git bash窗口输入如下命令，下载airsim源码 1git clone https://github.com/microsoft/AirSim.git 下载完后，将Airsim文件存放到其他位置，如我的存放在D盘目录下的自定义新文件夹下 4.2 编译（1）打开window菜单，找到Visual Studio 2017，并双击打开VS2017的x64本机命令提示 在打开的窗口中，进入到Airsim所在目录 （2）编译 首先，找到\AirSim\AirLib\deps\eigen3\Eigen\src\Core\arch\CUDA\Half.h文件，修改Half.h文件的“AS ls”的引号即可。如图所示 之所以修改该 “”，是因为如果不修改，会在后面的编译过程中，碰到如下错误 然后，进入Airsim目录后，执行如下命令： 1build.cmd --no-full-poly-car 其实也可以只执行build.cmd ，也不用添加后面的 —no-full-poly-car，之所以这样做是为了在编译过程中能节省很大的时间，在下面这一步，耗时较长，我这里大概要花20分钟 最后，编译成功后的界面显示如下图 五、UE4与Airsim联系起来5.1 启动Unreal Enigen 4.18 5.2 新建工程（c++项目），如Rolling 注意：名称一定要写成英文，不能用中文 5.3 复制文件复制AirSim\Unreal\Plugins文件夹 到 Rolling目录下； 复制AirSim\Unreal\Environments\Blocks文件夹下的clean.bat和GenerateProjectFiles.bat 文件到 Rolling目录下； 在这个过程中，可能会重新编译C++类，会在UE工程下生成Rollings.sln 5.4 运行Rolling工程在Rolling目录下，双击Rolling.sln在VS2017中打开该工程 首先修改配置：DebugGame Editor + win64 然后点击【生成】-【重新生成解决方案】 最后，按F5键，运行工程项目 5.5 添加Quadrotor进入 【设置】-【世界设置】，修改其中的Game mode，修改为AirSimGameMode 点击【播放】按钮，在弹出的框中选择【不】，即可 第一次加载界面如下图，右下角还在编译着色器，稍微等一会就好 等一会后，无人机就加载出来了 六、参考链接6.1 安装文档参考 Airsim官网 知乎文档 6.2 相关概念参考 Unreal Engine Airsim 总结 至此，Airsim在Windows平台下的搭建已经基本成功，后面会继续添加新的场景，如城市场景；也会自己在去编写一些程序代码进行控制 我搭建的电脑机器配置如下： i7处理器，8G RAM，GT730显卡，128G SSD]]></content>
      <categories>
        <category>Airsim</category>
      </categories>
      <tags>
        <tag>Airsim</tag>
        <tag>Unreal Engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新电脑上移植旧电脑上博客笔记]]></title>
    <url>%2F2019%2F06%2F09%2FHexo%2F%E6%96%B0%E7%94%B5%E8%84%91%E4%B8%8A%E7%A7%BB%E6%A4%8D%E6%97%A7%E7%94%B5%E8%84%91%E4%B8%8A%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[在新的电脑上移植旧电脑博客前言： 由于一些需要，有时可能需要在新的电脑上继续原来的工作，如博客整理、github本地文件等等，但这篇笔记只用来记录博客整理部分~ 一、准备工作1、拷贝博客项目 将整个博客项目文件拷贝到新的电脑上，如旧电脑存放在D盘根目录，这里新电脑也同样的存放在D盘根目录 2、相关软件安装2.1、安装Node.js Node.js官网 安装完后，在cmd命令行窗口中输入 node -v 和 npm -v，即可查看是否安装成功 2.2、安装Git Git官网 说明：下载的时候，不确定是否要翻墙，我在下载的过程中，打开了vpn，没打开的时候，下载的网页打不开。 2.3、安装hexo并配置 以下所有命令安装都在cmd命令窗口执行 （1）在cmd窗口中，进入到博客根目录下，如：cd d:blog （2）输入npm install hexo -g，安装hexo （3）输入npm install，安装所需组件 二、Hexo与Github配置 说明：下面的所有操作都在博客根目录下执行，使用git bash窗口 1、设置Git的user name 和email在窗口中分别输入如下两行代码（注意将username和email改为自己的）： 12git config --global user.name &quot;ldgcug&quot;git config --global user.email &quot;569167692@qq.com&quot; 2、生成秘钥输入 1ssh-keygen -t rsa -C “569167692@qq.com” 在输出的提示符中，连续输入三个回车即可 3、添加秘钥输入 1eval “$(ssh-agent -s)” 将秘钥添加到ssh-agent 4、添加github的ssh秘钥（1）在 C:\Users\Administrator\.ssh文件夹下，找到id_rsa.pub并打开，复制其中的内容 （2）登陆Github，点击【头像】下的【setting】，点击左侧的【SSH and GPG keys】，点击【New SSH key】，将刚才复制的 id_rsa.pub 文件内容复制到Key中，添加ssh 5、测试github的ssh秘钥输入 1ssh -T git@github.com 如果看到Hi 后面是你的用户名，则添加成功 三、测试在git bash窗口中，进入到博客根目录 先随便新建一篇笔记，随便写点内容 输入 1hexo s 待窗口中有内容输入后，然后在浏览器上输入http://localhost:4000 若要上传到github更新的话，则输入 1hexo g -d]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)win10 git安装 exit code128错误]]></title>
    <url>%2F2019%2F06%2F05%2F%E7%B3%BB%E7%BB%9F%2F%EF%BC%88%E8%BD%AC%EF%BC%89win10%E5%AE%89%E8%A3%85git%E6%8A%A5exit%20code128%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[前言 刚重装完win10系统，结果在安装git的时候莫名的就报了错误，因此才做的相关笔记 测试成功的方法 适用于WIN10的解决方案： ​ A：电脑菜单搜索CMD ​ B：鼠标右键出现的命令提示符 ​ C：以管理员身份运行 ​ D：在出现的面板里输入 sfc /scannow==&gt;回车键 （可能需要重启电脑） ​ E： 等待修复完成======&gt;问题解决 原文网址： https://blog.csdn.net/zxssoft/article/details/84989850]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow相关问题和配置集锦]]></title>
    <url>%2F2019%2F05%2F30%2FLinux%2FTensorflow%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E5%92%8C%E9%85%8D%E7%BD%AE%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[前言 在做人工智能相关的训练方面，离不开Tensorflow，但是在安装或配置过程中都会遇到过一些问题，并且在做某一些特定的事也会有些小问题 一、配置方面1、Tensorflow指定CPU训练 在机器上进行训练时，有时候可能出现多个python程序在训练的情况，并且其他人使用的是GPU训练，而这时我在使用GPU训练会报错误，此时，可以在程序中添加代码以指定CPU进行训练，添加代码如下： 123import osos.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot; os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;-1&quot; 参考网址：网址1 未完待续]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[19年上半年校医院体检]]></title>
    <url>%2F2019%2F05%2F29%2F%E7%94%9F%E6%B4%BB%2F%E7%94%9F%E7%90%86%2F19%E5%B9%B4%E4%B8%8A%E5%8D%8A%E5%B9%B4%E6%A0%A1%E5%8C%BB%E9%99%A2%E4%BD%93%E6%A3%80%2F</url>
    <content type="text"><![CDATA[前言 由于马上需要去公司实习，可能需要上交体检表，在学校校医院做了一个简单的体检，其中基本的体检费40元，除此之外，我还做了一些其他方面的体检 整个体检项目如下图：总计146元 抽血 ​ 抽血检测方面，应该就会检测乙肝两对半、肾功能全套和血常规三项 心电图 在做心电图的过程中，主要就是医生用几个类似于夹子一样的东西给身体夹着，其中包括左右手掌下面一点位置夹两个，一个是右腿靠近脚的地方夹一个；另外在身体正面，用6个左右类似于吸盘的东西吸在上。 整个过程就是这么多，没有任何疼痛感 心电图方面有些小问题，医生建议说少熬夜。在做心电图的过程中，问了一下我平常运动情况和熬夜情况，最近确实熬夜比较多，运动也几乎没有，以后还是要注意，毕竟身体是革命的本钱。 其他体检 除了上面两个比较重要的外，其他的就例如内科（呼吸、心跳什么的）、外科（眼耳鼻喉外观观测一下）、放射性透视（胸透），最后就是一些其他的身高体重、视力、色盲情况如何了，不太重要 总结 目前尿酸530较高，心电图也有点问题，下次体检需要特别注意这两块]]></content>
      <categories>
        <category>生活</category>
        <category>生理</category>
      </categories>
      <tags>
        <tag>生理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[erle_copter仿真安装笔记之has_binary_operator.hpp]]></title>
    <url>%2F2019%2F05%2F28%2FROS%2Ferle-copter%E4%BB%BF%E7%9C%9F%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0%E4%B9%8Bhas-binary-operator-hpp%2F</url>
    <content type="text"><![CDATA[前言 在安装erle_copter过程中，为了避免出现BOOST的相关错误，有时需要修改文件里面的少许地方，但是该少许地方也不是特别好描述，因此创建该文件，直接将整个代码copy到需要修改的地方进行覆盖粘贴即可。 has_binary_operator.hpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232// (C) Copyright 2009-2011 Frederic Bron, Robert Stewart, Steven Watanabe &amp; Roman Perepelitsa.//// Use, modification and distribution are subject to the Boost Software License,// Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at// http://www.boost.org/LICENSE_1_0.txt).//// See http://www.boost.org/libs/type_traits for most recent version including documentation.#include &lt;boost/config.hpp&gt;#include &lt;boost/type_traits/ice.hpp&gt;#include &lt;boost/type_traits/integral_constant.hpp&gt;#include &lt;boost/type_traits/is_base_of.hpp&gt;#include &lt;boost/type_traits/is_const.hpp&gt;#include &lt;boost/type_traits/is_convertible.hpp&gt;#include &lt;boost/type_traits/is_fundamental.hpp&gt;#include &lt;boost/type_traits/is_integral.hpp&gt;#include &lt;boost/type_traits/is_pointer.hpp&gt;#include &lt;boost/type_traits/is_same.hpp&gt;#include &lt;boost/type_traits/is_void.hpp&gt;#include &lt;boost/type_traits/remove_cv.hpp&gt;#include &lt;boost/type_traits/remove_pointer.hpp&gt;#include &lt;boost/type_traits/remove_reference.hpp&gt;// should be the last #include#include &lt;boost/type_traits/detail/bool_trait_def.hpp&gt;// cannot include this header without getting warnings of the kind:// gcc:// warning: value computed is not used// warning: comparison between signed and unsigned integer expressions// msvc:// warning C4018: '&lt;' : signed/unsigned mismatch// warning C4244: '+=' : conversion from 'double' to 'char', possible loss of data// warning C4547: '*' : operator before comma has no effect; expected operator with side-effect// warning C4800: 'int' : forcing value to bool 'true' or 'false' (performance warning)// warning C4804: '&lt;' : unsafe use of type 'bool' in operation// warning C4805: '==' : unsafe mix of type 'bool' and type 'char' in operation// cannot find another implementation -&gt; declared as system header to suppress these warnings.#if defined(__GNUC__)# pragma GCC system_header#elif defined(BOOST_MSVC)# pragma warning ( push )# pragma warning ( disable : 4018 4244 4547 4800 4804 4805 4913 )#endifnamespace boost &#123;namespace detail &#123;// This namespace ensures that argument-dependent name lookup does not mess things up.#ifndef Q_MOC_RUNnamespace BOOST_JOIN(BOOST_TT_TRAIT_NAME,_impl) &#123;#endif// 1. a function to have an instance of type T without requiring T to be default// constructibletemplate &lt;typename T&gt; T &amp;make();// 2. we provide our operator definition for types that do not have one already// a type returned from operator BOOST_TT_TRAIT_OP when no such operator is// found in the type's own namespace (our own operator is used) so that we have// a means to know that our operator was usedstruct no_operator &#123; &#125;;// this class allows implicit conversions and makes the following operator// definition less-preferred than any other such operators that might be found// via argument-dependent name lookupstruct any &#123; template &lt;class T&gt; any(T const&amp;); &#125;;// when operator BOOST_TT_TRAIT_OP is not available, this one is usedno_operator operator BOOST_TT_TRAIT_OP (const any&amp;, const any&amp;);// 3. checks if the operator returns void or not// conditions: Lhs!=void and Rhs!=void// we first redefine "operator," so that we have no compilation error if// operator BOOST_TT_TRAIT_OP returns void and we can use the return type of// (lhs BOOST_TT_TRAIT_OP rhs, returns_void_t()) to deduce if// operator BOOST_TT_TRAIT_OP returns void or not:// - operator BOOST_TT_TRAIT_OP returns void -&gt; (lhs BOOST_TT_TRAIT_OP rhs, returns_void_t()) returns returns_void_t// - operator BOOST_TT_TRAIT_OP returns !=void -&gt; (lhs BOOST_TT_TRAIT_OP rhs, returns_void_t()) returns intstruct returns_void_t &#123; &#125;;template &lt;typename T&gt; int operator,(const T&amp;, returns_void_t);template &lt;typename T&gt; int operator,(const volatile T&amp;, returns_void_t);// this intermediate trait has member value of type bool:// - value==true -&gt; operator BOOST_TT_TRAIT_OP returns void// - value==false -&gt; operator BOOST_TT_TRAIT_OP does not return voidtemplate &lt; typename Lhs, typename Rhs &gt;struct operator_returns_void &#123; // overloads of function returns_void make the difference // yes_type and no_type have different size by construction static ::boost::type_traits::yes_type returns_void(returns_void_t); static ::boost::type_traits::no_type returns_void(int); BOOST_STATIC_CONSTANT(bool, value = (sizeof(::boost::type_traits::yes_type)==sizeof(returns_void((make&lt;Lhs&gt;() BOOST_TT_TRAIT_OP make&lt;Rhs&gt;(),returns_void_t())))));&#125;;// 4. checks if the return type is Ret or Ret==dont_care// conditions: Lhs!=void and Rhs!=voidstruct dont_care &#123; &#125;;template &lt; typename Lhs, typename Rhs, typename Ret, bool Returns_void &gt;struct operator_returns_Ret;template &lt; typename Lhs, typename Rhs &gt;struct operator_returns_Ret &lt; Lhs, Rhs, dont_care, true &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = true);&#125;;template &lt; typename Lhs, typename Rhs &gt;struct operator_returns_Ret &lt; Lhs, Rhs, dont_care, false &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = true);&#125;;template &lt; typename Lhs, typename Rhs &gt;struct operator_returns_Ret &lt; Lhs, Rhs, void, true &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = true);&#125;;template &lt; typename Lhs, typename Rhs &gt;struct operator_returns_Ret &lt; Lhs, Rhs, void, false &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = false);&#125;;template &lt; typename Lhs, typename Rhs, typename Ret &gt;struct operator_returns_Ret &lt; Lhs, Rhs, Ret, true &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = false);&#125;;// otherwise checks if it is convertible to Ret using the sizeof trick// based on overload resolution// condition: Ret!=void and Ret!=dont_care and the operator does not return voidtemplate &lt; typename Lhs, typename Rhs, typename Ret &gt;struct operator_returns_Ret &lt; Lhs, Rhs, Ret, false &gt; &#123; static ::boost::type_traits::yes_type is_convertible_to_Ret(Ret); // this version is preferred for types convertible to Ret static ::boost::type_traits::no_type is_convertible_to_Ret(...); // this version is used otherwise BOOST_STATIC_CONSTANT(bool, value = (sizeof(is_convertible_to_Ret(make&lt;Lhs&gt;() BOOST_TT_TRAIT_OP make&lt;Rhs&gt;()))==sizeof(::boost::type_traits::yes_type)));&#125;;// 5. checks for operator existence// condition: Lhs!=void and Rhs!=void// checks if our definition of operator BOOST_TT_TRAIT_OP is used or an other// existing one;// this is done with redefinition of "operator," that returns no_operator or has_operatorstruct has_operator &#123; &#125;;no_operator operator,(no_operator, has_operator);template &lt; typename Lhs, typename Rhs &gt;struct operator_exists &#123; static ::boost::type_traits::yes_type s_check(has_operator); // this version is preferred when operator exists static ::boost::type_traits::no_type s_check(no_operator); // this version is used otherwise BOOST_STATIC_CONSTANT(bool, value = (sizeof(s_check(((make&lt;Lhs&gt;() BOOST_TT_TRAIT_OP make&lt;Rhs&gt;()),make&lt;has_operator&gt;())))==sizeof(::boost::type_traits::yes_type)));&#125;;// 6. main trait: to avoid any compilation error, this class behaves// differently when operator BOOST_TT_TRAIT_OP(Lhs, Rhs) is forbidden by the// standard.// Forbidden_if is a bool that is:// - true when the operator BOOST_TT_TRAIT_OP(Lhs, Rhs) is forbidden by the standard// (would yield compilation error if used)// - false otherwisetemplate &lt; typename Lhs, typename Rhs, typename Ret, bool Forbidden_if &gt;struct trait_impl1;template &lt; typename Lhs, typename Rhs, typename Ret &gt;struct trait_impl1 &lt; Lhs, Rhs, Ret, true &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = false);&#125;;template &lt; typename Lhs, typename Rhs, typename Ret &gt;struct trait_impl1 &lt; Lhs, Rhs, Ret, false &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = ( ::boost::type_traits::ice_and&lt; operator_exists &lt; Lhs, Rhs &gt;::value, operator_returns_Ret &lt; Lhs, Rhs, Ret, operator_returns_void &lt; Lhs, Rhs &gt;::value &gt;::value &gt;::value ) );&#125;;// some specializations needs to be declared for the special void casetemplate &lt; typename Rhs, typename Ret &gt;struct trait_impl1 &lt; void, Rhs, Ret, false &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = false);&#125;;template &lt; typename Lhs, typename Ret &gt;struct trait_impl1 &lt; Lhs, void, Ret, false &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = false);&#125;;template &lt; typename Ret &gt;struct trait_impl1 &lt; void, void, Ret, false &gt; &#123; BOOST_STATIC_CONSTANT(bool, value = false);&#125;;// defines some typedef for conveniencetemplate &lt; typename Lhs, typename Rhs, typename Ret &gt;struct trait_impl &#123; typedef typename ::boost::remove_reference&lt;Lhs&gt;::type Lhs_noref; typedef typename ::boost::remove_reference&lt;Rhs&gt;::type Rhs_noref; typedef typename ::boost::remove_cv&lt;Lhs_noref&gt;::type Lhs_nocv; typedef typename ::boost::remove_cv&lt;Rhs_noref&gt;::type Rhs_nocv; typedef typename ::boost::remove_cv&lt; typename ::boost::remove_reference&lt; typename ::boost::remove_pointer&lt;Lhs_noref&gt;::type &gt;::type &gt;::type Lhs_noptr; typedef typename ::boost::remove_cv&lt; typename ::boost::remove_reference&lt; typename ::boost::remove_pointer&lt;Rhs_noref&gt;::type &gt;::type &gt;::type Rhs_noptr; BOOST_STATIC_CONSTANT(bool, value = (trait_impl1 &lt; Lhs_noref, Rhs_noref, Ret, BOOST_TT_FORBIDDEN_IF &gt;::value));&#125;;#ifndef Q_MOC_RUN&#125; // namespace impl#endif&#125; // namespace detail// this is the accessible definition of the trait to end userBOOST_TT_AUX_BOOL_TRAIT_DEF3(BOOST_TT_TRAIT_NAME, Lhs, Rhs=Lhs, Ret=::boost::detail::BOOST_JOIN(BOOST_TT_TRAIT_NAME,_impl)::dont_care, (::boost::detail::BOOST_JOIN(BOOST_TT_TRAIT_NAME,_impl)::trait_impl &lt; Lhs, Rhs, Ret &gt;::value))&#125; // namespace boost#if defined(BOOST_MSVC)# pragma warning ( pop )#endif#include &lt;boost/type_traits/detail/bool_trait_undef.hpp&gt;]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Gazebo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[erle_copter仿真安装笔记]]></title>
    <url>%2F2019%2F05%2F28%2FROS%2Ferle-copter%E4%BB%BF%E7%9C%9F%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言 Erle_copter仿真无人机飞行器相比于ardone仿真无人机飞行器的优势在于，Erle_copter更偏底层控制一点，使用Erle_copter仿真，可以和自己组装的真机无人机相匹配，而ardrone仿真无人机飞行器更接近于真机的ardrone或bebop，如果想要对真机ardrone或真机bebop进行修改，会有一定的难度，且购买成品无人机比自己组装无人机价格更贵。 安装前提 操作系统：ubuntu16 ROS：kinetic Gazebo：8.6 一、安装gazebo812345$ sudo sh -c 'echo "deb http://packages.osrfoundation.org/gazebo/ubuntu-stable `lsb_release -cs` main" &gt; /etc/apt/sources.list.d/gazebo-stable.list'$ wget http://packages.osrfoundation.org/gazebo.key -O - | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install gazebo8$ sudo apt-get install libgazebo8-dev 二、安装ros kinetci（不安装desktop-full版本）123456789$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/ros-latest.list'$ sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116$ sudo apt-get update$ sudo apt-get install ros-kinetic-desktop$ sudo rosdep init$ rosdep update$ echo "source /opt/ros/kinetic/setup.bash" &gt;&gt; ~/.bashrc$ source ~/.bashrc$ sudo apt-get install python-rosinstall python-rosinstall-generator python-wstool build-essential 三、安装必要ROS程序包12345678910111213$ sudo apt-get install ros-kinetic-gazebo8-msgs$ sudo apt-get install ros-kinetic-gazebo8-ros-control$ sudo apt-get install ros-kinetic-gazebo8-plugins$ sudo apt-get install ros-kinetic-gazebo8-ros-pkgs$ sudo apt-get install ros-kinetic-gazebo8-ros$ sudo apt-get install ros-kinetic-image-view$ sudo apt-get install ros-kinetic-mavlink$ sudo apt-get install ros-kinetic-octomap-msgs$ sudo apt-get install libgoogle-glog-dev protobuf-compiler ros-$ROS_DISTRO-octomap-msgs ros-$ROS_DISTRO-octomap-ros ros-$ROS_DISTRO-joy$ sudo apt-get install libtool automake autoconf libexpat1-dev$ sudo apt-get install ros-kinetic-mavros-msgs$ sudo apt-get install ros-kinetic-gazebo-msgs 四、安装erle_copter仿真环境 安装基础包 12$ sudo apt-get update$ sudo apt-get install gawk make git curl cmake -y 安装MAVProxy依赖 1$ sudo apt-get install g++ python-pip python-matplotlib python-serial python-wxgtk2.8 python-scipy python-opencv python-numpy python-pyparsing ccache realpath libopencv-dev -y 如果安装python-wxgtk2.8报该错误：E: Package ‘python-wxgtk2.8’ has no installation candidate 则按下面方法即可解决 123$ sudo add-apt-repository ppa:nilarimogard/webupd8$ sudo apt-get update$ sudo apt-get install python-wxgtk2.8 安装MAVProxy 1234$ sudo pip install future$ sudo apt-get install libxml2-dev libxslt1-dev -y$ sudo pip2 install pymavlink catkin_pkg --upgrade$ sudo pip install MAVProxy==1.5.2 下载相关程序包 1$ git clone https://github.com/ldgcug/erlecopter_gazebo8.git 安装ArUco 12345678910111213$ cp -r ~/erlecopter_gazebo8/aruco-1.3.0/ ~/Downloads/$ cd ~/Downloads/aruco-1.3.0/build$ cmake ..$ make$ sudo make install说明：如果 cmake .. 或 make 等报错，则删除build文件，重新创建build文件并编译，具体操作如下：$ cd ~/Downloads/aruco-1.3.0/$ rm -rf build/$ mkdir build &amp;&amp; cd build$ cmake ..$ make$ sudo make install 下载ardupilot到特定文件夹 12$ mkdir -p ~/simulation; cd ~/simulation$ git clone https://github.com/erlerobot/ardupilot -b gazebo 创建ros工作空间及初始化工作空间 123456$ mkdir -p ~/simulation/ros_catkin_ws/src$ cd ~/simulation/ros_catkin_ws/src$ catkin_init_workspace$ cd ~/simulation/ros_catkin_ws$ catkin_make$ source devel/setup.bash 拷贝相关源码到工作空间内 123456789101112$ cp -r ~/erlecopter_gazebo8/ardupilot_sitl_gazebo_plugin/ ~/simulation/ros_catkin_ws/src/ $ cp -r ~/erlecopter_gazebo8/hector_gazebo/ ~/simulation/ros_catkin_ws/src/ $ cp -r ~/erlecopter_gazebo8/rotors_simulator/ ~/simulation/ros_catkin_ws/src/ $ cp -r ~/erlecopter_gazebo8/mav_comm/ ~/simulation/ros_catkin_ws/src/ $ cp -r ~/erlecopter_gazebo8/glog_catkin/ ~/simulation/ros_catkin_ws/src/ $ cp -r ~/erlecopter_gazebo8/catkin_simple/ ~/simulation/ros_catkin_ws/src/ $ cp -r ~/erlecopter_gazebo8/mavros/ ~/simulation/ros_catkin_ws/src/$ cp -r ~/erlecopter_gazebo8/gazebo_ros_pkgs/ ~/simulation/ros_catkin_ws/src/添加Python和C++样例$ cp -r ~/erlecopter_gazebo8/gazebo_cpp_examples/ ~/simulation/ros_catkin_ws/src/$ cp -r ~/erlecopter_gazebo8/gazebo_python_examples/ ~/simulation/ros_catkin_ws/src/ 拷贝fix-unused-typedef-warning.patch文件到工作空间内 1$ cp -r ~/erlecopter_gazebo8/fix-unused-typedef-warning.patch ~/simulation/ros_catkin_ws/src/ 安装drcsim7（ubuntu16不支持apt-get，需使用源码下载安装） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu xenial main" &gt; /etc/apt/sources.list.d/ros-latest.list'$ wget http://packages.ros.org/ros.key -O - | sudo apt-key add -$ sudo apt-get update# Install osrf-common's dependencies$ sudo apt-get install -y cmake \ debhelper \ ros-kinetic-ros \ ros-kinetic-ros-comm # Install sandia-hand's dependencies$ sudo apt-get install -y ros-kinetic-xacro \ ros-kinetic-ros \ ros-kinetic-image-common \ ros-kinetic-ros-comm \ ros-kinetic-common-msgs \ libboost-dev \ avr-libc \ gcc-avr \ libqt4-dev # Install gazebo-ros-pkgs $ sudo apt-get install -y libtinyxml-dev \ ros-kinetic-opencv3 \ ros-kinetic-angles \ ros-kinetic-cv-bridge \ ros-kinetic-driver-base \ ros-kinetic-dynamic-reconfigure \ ros-kinetic-geometry-msgs \ ros-kinetic-image-transport \ ros-kinetic-message-generation \ ros-kinetic-nav-msgs \ ros-kinetic-nodelet \ ros-kinetic-pcl-conversions \ ros-kinetic-pcl-ros \ ros-kinetic-polled-camera \ ros-kinetic-rosconsole \ ros-kinetic-rosgraph-msgs \ ros-kinetic-sensor-msgs \ ros-kinetic-trajectory-msgs \ ros-kinetic-urdf \ ros-kinetic-dynamic-reconfigure \ ros-kinetic-rosgraph-msgs \ ros-kinetic-tf \ ros-kinetic-cmake-modules # Install drcsim's dependencies $ sudo apt-get install -y cmake debhelper \ ros-kinetic-std-msgs ros-kinetic-common-msgs \ ros-kinetic-image-common ros-kinetic-geometry \ ros-kinetic-robot-state-publisher \ ros-kinetic-image-pipeline \ ros-kinetic-image-transport-plugins \ ros-kinetic-compressed-depth-image-transport \ ros-kinetic-compressed-image-transport \ ros-kinetic-theora-image-transport \ ros-kinetic-ros-controllers \ ros-kinetic-moveit-msgs \ ros-kinetic-joint-limits-interface \ ros-kinetic-transmission-interface \ ros-kinetic-laser-assembler $ sudo apt-get install ros-kinetic-pr2-controllers 拷贝drcsim相关包到工作空间 123$ cp -r ~/erlecopter_gazebo8/osrf-common/ ~/simulation/ros_catkin_ws/src/$ cp -r ~/erlecopter_gazebo8/sandia-hand/ ~/simulation/ros_catkin_ws/src/$ cp -r ~/erlecopter_gazebo8/drcsim/ ~/simulation/ros_catkin_ws/src/ source一下 1$ source /opt/ros/kinetic/setup.bash 修改 has_binary_operator.hpp文件（为避免包BOOST_JOIN错误） 1$ sudo gedit /usr/include/boost/type_traits/detail/has_binary_operator.hpp 点击此处，拷贝其中的has_binary_operator.hpp代码，并粘贴至当前的has_binary_operator.hpp 文件中 主要的修改是在源文件中的两处位置添加了 #ifndef Q_MOC_RUN 和#endif 下载相应包进行替换（替换掉原工作空间的drcsim、hector_gazebo、gazebo_ros_pkgs） 链接: https://pan.baidu.com/s/1TufCNJ8z5TxyC5rnZhi56A 提取码: usjz 下载文件主要包含三个文件，分别是drcsim、hector_gazebo、gazebo_ros_pkgs，将它们解压，并将（drcsim、hector_gazebo、gazebo_ros_pkgs）复制到~/simulation/ros_catkin_ws/src目录下，将前面的三个文件进行替换 编译工作空间 1234$ cd ~/simulation/ros_catkin_ws$ catkin_make --pkg mav_msgs mavros_msgs gazebo_msgs$ source devel/setup.bash$ catkin_make -j 4 下载gazebo模型 123$ mkdir -p ~/.gazebo/models$ git clone https://github.com/erlerobot/erle_gazebo_models$ mv erle_gazebo_models/* ~/.gazebo/models 五、启动erle_copter 启动ArduCopter（一个终端） 123$ source ~/simulation/ros_catkin_ws/devel/setup.bash$ cd ~/simulation/ardupilot/ArduCopter$ ../Tools/autotest/sim_vehicle.sh -j 4 -f Gazebo 在另一个终端启动launch 123$ cd ~/simulation/ros_catkin_ws/$ source ~/simulation/ros_catkin_ws/devel/setup.bash$ roslaunch ardupilot_sitl_gazebo_plugin erlecopter_spawn.launch 在第一个终端上输入如下命令： 1$ param load /[path_to_your_home_directory]/simulation/ardupilot/Tools/Frame_params/Erle-Copter.param 用你的实际目录替换掉上面的path_to_your_home_directory，如我的是：param load /home/cug/simulation/ardupilot/Tools/Frame_params/Erle-Copter.param 起飞测试，仍然在第一个终端执行 123$ mode GUIDED$ arm throttle$ takeoff 2 说明：在执行了arm throttle后，尽快输入takeoff 2 已完成起飞 官网安装网址： http://docs.erlerobotics.com/simulation/configuring_your_environment Github安装网址： https://github.com/ldgcug/erlecopter_gazebo8/blob/master/README.md 总结 在前面的安装过程中，中间有一个步骤是从百度云盘上下载了三个文件夹，并将之前的工作空间内的相关文件夹进行替换，之所以这样做，是因为在将文件上传到github的过程中，好像有部分文件丢失，因此下载了这缺失的部分文件，在编译过程中会报错，因此将文件解压并上传到百度云盘上以解决该问题 第二个是还是从别人的github上git程序包，原因如上，但是也压缩上传到百度云，下载后替换掉没有换，由于这个不在工作空间内，因此不影响编译，但是在运行时，会报权限的相关错误。因此还是从别人的github上下载较好。]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Gazebo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多个csv文件合并成一个csv]]></title>
    <url>%2F2019%2F05%2F26%2F%E8%AF%AD%E8%A8%80%2FPython%2Fpython%E5%A4%9A%E4%B8%AAcsv%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%E6%88%90%E4%B8%80%E4%B8%AAcsv%2F</url>
    <content type="text"><![CDATA[前言 在训练过程中，会产生多个txt转的csv文件，最后需要合并成一个完整的csv。 12345678910111213141516# -*- coding:utf8 -*-import globimport timecsvx_list = glob.glob(&apos;*.csv&apos;)print(&apos;总共发现%s个CSV文件&apos;% len(csvx_list))time.sleep(2)print(&apos;正在处理............&apos;)for i in csvx_list: fr = open(i,&apos;r&apos;).read() with open(&apos;csv_to_csv.csv&apos;,&apos;a&apos;) as f: f.write(fr) print(&apos;写入成功！&apos;)print(&apos;写入完毕！&apos;)print(&apos;10秒钟自动关闭程序！&apos;)time.sleep(10)]]></content>
      <categories>
        <category>语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ros在多台计算机之间的通信]]></title>
    <url>%2F2019%2F05%2F26%2FROS%2Fros%E5%9C%A8%E5%A4%9A%E5%8F%B0%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[前言 利用ROS的性能，能实现两台机器之间的网络通信，并且具有跨系统性（可以在不同的ros版本版本之间通信） 需要两台机器在同一个局域网内 这里默认已经装好ROS（可以为不同版本） 一、两台电脑通信前的准备工作1、查看两台电脑各自的用户名和IP信息 在终端输入who能查看用户名，输入ifconfig能查看IP信息 如：我这里的两台服务器的用户名和IP分别是 cug_local，192.168.1.57 cug_master，192.168.1.58 2、修改/etc文件夹下的hosts文件 修改的目的是将两台电脑的ip和用户名绑定，这样在ping对方用户名时，可以解析成功 （1）修改权限 1sudo chmod a+w /etc/hosts （2）在/etc/hosts最后两行添加代码 1sudo gedit /etc/hosts 添加的第一行是本机的IP和用户名 添加的第二行是另一台机器的IP和用户名 （3）重启网络 1sudo /etc/init.d/networking restart 如果无法重启网络，可以参考该网址 两台电脑都做上面三个步骤操作 二、两台电脑间通信测试1、安装chrony 两台电脑上都安装chrony包，用于实现同步 1sudo apt-get install chrony 2、安装ssh服务端 两台电脑上都安装ssh服务端（默认ubuntu系统自带ssh客户端） 1sudo apt-get install openssh-server 服务端启动测试 1ps -e |grep ssh 如果看到了sshd，说明ssh-server已经启动成功 3、ping测试（1）cug_local机器ping 机器cug_master 12ssh cug_localping cug_master 如果出现如下信息，则通信正常 （2）反向测试，cug_master机器ping机器cug_local 12ssh cug_masterping cug_local 三、~/.bashrc配置 说明：假设将cug_master机器当做主机master （1）在cug_local机器的~/.bashrc文件中添加如下两行代码 12export ROS_HOSTNAME=cug_localexport ROS_MASTER_URI=http://cug_master:11311 添加完后，需要source一下 1source ~/.bashrc （2）在cug_master机器的~/.bashrc文件中添加如下两行代码 12export ROS_HOSTNAME=cug_masterexport ROS_MASTER_URI=http://cug_master:11311 添加完后，需要source一下 1source ~/.bashrc 四、ros通信 说明：假设将cug_master机器当做主机master 1、在cug_master机器上执行如下命令（1）打开一个新终端，输入roscore （2）打开另一个新终端，输入如下命令 1rosrun rospy_tutorials listener.py 2、在cug_local机器上执行如下命令打开一个新终端，输入如下命令 1rosrun rospy_tutorials talker.py 3、结果显示（1）cug_master机器上显示（rosrun的终端上） （2）cug_local机器上显示（rosrun的终端上） 如上两图显示，则整个配置成功 五、架构图 参考网址 网址1 网址2]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS+Gazebo+Ardrone关系及基本知识学习]]></title>
    <url>%2F2019%2F05%2F25%2FROS%2FROS-Gazebo-Ardrone%E5%85%B3%E7%B3%BB%E5%8F%8A%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前言 主要记录ros的一些基本指令和gazebo及ardrone的一些名词解释和相互关系 1、名词解释 ROS：ROS(Robot Operating System, 机器人操作系统)是一个适用于机器人的开源的元操作系统。它提供了操作系统应有的服务：如硬件抽象、设备驱动、函数库、可视化工具、消息传递和软件包管理等诸多功能。 Gazebo：可以主要用来进行机器人动力学的仿真。 Ardrone：四轴飞行器，支持ROS系统。 2、相互关系 3、Gazebo基本组成部分 4、ROS基本概念 节点（node）：一个节点即为一个可执行文件，它可以通过ROS与其他节点进行通信 例子：咱们有一个机器人，和一个遥控器，那么这个机器人和遥控器开始工作后，就是两个节点。遥控器起到了下达指 令的作用；机器人负责监听遥控器下达的指令，完成相应动作。从这里我们可以看出，节点是一个能执行特定工作任 务的工作单元，并且能够相互通信，从而实现一个机器人系统整体的功能。在这里我们把遥控器和机器人简单定义为两个节点，实际上在机器人中根据控制器、传感器、执行机构等不同组成模块，还可以将其进一步细分为更多的节点，这个是根据用户编写的程序来定义的。 消息（message）：消息是一种ROS数据类型，用于订阅或发布到一个话题。 消息是一种数据结构，支持多种数据类型（整形、浮点、布尔型、数组等），同时也支持消息的嵌套定义。ROS提供了大量的系统默认消息供用户使用，如geometry_msgs、sensor_msgs等，同时也支持用户定义专属数据结构的消息类型。 话题（Topic）：节点可以发布消息到话题，也可以订阅话题以接收消息。 话题是消息的载体，作用是用不同的名称区分不同消息。 话题与消息是紧密联系在一起的。话题就像公交车，消息是公交车里装的人。公交车里可以没有人（话题上没有有效消息），但能装什么人一定会预先指定（话题一定有类型）。整个公交网络中线路名称不能重复（话题名称不能重复），要是真有两个话题名称相同类型也相同，ROS不会对其中的数据做区分，这种冲突是没有提示的。 订阅/发布话题是不同步的，发布的人只管说话，订阅的人只管偷听，发布的人连续说了100句话，这100句话会排成一个队列，偷听的人要一句一句听，哦，对了，偷听的人可能不止一个 服务（service）：服务是应答响应模式下的信息交互方式。这种方式是基于客户端/服务器模型的。 与话题不同的是，当服务端收到服务请求后，会对请求做出响应，将数据的处理结果返回给客户端。这种模式更适用于双向同步的信息传输，在同一个ROS网络中节点指定服务名称时不能重名。当节点A找节点B借钱时，整个网络里只有一个B，谁要是冒充B借了钱，那他就是2B。 master：节点管理器，ROS名称服务 (比如帮助节点找到彼此)。 master是整个ROS运行的核心，它主要的功能就是登记注册节点、服务和话题的名称，并维护一个参数服务器。没有它你就甭想启动任何一个节点，roscore就是用来启动master的。 参考网址：ROS官网、转载 5、ROS Ardrone常用命令 起飞： 1rostopic pub -1 /ardrone/takeoff std_msgs/Empty 降落 1rostopic pub -1 /ardrone/land std_msgs/Empty 切换相机 1rosservice call /ardrone/togglecam 获取前置相机图像 1rosrun image_view image_view image:=/ardrone/front/image_raw 获取下置相机图像 1rosrun image_view image_view image:=/ardrone/bottom/image_raw 6、ROS常用信息显示（1）rostopic list ：显示所有的话题信息 （2）rostopic echo [topic] ：显示发布的话题的数据信息 如：rostopic echo /ardrone/navdata （3）rostopic type [topic]：返回发布的话题的消息类型 （4）rostopic show ardrone_autonomy/Navdata：显示ardrone_autonomy/Navdata 参考网址：ros官网]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Gazebo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gazebo配置]]></title>
    <url>%2F2019%2F05%2F24%2FROS%2Fgazebo%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言 记录的是研究生阶段以来使用gazebo过程中遇到过的一些坑，以及一些相关的gazebo配置 1、gazebo模型文件说明 如果不是安装的gazebo8，如默认安装的ros kinetic版本对应的gazebo7和ros indigo版本对应的gazebo2的话，在终端输入gazebo的时候，会卡着，gazebo界面出不来。其原因是因为没有下载gazebo的模型，下载完后将模型拷贝到~/.gazebo/models/文件夹即可。 值得注意的是：~/.gazebo/models/文件夹下只需要有sun模型文件和ground_plane模型文件就能正常打开gazebo界面 2、gazebo_model_path配置 针对以往的总是要将gazebo model文件放在~/.gazebo/models文件，而不能存放在自己下载的ros程序包下的model文件夹下，主要是因为环境变量没有配置，配置好后就可以将所有的模型文件放在自己想放的位置。 （1）首先在~/.bashrc最后一行添加gazebo_model_path路径 1export GAZEBO_MODEL_PATH="/home/cugrobot/catkin_ws/src/ardrone_simulator_gazebo7/cvg_sim_gazebo/models" 其对应的模型文件夹如下： （2）然后，在launch启动文件夹下，添加env代码，如下 通过env | grep GAZEBO_MODEL_PATH命令可以查看其配置路径，同理可以应用于其他路径查看 3、gazebo关闭client界面 在做强化学习训练时，打开gazebo界面可能会使训练比较耗时，因此关闭client界面也许是一种比较好的方法。 gazebo平台第三视角关闭方法如下： 123$ roscd gazebo_ros$ cd launch$ sudo gedit empty_world.launch 在打开的界面中修改两处地方，第一处在第7行，将其中的true改为false；第二处在第41行，有一个 的注释，将注释范围扩大，将42-44行全部注释掉，即该后面的整个group注释。 若上面方法还不能关闭界面，在launch启动文件里面，找到所有相关联的启动文件，将上面的修改方法在launch里面也执行一遍。 4、gazebo仿真世界中模型位置修改4.1 通过os.system()函数实现rosservice服务 我们能在终端通过调用 /gazebo/set_model_state服务来重置pose和twist，这是最初始的时候的方法，但在测试过程中，重置位置容易出现在限定范围外，因此不好，关于修改模型位置方法可以参考4.2节 1rosservice call /gazebo/set_model_state &apos;&#123;model_state: &#123; model_name: quadrotor, pose: &#123; position: &#123; x: 5, y: 0 ,z: 1 &#125;, orientation: &#123;x: 0, y: 0.491983115673, z: 0, w: 0.870604813099 &#125; &#125;, twist: &#123; linear: &#123;x: 0.0 , y: 0 ,z: 0 &#125; , angular: &#123; x: 0.0 , y: 0 , z: 0.0 &#125; &#125; , reference_frame: world &#125; &#125;&apos; 其中，需要说明的是quadrotor是你要修改的模型名称 参考网址：http://wiki.ros.org/simulator_gazebo/Tutorials/Gazebo_ROS_API 如果要在python程序中调用上面的重置命令，则需要使用os.system（）函数来实现 1os.system(&apos;&apos;&apos;rosservice call /gazebo/set_model_state &quot;[quadrotor, [[5, 0, 0], [0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], &apos;&apos;]&quot;&apos;&apos;&apos; 如果写成动态加载代码的话，可以如下图所示： 参考网址：https://blog.csdn.net/lordofrobots/article/details/78088517?utm_source=debugrun&amp;utm_medium=referral 4.2 rospy.ServiceProxy()函数实现位置修改和获取模型位置 重置模型坐标位置主要是调用/gazebo/set_model_state服务，而获取模型坐标位置是调用/gazebo/get_model_state服务 其中，如/gazebo/set_model_state的type类型可以通过rosservice info /gazebo/set_model_state命令来确定 具体代码如下： 1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/env python#coding=utf8import rospyfrom gazebo_msgs.srv import *# 重置无人机坐标位置def set_model_pos(): rospy.wait_for_service('/gazebo/set_model_state') set_state_service = rospy.ServiceProxy('/gazebo/set_model_state',SetModelState) objstate = SetModelStateRequest() #set quadrotor pose objstate.model_state.model_name = 'quadrotor' objstate.model_state.pose.position.x = 5 objstate.model_state.pose.position.y = 0 objstate.model_state.pose.position.z = 0 objstate.model_state.pose.orientation.w = 1 objstate.model_state.pose.orientation.x = 0 objstate.model_state.pose.orientation.y = 0 objstate.model_state.pose.orientation.z = 0 objstate.model_state.twist.linear.x = 0.0 objstate.model_state.twist.linear.y = 0.0 objstate.model_state.twist.linear.z = 0.0 objstate.model_state.twist.angular.x = 0.0 objstate.model_state.twist.angular.y = 0.0 objstate.model_state.twist.angular.z = 0.0 objstate.model_state.reference_frame = "world" result = set_state_service(objstate) # 获取无人机坐标位置def get_model_pos(): get_state_service = rospy.ServiceProxy('/gazebo/get_model_state',GetModelState) model = GetModelStateRequest() model.model_name = 'quadrotor' objstate = get_state_service(model) state = (objstate.pose.position.x,objstate.pose.position.y,objstate.pose.position.z) print('pos',state) 该代码转载于此处 5、gazebo仿真时间加速 主要遇到的问题是在进行DQN训练时，由于机器性能及每步飞行时间等原因，训练时长较久，因此能够在仿真中修改一些配置，使得仿真的时间比现实时间更快。 主要是通过修改world文件里面的一些物理属性，来实现仿真时间加速的效果，若需要加速时，还是在重新测试比较好，我修改为如下代码后，较之前能有3倍左右的提升速度，并且没有使用rospy.sleep(2)函数，而是使用的rospy.Rate(0.46).sleep()来代替。 12345&lt;physics name=&apos;default_physics&apos; default=&apos;0&apos; type=&apos;ode&apos;&gt; &lt;real_time_update_rate&gt;0&lt;/real_time_update_rate&gt; &lt;max_step_size&gt;0.002&lt;/max_step_size&gt; &lt;real_time_factor&gt;1&lt;/real_time_factor&gt;&lt;/physics&gt; 通过修改max_step_size的值能对gazebo进行加速，一般修改的时候real_time_update_rate设置为0。 Max_step_size:0.001（默认值） Real_time_update_rate:1（默认值） Real_time_update_rate:1000（默认值） 参考网址：http://gazebosim.org/tutorials?tut=modifying_world&amp;cat=build_world 6、gazebo添加定制模型 以前都是自己制作的一些简单模型导入到gazebo仿真世界中，但想加载一些特定的模型时，不一定能自己制作出来，这时可以下载3D Warehouse网站上做好的模型，导入到gazebo仿真直接中即可 例如，这样的模型则不一定能自己制作出来 因此，我们可以在3D Warehouse上搜索关键词，然后找到想要的模型，点击进去后，以Collada File文件形式下载即可。 （1）下载完后，解压，解压后更改dae的名字。然后在gazebo_model_path的文件夹目录下创建相对应的模型文件，主要包括model.config、model.sdf文件和mesh文件夹（文件夹下只有dae文件） 其中，model.sdf文件内容如下： 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; ?&gt;&lt;sdf version=&quot;1.4&quot;&gt; &lt;model name=&quot;Arc&quot;&gt; &lt;pose&gt;0 5 0 0 0 0&lt;/pose&gt; &lt;static&gt;true&lt;/static&gt; &lt;link name=&quot;link&quot;&gt; &lt;inertial&gt; &lt;mass&gt;0.1&lt;/mass&gt; &lt;/inertial&gt; &lt;collision name=&quot;collision&quot;&gt; &lt;geometry&gt; &lt;box&gt; &lt;size&gt;10 10 0.2&lt;/size&gt; &lt;/box&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;mesh&gt; &lt;!--uri&gt;model://marker/meshes/artag_01.dae&lt;/uri--&gt; &lt;uri&gt;model://arc/meshes/arc.dae&lt;/uri&gt; &lt;!-- &lt;scale&gt; 0.01 0.01 0.01 &lt;/scale&gt;--&gt; &lt;/mesh&gt; &lt;/geometry&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/model&gt;&lt;/sdf&gt; （2）测试 创建arc.world文件，并将如下内容拷贝进去 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=&quot;1.0&quot;?&gt;&lt;sdf version=&quot;1.4&quot;&gt; &lt;world name=&quot;default&quot;&gt; &lt;include&gt; &lt;uri&gt;model://ground_plane&lt;/uri&gt; &lt;/include&gt; &lt;include&gt; &lt;uri&gt;model://sun&lt;/uri&gt; &lt;/include&gt;&lt;!-- &lt;model name=&quot;arc&quot;&gt; &lt;pose&gt;0 0 0 0 0 0&lt;/pose&gt; &lt;static&gt;true&lt;/static&gt; &lt;link name=&quot;body&quot;&gt; &lt;visual name=&quot;visual&quot;&gt; &lt;geometry&gt; &lt;mesh&gt; &lt;uri&gt;file://arc.dae&lt;/uri&gt; &lt;scale&gt; 0.03 0.03 0.03&lt;/scale&gt; &lt;/mesh&gt; &lt;/geometry&gt; &lt;/visual&gt; &lt;/link&gt; &lt;/model&gt;--&gt; &lt;model name=&apos;arc&apos;&gt; &lt;static&gt;1&lt;/static&gt; &lt;link name=&apos;arc_link&apos;&gt; &lt;pose frame=&apos;&apos;&gt;0 0 0 0 -0 0&lt;/pose&gt; &lt;collision name=&apos;collision&apos;&gt; &lt;geometry&gt; &lt;mesh&gt; &lt;uri&gt;model://arc/meshes/arc.dae&lt;/uri&gt; &lt;scale&gt;0.03 0.03 0.03&lt;/scale&gt; &lt;/mesh&gt; &lt;/geometry&gt; &lt;/collision&gt; &lt;visual name=&apos;visual&apos;&gt; &lt;geometry&gt; &lt;mesh&gt; &lt;uri&gt;model://arc/meshes/arc.dae&lt;/uri&gt; &lt;scale&gt;0.03 0.03 0.03&lt;/scale&gt; &lt;/mesh&gt; &lt;/geometry&gt; &lt;/visual&gt; &lt;gravity&gt;1&lt;/gravity&gt; &lt;/link&gt; &lt;pose frame=&apos;&apos;&gt;0 0 0.05 0 0 0&lt;/pose&gt; &lt;/model&gt; &lt;/world&gt;&lt;/sdf&gt; 说明：上面代码中注销掉的部分如 file://arc.dae是以文件形式导入，而后面model://arc/meshes/arc.dae是以sdf形式导入。 需要注意的是：需要使用0.03 0.03 0.03来对模型进行调节大小 （3）显示 运行gazebo arc.world 参考网址：http://gazebosim.org/tutorials?tut=import_mesh#PreparetheMesh https://answers.ros.org/question/42529/how-to-import-collada-dae-files-into-gazebo-rosfuerte/ 7、Gazebo场景纹理图重置 主要是我在做强化学习（DQN）训练的过程中，需要将纹理时常更换，因此在网上查找相关教程，最终实现了gazebo的场景重置 先看代码，后面在进行解释，该代码是从训练的代码中截取的部分，需要的内容全部都在了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#!/usr/bin/env python#coding=utf8from geometry_msgs.msg import Posefrom gazebo_msgs.srv import *import rospyimport roslib;roslib.load_manifest('test')class DQN(): def __init__(self): self.pubModelStates = rospy.Subscriber('gazebo/model_states',ModelStates,self.get_model_pos) self.database_model_name = \ ["asphalt1","asphalt2","asphalt3","asphalt4", "brick1","brick2","brick3","brick4", ] #重置特定模型位置 def set_model_pos(self): rospy.wait_for_service('/gazebo/set_model_state') set_state_service = rospy.ServiceProxy('/gazebo/set_model_state',SetModelState) objstate = SetModelStateRequest() #set quadrotor pose objstate.model_state.model_name = 'quadrotor' objstate.model_state.pose.position.x = 5 objstate.model_state.pose.position.y = 0 objstate.model_state.pose.position.z = 0 objstate.model_state.pose.orientation.w = 1 objstate.model_state.pose.orientation.x = 0 objstate.model_state.pose.orientation.y = 0 objstate.model_state.pose.orientation.z = 0 objstate.model_state.twist.linear.x = 0.0 objstate.model_state.twist.linear.y = 0.0 objstate.model_state.twist.linear.z = 0.0 objstate.model_state.twist.angular.x = 0.0 objstate.model_state.twist.angular.y = 0.0 objstate.model_state.twist.angular.z = 0.0 objstate.model_state.reference_frame = "world" result = set_state_service(objstate) #获取特定模型位置 def get_model_pos(self): get_state_service = rospy.ServiceProxy('/gazebo/get_model_state',GetModelState) model = GetModelStateRequest() model.model_name = 'quadrotor' objstate = get_state_service(model) state = (objstate.pose.position.x,objstate.pose.position.y,objstate.pose.position.z) print('pos',state) #删除模型文件 def delete_sdf_model(self): rospy.wait_for_service('gazebo/delete_model') delete_model_service = rospy.ServiceProxy('gazebo/delete_model',DeleteModel) objstate = DeleteModelRequest() objstate.model_name = "grass7_plane" if objstate.model_name in self.database_model_name: try: delete_model_service(objstate) print('delete model success') except Exception as e: print("delete model failed") self.spawn_sdf_model() #重置模型文件 def spawn_sdf_model(self): rospy.wait_for_service('gazebo/spawn_sdf_model') spawn_model_service = rospy.ServiceProxy('gazebo/spawn_sdf_model',SpawnModel) with open("/home/cug/qlab_ws/src/qlab/qlab/qlab_gazebo/models/asphalt1/model.sdf","r") as f: model_xml = f.read() #model_name model_xml robot_namespace initial_pose reference_frame objstate = SpawnModelRequest() objstate.model_name = "asphalt1" objstate.model_xml = model_xml objstate.robot_namespace = "" pose = Pose() pose.position.x = 0.222657 pose.position.y = -0.204052 pose.position.z = 0 objstate.initial_pose = pose objstate.reference_frame = "world" try: #spawn_model_service("asphalt1_plane",model_xml,"",pose,"world") spawn_model_service(objstate) print('spawn model success') except Exception as e: print('spawn model failed') if __name__ == "__main__": rospy.init_node('test') dqn = DQN() dqn.delete_sdf_model() dqn.set_model_pos() dqn.get_model_pos( 说明：gazebo场景重置，主要用到两个rosservice，分别是gazebo/delete_model和gazebo/spawn_sdf_model 在重置的过程中，首先需要先删除模型，即调用gazebo/delete_model服务，然后在重新生成，这时需要找到想要生成的模型的sdf文件所在位置，然后读取并调用gazebo/spawn_sdf_model服务，即可实现gazebo场景纹理图的重置 主要需要查看的帮助信息是 （1）roservice list：查找相关service服务 （2）rosservice info [service_name]：查找对应服务的数据类型 对其中一个进行解释说明：如 1234567891011objstate = SpawnModelRequest()objstate.model_name = "asphalt1"objstate.model_xml = model_xmlobjstate.robot_namespace = ""pose = Pose()pose.position.x = 0.222657 pose.position.y = -0.204052pose.position.z = 0objstate.initial_pose = poseobjstate.reference_frame = "world" 这里就和/gazebo/spawn_sdf_model里面的Args对应，分别是model_name、model_xml、robot_namespace、initial_pose、reference_frame 又如： 12345678910111213141516171819202122#重置特定模型位置 def set_model_pos(self): rospy.wait_for_service('/gazebo/set_model_state') set_state_service = rospy.ServiceProxy('/gazebo/set_model_state',SetModelState) objstate = SetModelStateRequest() #set quadrotor pose objstate.model_state.model_name = 'quadrotor' objstate.model_state.pose.position.x = 5 objstate.model_state.pose.position.y = 0 objstate.model_state.pose.position.z = 0 objstate.model_state.pose.orientation.w = 1 objstate.model_state.pose.orientation.x = 0 objstate.model_state.pose.orientation.y = 0 objstate.model_state.pose.orientation.z = 0 objstate.model_state.twist.linear.x = 0.0 objstate.model_state.twist.linear.y = 0.0 objstate.model_state.twist.linear.z = 0.0 objstate.model_state.twist.angular.x = 0.0 objstate.model_state.twist.angular.y = 0.0 objstate.model_state.twist.angular.z = 0.0 objstate.model_state.reference_frame = "world" 这里需要设置很多position和orientation和twist是因为如下图所示包含的信息 参考网址：网址1、网址2、网址3、网址4]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Gazebo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卸载原版gazebo并安装新版gazebo]]></title>
    <url>%2F2019%2F05%2F22%2FROS%2F%E5%8D%B8%E8%BD%BD%E5%8E%9F%E7%89%88gazebo%E5%B9%B6%E5%AE%89%E8%A3%85%E6%96%B0%E7%89%88gazebo%2F</url>
    <content type="text"><![CDATA[前言 从最初始在ubuntu14上安装ros indigo版本，到后面在ubuntu16安装ros kinetic版本，中间遇到过需要安装新版本gazebo的问题，如u14上安装ros后，默认安装gazebo2，可能需要改成gazebo7；u16上安装ros后，默认安装gazebo7，可能需要改成gazebo8 卸载gazebo2.2安装gazebo7网址：点击此处 卸载gazebo7安装gazebo8步骤如下： （1）卸载ros-kinetic-desktop-full 1$ sudo apt-get remove ros-kinetic-desktop-full （2）卸载gazebo7 1$ sudo apt-get remove gazebo-* （3）安装gazebo8 12345$ sudo sh -c 'echo "deb http://packages.osrfoundation.org/gazebo/ubuntu-stable `lsb_release -cs` main" &gt; /etc/apt/sources.list.d/gazebo-stable.list'$ wget http://packages.osrfoundation.org/gazebo.key -O - | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install gazebo8$ sudo apt-get install libgazebo8-dev 说明：第一步就卸载了ros-kinetic-desktop-full，因此需要重新安装ros-kinetic-desktop。 特别注意：此次安装没有full，有full的则会默认安装 （4）安装ros-kinetic-desktop 123456789$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/ros-latest.list'$ sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116$ sudo apt-get update$ sudo apt-get install ros-kinetic-desktop$ sudo rosdep init$ rosdep update$ echo "source /opt/ros/kinetic/setup.bash" &gt;&gt; ~/.bashrc$ source ~/.bashrc$ sudo apt-get install python-rosinstall python-rosinstall-generator python-wstool build-essential （5）安装一些必要ros包 123456$ sudo apt-get install ros-kinetic-gazebo8-msgs$ sudo apt-get install ros-kinetic-gazebo8-ros-control$ sudo apt-get install ros-kinetic-gazebo8-plugins$ sudo apt-get install ros-kinetic-gazebo8-ros-pkgs$ sudo apt-get install ros-kinetic-gazebo8-ros$ sudo apt-get install ros-kinetic-image-view 其他可能帮助信息 如果需要卸载ros的话，参考如下命令： 123$ sudo apt-get purge ros-*$ sudo rm -rf /etc/ros$ gedit ~/.bashrc 找到：带有kinetic的那一行删除，保存，然后： 1$ source ~/.bashrc 如果不删掉这一行或者在他之前多余的命令，那么你会在打开终端后发现第一行永远是个报错信息，虽然有时候没有什么影响。 参考网址：网址1]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16安装ROS]]></title>
    <url>%2F2019%2F05%2F22%2FROS%2Fubuntu16%E5%AE%89%E8%A3%85ROS%2F</url>
    <content type="text"><![CDATA[ubuntu与ros对应版本关系（我目前更多的用的是ubuntu16） ubuntu16下ros安装步骤 123456789$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/ros-latest.list'$ sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116$ sudo apt-get update$ sudo apt-get install ros-kinetic-desktop-full$ sudo rosdep init$ rosdep update$ echo "source /opt/ros/kinetic/setup.bash" &gt;&gt; ~/.bashrc$ source ~/.bashrc$ sudo apt-get install python-rosinstall python-rosinstall-generator python-wstool build-essential 安装完成后，在终端输入roscore，若最后出现roscore，则说明安装成功 原文：ros官网 总结：上面安装步骤安装的是ros kinetic 桌面完全版，并且安装完后，默认安装了gazebo7。但在终端输入gazebo，并不会出现gazebo界面，原因是缺少gazebo的两个最基本模型文件（sun和ground_plane）。gazebo_models文件下载，点击此处。]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Youtube视频链接下载方法]]></title>
    <url>%2F2019%2F05%2F21%2F%E7%94%9F%E6%B4%BB%2F%E8%A7%86%E9%A2%91%2FYoutube%E8%A7%86%E9%A2%91%E9%93%BE%E6%8E%A5%E4%B8%8B%E8%BD%BD%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在线下载Youtube视频链接网址：1https://www.vidpaw.com/cn]]></content>
      <categories>
        <category>生活</category>
        <category>视频</category>
      </categories>
      <tags>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16软件安装]]></title>
    <url>%2F2019%2F05%2F21%2FLinux%2Fubuntu16%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前言 仅用来在ubuntu上安装一些平时常用软件 1、Teamviewer安装 Teamviewer下载，Ubuntu系统下，文件默认下载到~/Downloads目录下 安装步骤 12$ cd ~/Downloads$ sudo dpkg -i *.deb 在执行上面的sudo dpkg -i步骤后，会出现一个Error报错，不用着急，执行下面命令处理依赖即可 1$ sudo apt-get install –f 说明：有时候会遇到Teamviewer无法打开，即双击Teamviewer无法显示，此时只需要命令启动即可 123$ teamviewer --daemon stop$ teamviewer --daemon start 2、搜狗输入法安装 下载地址，Ubuntu系统下，文件默认下载到~/Downloads目录下 安装步骤 12$ cd ~/Downloads$ sudo dpkg -i *.deb 在执行上面的sudo dpkg -i步骤后，会出现一个Error报错，不用着急，执行下面命令处理依赖即可 1$ sudo apt-get install –f 输入法配置 （1）安装完成后，在电脑设置里面找到Language Support （2）键盘输入方式选择：fctix （3）若没有fctix，在终端输入命令进行安装 1$ sudo apt-get install fcitx （4）注销退出，重新登录进去 如果还不能切换中文输入法，参考该网址 若出现中文输入乱码情况，解决方法如下： 12$ cd ~/.config$ rm -rf SogouPY* sogou* 执行完后，重启电脑即可 3、Vscode安装1234$ sudo add-apt-repository ppa:ubuntu-desktop/ubuntu-make$ sudo apt-get update$ sudo apt-get install ubuntu-make$ sudo umake ide visual-studio-code 安装完成后，log out，然后在打开，就能在应用里看到Vscode 4、sublime text3安装123$ sudo add-apt-repository ppa:webupd8team/sublime-text-3$ sudo apt-get update$ sudo apt-get install sublime-text-installer 5、pycharm命令行安装 添加源 1$ sudo add-apt-repository ppa:mystic-mirage/pycharm 安装免费社区版 12$ sudo apt update$ sudo apt install pycharm-community 以前在u14上安装pycharm的另一个方法笔记（感觉还是上面一个好一点，上面是最新的笔记） 1234$ sudo add-apt-repository ppa:ubuntu-desktop/ubuntu-make$ sudo apt-get update$ sudo apt-get install ubuntu-make$ umake ide pycharm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)自己手动搭建翻墙服务器]]></title>
    <url>%2F2019%2F05%2F21%2F%E6%94%B6%E8%97%8F%E7%BD%91%E5%9D%80%2F%E8%BD%AC-%E8%87%AA%E5%B7%B1%E6%89%8B%E5%8A%A8%E6%90%AD%E5%BB%BA%E7%BF%BB%E5%A2%99%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[原文网址：https://github.com/XX-net/XX-Net/issues/6506 说明：（1）创建服务器是，地区选择美国 （2）选择每月3.5$ 的价格 （3）其他的照着原网址就可以了]]></content>
      <categories>
        <category>网址收藏</category>
      </categories>
      <tags>
        <tag>网址收藏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GPU Tensorflow 安装]]></title>
    <url>%2F2019%2F05%2F21%2FLinux%2FGPU-Tensorflow-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[此安装过程仅在Ubuntu16下安装测试 1、安装nvidia显卡驱动1$ sudo add-apt-repository ppa:graphics-drivers/ppa 2、查看可安装的驱动版本1$ ubuntu-drivers devices 3、选择推荐版本号进行安装1$ sudo apt-get install nvidia-390 nvidia-settings nvidia-prime 说明：我在安装的电脑上推荐的版本是390，因此使用的是390安装 但上图推荐的是430，在安装过程中需要更改为430，如：nvidia-430 123$ sudo apt-get install mesa-common-dev$ sudo apt-get install freeglut3-dev$ sudo reboot 4、安装cuda 从cuda官网下载run文件，下载完后，执行下面代码命令 12$ cd Downloads$ sudo sh cuda_9.0.176_384.81_linux.run 5、下载cudnn及安装cudnn cudnn官网下载，下载7.5版本，对应CUDA9.0 百度云盘下载： 链接: https://pan.baidu.com/s/1cFoaZj_FRmQGXFneg9NL2A 提取码: 2a1v 我这里是从云盘上下载，官网下载也差不多执行下面步骤 下载完后解压，并进到该解压文件所在目录 123$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 6、配置cuda环境12345$ sudo gedit ~/.bashrc在最后添加如下两行代码$ export PATH=/usr/local/cuda-9.0/bin:$PATH$ export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH$ source ~/.bashrc ~/.bashrc相关的配置如下图 7、安装gpu-tensorflow1$ pip install tensorflow-gpu==1.6 说明：当安装1.6版本的tensorflow时，如果报错，可以尝试将版本降低，如改为 tensorflow-gpu==1.5 或 tensorflow-gpu==1.3 8、测试tensorflow安装成功与否1在python环境下输入import tensorflow，不报错，即安装成功 其他可能帮助信息cuda、cudnn版本对应关系https://www.tensorflow.org/install/source#tested_source_configurations 查看GPU显卡型号和驱动版本123$ lspci | grep -i nvidia$ sudo dpkg --list | grep nvidia-*网址：https://www.nvidia.cn/Download/driverResults.aspx/137427/cn 在服务器上安装时需要注意的事项 参考网址：https://blog.csdn.net/QLULIBIN/article/details/78714596 （1）在安装cuda时，需要关闭图形化界面（ctrl+alt+F2键） （2）在出现的选项里，关于opengl的选择选择no，其他的选accept或yes。 （3）如果在重启之后，可能在登陆界面一直循环往复，可能也需要到命令界面，切换为intel显卡]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16下翻墙搭建]]></title>
    <url>%2F2019%2F05%2F18%2FLinux%2Fubuntu16%E4%B8%8B%E7%BF%BB%E5%A2%99%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[说明：此教程为ubuntu下如何翻墙教程，相对来说步骤比较繁琐，可能有更简单的方法，以后用到会继续更新需要安装软件： chrome浏览器 shadowsocks-qt5 安装步骤：1、ubuntu16安装chrome浏览器1234$ sudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/$ wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install google-chrome-stable 2、安装shadowsocks-qt5123$ sudo add-apt-repository ppa:hzwhuang/ss-qt5$ sudo apt-get update$ sudo apt-get install shadowsocks-qt5 3、shadowscoks-qt5配置 打开shadowsocks-qt5 在如下界面，设置IP、端口、密码等等 如果没有设置服务器IP，需要先搭建一个服务器，然后设置对应的端口及密码等等……（这里不介绍如何搭建翻墙服务器） 4、添加插件 添加Proxy SwitchOmega.crx插件 插件下载地址：https://github.com/FelisCatus/SwitchyOmega/releases 说明：该插件下载地址还没有使用过，之前下载过该插件的其他地址，若这个不能使用，还是使用存在于硬盘的插件 将该插件直接拖曳到chrome的扩展中，若拖曳失败，不能正常拖曳，参考该网址：https://blog.csdn.net/qq_33033367/article/details/80952291 注意：要切换为开发模式，然后进行拖曳，注意图中红色框 5、switchomega配置 下图中的URL地址需要自己手动输入，直接复制下面代码即可 1https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 最后点击Download ProfileNow 6、chrome浏览器访问google 首先需要打开shadowsocks-qt5，然后连接服务器 然后通过切换规则，切换为auto switch，就可以访问google了 打开浏览器，输入www.google.com，如果出现这种问题，修改为proxy即可]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win7下局域网打印机共享]]></title>
    <url>%2F2019%2F05%2F17%2F%E7%B3%BB%E7%BB%9F%2Fwin7%E4%B8%8B%E5%B1%80%E5%9F%9F%E7%BD%91%E6%89%93%E5%8D%B0%E6%9C%BA%E5%85%B1%E4%BA%AB%2F</url>
    <content type="text"><![CDATA[说明：本方法只在Win7下运行成功，Win10下还未测试，不过Win10应该也能行 一、打印机所在电脑配置1、取消禁用Guest用户因为别人要访问安装打印机的这个电脑就是以guest账户访问的 点击【开始】按钮，选择【计算机】并右键，选择【管理】，如下图： 在弹出的【计算机管理】窗口中，找到【本地用户和组】，选择【用户】，双击【Guest】，打开【Guest属性】窗口，确保【账户已禁用】选项没有被勾选 2、设置共享目标打印机（1）共享打印机防火墙设置 在桌面选择【计算机】，右键【管理】 在弹出的【计算机管理】窗口中，选择【服务和应用程序】，接着选择【服务】，然后找到【Windows Firewall】这一项 在【Windows Firewall】选项上，右键，点击【属性】，在弹出的窗口中，【启动类型】设置为自动，点击【启动】，如下图所示： （2）设置共享目标打印机 点击【开始】按钮，选择【设备和打印机】 在弹出的窗口，找到想要共享的打印机（前提打印机已正确连接，驱动已正确安装） 若没有安装驱动程序，则参考如下： 在惠普官网上，下载对应打印机驱动（我的为HP LaserJet 1020）程序并安装即可，如下图 在打印机已正确连接，且驱动已正确安装后，选择共享打印机，在该打印机上右键，选择【打印机属性】，如下图： 在弹出的窗口中，切换到【共享】选项卡，勾选【共享这台打印机】，并设置一个共享名（请记住该共享名，后面可能会用到），如下图： （3）高级共享设置 打开【开始】按钮，点击【控制面板】，选择【网络和Internet】，选择【网络和共享中心】，在打开的窗口中，记所处的网络类型，选择【选择家庭组和共享选项】 在弹出的窗口中，单击【更改高级共享设置】，【保存修改】 在弹出的窗口中，注意红色方框标记的选择 二、局域网内其他电脑配置1、首先确保和打印机所在电脑在同一个局域网内2、在桌面打开【计算机】，单击【网络】，找到需要连接的机器 3、账户登录 找到需要连接的机器后，双击，在弹出的窗口中，输入用户名，密码为空 账户用户名，需要在打印机所在电脑查看用户名，具体操作如下： 在打印机所在电脑上，点击【开始】按钮，找到用户名，下图中红色方框标记 4、登录成功后，就能找到打印机 5、右键打印机（HP LaserJet 1020），选择【连接】，进行驱动安装三、打印机使用打印测试 新建空白word文档， 选择【文件】，【打印】，在打印机列表中，选择对应的打印机，即可完成打印]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下 Python2 与 Python3 版本切换]]></title>
    <url>%2F2019%2F05%2F17%2FLinux%2FLinux%E4%B8%8B-Python2-%E4%B8%8E-Python3-%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[说明：在Linux系统下，Python默认版本为2.7，但在使用过程中，可能经常需要使用Python3，因此，通过在网上的一系列搜索，找出了Python2与Python3的切换方法 一、添加软连接12$ sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 100$ sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150 二、版本切换 输入如下命令后，在提示信息里面输入对应数字即可实现切换 添加过软连接后，以后需要切换python版本只用输入如下命令 1$ sudo update-alternatives --config python]]></content>
      <categories>
        <category>Linux</category>
        <category>语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统重装后必要软件安装介绍]]></title>
    <url>%2F2019%2F05%2F16%2F%E7%B3%BB%E7%BB%9F%2F%E7%B3%BB%E7%BB%9F%E9%87%8D%E8%A3%85%E5%90%8E%E5%BF%85%E8%A6%81%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[此软件全部在Windows下1、Windows激活工具 KMS激活工具：http://www.yishimei.cn/network/319.html 说明：此工具用于激活windows极好，但是office等好像没有激活过（近两年没测试过） 2、办公软件（Office 2013软件及破解工具） 百度网盘下载地址：链接: https://pan.baidu.com/s/1O1EoQi0e4K-9CB7-Xeg56A 提取码: j5ku 说明：KMSpico_Install为 office激活工具 解压后，运行KMDELDI.exe，在弹出的窗口中，点击红色按钮即可激活office 3、解压软件（7zip） 下载地址：https://www.7-zip.org/ 说明：相比于其他的解压软件，无广告，且解压速度快，体验效果最好 4、输入法（搜狗输入法） 下载地址：https://pinyin.sogou.com/ 说明：一直都用的搜狗输入法，感觉挺不错的 5、浏览器（Chrome） 下载地址：https://www.google.cn/chrome/ 说明：作为一名IT行业相关人员，chrome浏览器首选，其次，火狐 6、文本编辑器（Sublime Text3） 下载地址：https://www.sublimetext.com/3 说明：相比于Vscode，本人更倾向于Sublime 7、PDF阅读器（福昕阅读器） 下载地址：https://www.foxitsoftware.cn/downloads/ 说明：听说该软件还有破解版的，可以直接对PDF文件进行编辑 8、视频播放器（PotPlayer） 下载地址：https://daumpotplayer.com/download/ 说明：比较好用的一个视频播放器 9、视频格式转换器（格式工厂） 下载地址：http://www.pcfreetime.com/formatfactory/CN/download.html 说明：视频之间的格式转换，以及截取视频部分片段都挺好用的 10、文件搜索工具（Everything） 下载地址：https://www.voidtools.com/zh-cn/downloads/ 说明：文件整个硬盘搜索，又快又方便，强烈推荐 11、思维导图工具（XMind） 下载地址：https://www.xmind.net/ 说明：官网下载的只能免费试用一段时间，但是做思维导图，效果极棒 破解版下载地址：链接: https://pan.baidu.com/s/1RaNGeqRb4uSJRYo-Aq8l4w 提取码: ks42 12、远程控制工具（Teamviewer） 下载地址：https://www.teamviewer.com/cn/ 说明：非常好用的软件，比QQ远程电脑控制不知道好在哪里去，并且可以手机远程控制电脑，只要双方都下载该软件，即能实现远程控制 但是用久了之后，可能会被检测出商业用途，5分钟内强制下线，可以通过修改mac地址解决 13、笔记（有道云笔记） 下载地址：https://note.youdao.com/?keyfrom=ydoc 说明：本人用这个比较习惯一点，并且我还扩容过，总共有13G呢~，但是部分人喜欢印象笔记，因人而异吧 14、音乐（网易云音乐） 下载地址：https://music.163.com/ 说明：相比其他的音乐播放器，更爱网易云，很多人来网易云不是听歌，而是为了看评论，哈哈~ 15、便签工具（win7自带桌面便签） 说明：使用windows自带的便签，每次开机都会显示在桌面，可以起到很好的提醒作用，但是有一款软件好像还可以，叫敬业签，但是还没有用过，还不清楚性能咋样 16、百度云盘下载加速软件（Speed盘） 说明：这里不添加下载链接，因为对此软件存疑 该软件能对百度云盘的文件下载起到很快的加速作用，但使用该软件是否会泄露我们的相关信息，这点暂时未知 只知道以前下载时是不需要登录百度云盘账号的，现在下载好像必须要登录云盘账号 因此，不太推荐使用该软件 17、广告（视频）过滤软件（ADSafe） 下载地址：http://www.ad-safe.com/ 说明：以前没有办视频会员的时候，经常使用该软件进行视频广告过滤，过滤效果很好，但是现在一般都用会员看视频了，已经很长时间没有使用过该软件。 但是这个安全性如何，不太清楚 18、以下为个人喜欢的一些软件（IT行业相关）（1）SSH远程访问软件 Xshell：https://www.netsarang.com/zh/ 说明：感觉还不错，相比其他的，这个整体效果不错 WinSCP：https://winscp.net/eng/download.php 说明：可以直接对远程电脑文件进行修改，不需要本地修改在上传，值得推荐（在线编辑远程文件） FileZilla：https://filezilla-project.org/ 说明：可以本地修改文件，然后上传到远程电脑上（更多用于文件上传） Putty：https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html 说明：连接上远程电脑后，可以运行程序，进行相应操作 小结：刚入门时，使用Putty软件，进行远程控制，但是发现无法上传文件，于是使用FileZilla软件进行文件上传，后来发现，每次需要本地修改在上传比较麻烦，使用WinSCP软件，连接后可以在线编辑。而Xshell软件用于连接服务器挺好的 （2）Git工具 下载地址：https://git-scm.com/downloads 说明：用于github管理，作为一名程序猿，这个还是得有 （3）翻墙软件（SS） 下载地址：https://github.com/shadowsocks/shadowsocks-windows/releases 说明：ss全称是shadowsocks，国内翻墙一般都用ss，但是也还有另外一款翻墙软件，简称叫ssr，全名应该叫shadowsocks-rss，是基于ss做过修改的，但是具体哪个好用，感觉都差不多，我用的比较多的是ss （4）MD编辑器（Markdown编辑器之Typora） 下载地址：https://www.typora.io/ 说明：在博客编辑这方面，Typora非常好用，虽然刚体验，但体验效果贼好 （5）C++编译器（Dev C++） 下载地址：https://sourceforge.net/projects/orwelldevcpp/ 说明：喜欢这一款编译器，是因为易安装，轻量级，只用于C++代码编程，如果要进行C++开发的话，还是选择VS 19、好友推荐：（1）Java编译器（IDEA）（2）在线工具 网址：http://tool.oschina.net/?tdsourcetag=s_pctim_aiomsg]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网址收藏]]></title>
    <url>%2F2019%2F05%2F15%2F%E6%94%B6%E8%97%8F%E7%BD%91%E5%9D%80%2F%E7%BD%91%E5%9D%80%E6%94%B6%E8%97%8F%2F</url>
    <content type="text"><![CDATA[NexT:NexT官网：https://theme-next.iissnan.com/getting-started.html Hexo指南：https://hexo-guide.readthedocs.io/zh_CN/latest/index.html 好友博客： qzjiang：https://ourvibes.github.io/ 他人博客： https://xian6ge.cn/ https://yangbingdong.com/ 简书： https://www.jianshu.com/p/3a05351a37dc?tdsourcetag=s_pctim_aiomsg 其他： 搜索引擎优化：https://juejin.im/post/590b451a0ce46300588c43a0?tdsourcetag=s_pctim_aiomsg]]></content>
      <categories>
        <category>网址收藏</category>
      </categories>
      <tags>
        <tag>网址收藏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不蒜子访客人数统计和博客运行时间显示]]></title>
    <url>%2F2019%2F05%2F15%2FHexo%2F%E4%B8%8D%E8%92%9C%E5%AD%90%E8%AE%BF%E5%AE%A2%E4%BA%BA%E6%95%B0%E7%BB%9F%E8%AE%A1%E5%92%8C%E5%8D%9A%E5%AE%A2%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4%E6%98%BE%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[一、Hexo页脚增加网站运行时间统计解决方案 1、找到 \themes\next\layout\_partials\ 下的 footer.swig文件 2、在以下位置插入所示代码 123456789101112131415161718192021222324252627282930313233343536&lt;span id="sitetime"&gt;&lt;/span&gt;&lt;script language=javascript&gt; function siteTime()&#123; window.setTimeout("siteTime()", 1000); var seconds = 1000; var minutes = seconds * 60; var hours = minutes * 60; var days = hours * 24; var years = days * 365; var today = new Date(); var todayYear = today.getFullYear(); var todayMonth = today.getMonth()+1; var todayDate = today.getDate(); var todayHour = today.getHours(); var todayMinute = today.getMinutes(); var todaySecond = today.getSeconds(); /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳) year - 作为date对象的年份，为4位年份值 month - 0-11之间的整数，做为date对象的月份 day - 1-31之间的整数，做为date对象的天数 hours - 0(午夜24点)-23之间的整数，做为date对象的小时数 minutes - 0-59之间的整数，做为date对象的分钟数 seconds - 0-59之间的整数，做为date对象的秒数 microseconds - 0-999之间的整数，做为date对象的毫秒数 */ var t1 = Date.UTC(2019,05,14,00,00,00); //这里调整博客建站时间，时间：2019-05-14 00:00:00 var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond); var diff = t2-t1; var diffYears = Math.floor(diff/years); var diffDays = Math.floor((diff/days)-diffYears*365); var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours); var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes); var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds); document.getElementById("sitetime").innerHTML=" 已运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒"; &#125;/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/ siteTime();&lt;/script&gt; 3、结果显示 转载：https://xian6ge.cn/posts/82ce1911/二、Hexo 页脚 不蒜子访客统计 在百度和谷歌上搜索过太多的Hexo 访问统计，但很多博客都不能显示人数，终于不负有心人，找到了一个可以实现 Hexo 页脚访问统计的网址，Hexo指南网址提供的文档能访问 1、添加代码找到 \themes\next\layout\_partials\ 下的 footer.swig文件，在如下位置插入代码 123456789&lt;script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; &lt;span id="busuanzi_container_site_pv"&gt; 本站总访问量&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次 &lt;/span&gt; &lt;span id="busuanzi_container_site_uv"&gt; 本站访客数&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次 &lt;/span&gt; 2、说明 安装脚本 12&lt;script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 显示站点总访问量 （1）pv：单个用户连续点击n篇文章，记录n次访问量 （2）uv：单个用户连续点击n篇文章，只记录1次访客数 pv方式加入 123&lt;span id="busuanzi_container_site_pv"&gt; 本站总访问量&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次&lt;/span&gt; nv方式加入 123&lt;span id="busuanzi_container_site_uv"&gt; 本站访客数&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次&lt;/span&gt; 转载：https://hexo-guide.readthedocs.io/zh_CN/latest/third-service/[%E4%B8%8D%E8%92%9C%E5%AD%90]%E8%AE%BF%E5%AE%A2%E4%BA%BA%E6%95%B0.html]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo之theme主题配置]]></title>
    <url>%2F2019%2F05%2F14%2FHexo%2FHexo%E4%B9%8Btheme%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[说明：每一个大标题结尾都有一个参考链接，出现问题时，请参考其参考链接一、下载 Hexo主题1、到官网选择自己喜欢的Hexo主题 2、下载 NextT主题 在git bash窗口上，输入如下命令： 12$ cd &lt;博客存放的目录&gt;$ git clone https://github.com/iissnan/hexo-theme-next themes/next 将主题克隆到theme目录下后，会在其目录下发现多出一个next文件夹 注：若需要下载其他主题，只需要在上面的代码中，将next更改为其他主题名称即可 参考网址：https://www.jianshu.com/p/33bc0a0a6e90?tdsourcetag=s_pctim_aiomsg二、NexT主题配置 在 Hexo中有两份主要的配置文件，其名称都是 _config.yml。其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。 为了描述方便，在以下说明中，将前者称为站点配置文件， 后者称为主题配置文件。 以下所有终端执行的命令都在你的 Hexo 根目录下 1、基本信息配置 基本信息包括：博客标题、作者、描述、语言等等。 打开 站点配置文件 ，找到Site模块 123456title: 标题subtitle: 副标题description: 描述author: 作者language: 语言（简体中文是zh-Hans）timezone: 网站时区（Hexo 默认使用您电脑的时区，不用写） 我的配置如下： 2、菜单设置 菜单包括：首页、归档、分类、标签、关于等等 我们刚开始默认的菜单只有首页和归档两个，不能够满足我们的要求，所以需要添加菜单，打开 主题配置文件 找到Menu Settings 123456789menu: home: / || home //首页 archives: /archives/ || archive //归档 categories: /categories/ || th //分类 tags: /tags/ || tags //标签 about: /about/ || user //关于 #schedule: /schedule/ || calendar //日程表 #sitemap: /sitemap.xml || sitemap //站点地图 #commonweal: /404/ || heartbeat //公益404 我的配置如下： 3、Next主题样式设置打开 主题配置文件 找到Scheme Settings 12345# Schemes# scheme: Muse# scheme: Mist# scheme: Piscesscheme: Gemini 我选择的是Gemini风格 4、侧栏设置 侧栏设置包括：侧栏位置、侧栏显示与否、文章间距、返回顶部按钮等等 打开 主题配置文件 找到sidebar字段 12345678910111213141516sidebar:# Sidebar Position - 侧栏位置（只对Pisces | Gemini两种风格有效） position: left //靠左放置 #position: right //靠右放置# Sidebar Display - 侧栏显示时机（只对Muse | Mist两种风格有效） #display: post //默认行为，在文章页面（拥有目录列表）时显示 display: always //在所有页面中都显示 #display: hide //在所有页面中都隐藏（可以手动展开） #display: remove //完全移除 offset: 12 //文章间距（只对Pisces | Gemini两种风格有效） b2t: false //返回顶部按钮（只对Pisces | Gemini两种风格有效） scrollpercent: true //返回顶部按钮的百分比 5、头像设置打开 主题配置文件 找到Sidebar Avatar字段 12# Sidebar Avataravatar: /images/header.jpg 这是头像的路径，只需把你的头像命名为header.jpg（随便命名）放入themes/next/source/images中，将avatar的路径名改成你的头像名就OK啦！ 6、设置RSS（后面可以在继续操作，目前还存在问题，这里不显示操作步骤，详细看后面的参考链接）7、添加分类模块1、新建一个分类页面 1$ hexo new page categories 2、你会发现你的source文件夹下有了categorcies/index.md，打开index.md文件将title设置为title: 分类 目录结构如下： 3、把文章归入分类只需在文章的顶部标题下方添加categories字段，即可自动创建分类名并加入对应的分类中 举个栗子： 12title: 分类测试文章标题categories: 分类名 8、添加标签模块1、新建一个标签页面 1$ hexo new page tags 2、你会发现你的source文件夹下有了tags/index.md，打开index.md文件将title设置为title: 标签 3、把文章添加标签只需在文章的顶部标题下方添加tags字段，即可自动创建标签名并归入对应的标签中 举个栗子： 12345title: 标签测试文章标题tags: - 标签1 - 标签2 ... 9、添加关于模块1、新建一个关于页面 1$ hexo new page about 2、你会发现你的source文件夹下有了about/index.md，打开index.md文件即可编辑关于你的信息，可以随便编辑。 10、添加搜索功能1、安装 hexo-generator-searchdb 插件 1$ npm install hexo-generator-searchdb --save 2、打开 站点配置文件 找到Extensions在下面添加 123456# 搜索search: path: search.xml field: post format: html limit: 10000 我的配置如下： 3、打开 主题配置文件 找到Local search，将enable设置为true 11、设置后博客界面 参考链接：https://www.jianshu.com/p/3a05351a37dc?tdsourcetag=s_pctim_aiomsg三、访问统计及数字统计1、数字统计 显示文章字数统计、阅读时长、总字数 安装插件 1$ npm i --save hexo-wordcount 在 主题配置文件 中，搜索关键字 post_wordcount 1234567891011# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true #字数统计 wordcount: true #预览时间 min2read: true #总字数,显示在页面底部 totalcount: false separated_meta: true 2、访问统计 LeabCloud - 文章阅读量 注册 LeabCloud，访问控制台 ，创建应用 ，新应用名称可任意填写，选择“开发板”创建应用。创建完成之后点击新创建的应用的名字来打开应用参数配置界面，并点击点击左侧右上角的齿轮图标，新建Class，如下图所示： 创建完成之后，左侧数据栏应该会多出一栏名为 Counter 的栏目，这个时候我们点击设置，切换到test应用的操作界面。在弹出的界面中，选择左侧的 应用Key 选项，即可发现我们创建应用的 AppID 以及 AppKey ，有了它，我们就有权限能够通过主题中配置好的Javascript代码与这个应用的Counter表进行数据存取操作了。 在 主题配置文件 中，搜索关键字 leancloud_visitors ，将 false 改为 true ，并复制粘贴上述的 AppID以及 AppKey 需要特别说明的是：记录文章访问量的唯一标识符是文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计。 Web 安全。因为 AppID 以及 AppKey 是暴露在外的，因此如果一些别有用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启 Web 安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。 选择应用的设置的 安全中心 选项卡: 在 Web 安全域名 中填入我们自己的博客域名，来确保数据调用的安全。 3、图形显示 参考链接：http://dinghongkai.com/2017/12/19/Blog-development-5-NexT-Theme-Advanced-Customization/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现谷歌翻译PDF换行的问题]]></title>
    <url>%2F2019%2F05%2F14%2F%E8%AF%AD%E8%A8%80%2FPython%2Fpython%E5%AE%9E%E7%8E%B0%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91PDF%E6%8D%A2%E8%A1%8C%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前提条件（1）浏览器 chrome（2）python编辑器：pycharm使用说明1、 pycharm上安装pyperclip、webbrowserdownloader 2、创建google_translate.py文件，并将如下代码复制粘贴至google_translate.py文件 添加了百度翻译，运行这个py程序，将会在谷歌窗口弹出两个页面，分别是谷歌翻译和百度翻译 123456789101112131415161718#coding=utf-8import pyperclipimport webbrowsercopyBuff = ' 'num = 1#convinent to change numwhile num == 1: num = num + 1 copyedText = pyperclip.paste() if copyBuff != copyedText: copyBuff = copyedText normalizedText = copyBuff.replace('\n', ' ') url = 'https://translate.google.cn/#en/zh-CN/' + normalizedText webbrowser.open(url) url = 'https://fanyi.baidu.com/#en/zh/' + normalizedText webbrowser.open(url) 3、运行（1）在pdf上任意选择一个段落，复制 （2）运行google_translate.py程序，chrome浏览器会自动弹出一个google翻译界面，刚才复制的内容就会直接翻译 注：在换行出现‘-’的情况下，可能没有处理，即如下情况：在di-rectly这里，google翻译中没有解决，不过不影响翻译 We present the first deep learning model to successfully learn control policies di-rectly from high-dimensional sensory input using reinforcement learning.]]></content>
      <categories>
        <category>语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo图片显示和Github上图片显示的解决办法]]></title>
    <url>%2F2019%2F05%2F14%2FHexo%2FHexo%E5%9B%BE%E7%89%87%E6%98%BE%E7%A4%BA%E5%92%8CGithub%E4%B8%8A%E5%9B%BE%E7%89%87%E6%98%BE%E7%A4%BA%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1、配置_config.yml文件找到博客根目录下的 _config.yml，如我的文件路径为 D:\blog\\_config.yml打开_config.yml，查找 post_asset_folder字段，将post_asset_floder设置为true。如下图 2、post_asset_folder 设置为true和false的区别当post_asset_folder设置为true后，使用命令：hexo new post “新博客名称” ，创建新博客时，Hexo会自动建立一个与文章同名的文件夹，如下图 3、安装插件打开git bash，进入到博客的根目录下（如：cd d/blog/），执行命令：npm install https://github.com/CodeFalling/hexo-asset-image—save ，完成插件的安装4、图片存放将所有图片资源都放到对应的文件夹下（如：Hexo图片显示和Github上图片显示的解决办法）5、本地查看显示效果使用 hexo s 命令，运行本地博客，成功显示图片如下图 6、上传github并显示使用 hexo g -d ，重新生成并上传，浏览器输入https://ldgcug.github.io 即可查看效果 遇到过的坑：（1）图片的路径问题 图片加载格式：由两部分组成（![图片描述] + (图片路径)） 输入![logo] (图片路径)，（由于正确加载，前面的输入会变成一个图片，因此在[logo] （图片路径）中间加了一个空格，正确输入时，应去掉空格）会出现一个输入图片路径的提示，从文件中选择路径后如下图 此时，用该绝对路径上传到github后，输入https://ldgcug.github.io/，会发现，图片无法加载解决方法：将绝对路径改为相对路径 （2）图片的命名问题 不能使用”_”作为开头]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github创建博客过程中踩过的坑]]></title>
    <url>%2F2019%2F05%2F14%2FHexo%2FHexo-Github%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2%E8%BF%87%E7%A8%8B%E4%B8%AD%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[一、系统环境配置要使用Hexo，需要在系统中支持Nodejs以及Git1、安装Node.jsNode.js官网 ​ 打开cmd命令行，输入node -v和npm -v，成功界面如下 2、安装GitGit官网 ​ 安装成功后，在电脑桌面任意空白位置，右键，出现以下界面（自行配置环境） 二、Github配置前提条件：已经拥有一个github账号，并已成功登陆1、点击【New repository】，创建一个版本仓库​ 注：仓库名称必须为username.github.io，我这里的username为ldgcug 2、创建完成后，在项目右侧，有个【Settings按钮】，点击【Settings按钮】，向下滑到【GitHub Pages】位置，能在这里发现一个网址，并且可以通过外网访问 三、安装Hexo并配置安装Hexo，在自己认为合适的地方创建文件夹，如：我是在D盘创建了一个blog文件夹，然后通过cmd命令行进入到该文件夹里面​ 输入 npm install hexo -g ，开始安装Hexo ​ 安装成功后，输入 hexo -v，能看到版本号 ​ 输入 hexo init，初始化该文件夹（此处不显示图片过程） ​ 输入 npm install ，安装所需要的组件 ​ 输入 hexo g，体验Hexo ​ 输入 hexo s，开启服务器，访问http://localhost:4000，正式体验Hexo ​ 出现下图，则安装成功 四、Hexo与Github Pages联系起来 说明：下面的所有操作都在博客根目录下执行，使用git bash窗口 1、设置Git的user name 和email（如果第一次配置）​ 注：将用户名和邮箱换成自己的用户名和邮箱 2、输入 ssh-keygen -t rsa -C “569167692@qq.com”，连续三个回车，生成密钥，查看自己的文件存储路径（默认路径为：C:\Users\Administrator\.ssh） ​ 输入 eval “$(ssh-agent -s)” ，添加密钥到 ssh-agent ​ 输入 ssh-add ~/.ssh/id_rsa ，添加生成的SSH key 到 ssh-agent ​ 输入 cd ~/.ssh，检查是否有.ssh文件夹，若没有，则进入C:\Users\Administrator\.ssh ​ 输入 ls，查看.ssh文件夹下文件（第一次没有known_hosts文件） ​ 进入./ssh文件夹下，打开 id_rsa.pub文件，将其中的内容复制到缓存中 ​ 登陆Github，点击【头像】下的【setting】，点击左侧的【SSH and GPG keys】，点击【New SSH key】，将刚才复制的 id_rsa.pub 文件复制到Key中，添加ssh ​ 输入 ssh -T git@github.com ，测试添加ssh是否成功，如果看到Hi 后面是你的用户名，则添加成功 3、配置Deployment，在blog文件夹中，找到_config.yml文件，修改repo值（在文件末尾） 下图中，需要将repository更改为你自己的github.io ​ 其中，repo的值在github项目里的ssh中（注意使用SSH，而不是HTTPS） ​ 注：在_config_yaml文件中，type、repo、branch文件后面有一个空格，即上图中添加的代码会变成绿色，最开始没有空格时，遇到了很多的坑 五、新建博客 所有的命令操作都在博客根目录下执行 1、在cmd 命令种安装扩展​ 输入 ： npm install hexo-deployer-git —save 2、新建博客​ 在git bash窗口中，执行命令： hexo new post “博客名” ​ 会发现在soucrce\_posts 文件下，多出刚创建的博客.md 3、md编辑器（Typora编辑器）&gt;&gt; 在Typora官网上下载Typora编辑器，使用Typora编辑器对md文件进行编辑 4、部署发布​ 使用编辑器编辑好文章后，使用命令： hexo d -g ，就可以生成和部署了 ​ 部署成功后，访问你的地址：http//用户名.github.io 。就能看到刚生成的文章。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
